{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXRPF7T0b-QZ",
    "outputId": "821fc7ca-79e6-4cf6-e81d-37fd685b71f3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spafe in /home/ashutosh/anaconda3/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: pandas in /home/ashutosh/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: tensorflow in /home/ashutosh/anaconda3/lib/python3.11/site-packages (2.14.0)\n",
      "Requirement already satisfied: seaborn in /home/ashutosh/anaconda3/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: opencv-python in /home/ashutosh/anaconda3/lib/python3.11/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: tqdm in /home/ashutosh/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: librosa in /home/ashutosh/anaconda3/lib/python3.11/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from spafe) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from spafe) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from spafe) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (0.57.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (0.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.40.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: pycparser in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ashutosh/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#run\n",
    "!pip install spafe pandas tensorflow seaborn opencv-python tqdm librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s2K5aSHYb-Sm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from spafe.utils import vis\n",
    "from spafe.features.lfcc import lfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-xNIFhPtb-U5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 15:17:45.277138: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-15 15:17:45.279711: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-15 15:17:45.310777: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-15 15:17:45.310803: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-15 15:17:45.310823: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-15 15:17:45.316908: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-15 15:17:45.317591: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 15:17:46.070233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#run\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QgbvNKYAb-YM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50,ResNet101\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import StratifiedKFold , KFold ,RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LXsQV7ivcIcc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask zero out padding tokens.\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  return tf.matmul(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7WB2_09_e-qt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "# This allows to the transformer to know where there is real data and where it is padded\n",
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PjEQIR8TcIe3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # linear layers\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # split heads\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8hF4xYdLcSQA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # apply sin to even index in the array\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd index in the array\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j7ewC3iBcSSo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "  print(padding_mask)\n",
    "\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention1\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention1 = tf.keras.layers.Dropout(rate=dropout)(attention1)\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention1)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention2\")({\n",
    "          'query': attention1,\n",
    "          'key': attention1,\n",
    "          'value': attention1,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + attention2)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_8IFloblcSWJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "def encoder(time_steps,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            projection,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  if projection=='linear':\n",
    "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
    "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
    "    print('linear')\n",
    "\n",
    "  else:\n",
    "    projection=tf.identity(inputs)\n",
    "    print('none')\n",
    "\n",
    "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NXgJh2PpcIiK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "def transformer(time_steps,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                output_size,\n",
    "                projection,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
    "\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(tf.dtypes.cast(\n",
    "\n",
    "      #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
    "      # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked\n",
    "      tf.math.reduce_sum(\n",
    "      inputs,\n",
    "      axis=2,\n",
    "      keepdims=False,\n",
    "      name=None\n",
    "  ), tf.int32))\n",
    "\n",
    "  enc_outputs = encoder(\n",
    "      time_steps=time_steps,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      projection=projection,\n",
    "      name='encoder'\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  #We reshape for feeding our FC in the next step\n",
    "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
    "\n",
    "  #We predict our class\n",
    "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True, name=\"outputs\")(outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9H-kkyuLcbPZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run\n",
    "# num_batch_size = 32\n",
    "# num_epochs = 500\n",
    "# N_SPLIT = 10\n",
    "# num_labels=5\n",
    "# num_classes=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4Iw3KzecbRt",
    "outputId": "0ffe72e2-2c24-4cbd-9441-a681fcf507e4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2075, 289)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ajit\n",
    "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate5_spectral2.csv')\n",
    "# dm\n",
    "# df = pd.read_csv('./DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm.csv')\n",
    "df = pd.read_csv('./features/try.csv')\n",
    "\n",
    "\n",
    "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/melspectogram_mean_newdm_2075.csv')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/chroma_cqt_simple_mean_newdm_2075.csv')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/mfcc_simple_mean_newdm_2075.csv')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/spectral_centroid_meandm_2075_200.csv')\n",
    "\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HrbtbLXcbTx",
    "outputId": "7a1af2ba-848e-44d8-d239-a2518c22f9bd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415, 289)\n",
      "(415, 289)\n",
      "(415, 289)\n",
      "(415, 289)\n",
      "(415, 289)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "siz=415\n",
    "df_read = df.copy()\n",
    "df1 = df_read.sample(siz)\n",
    "df_read = df_read.drop(df1.index)\n",
    "df2 = df_read.sample(siz)\n",
    "df_read = df_read.drop(df2.index)\n",
    "df3 = df_read.sample(siz)\n",
    "df_read = df_read.drop(df3.index)\n",
    "df4 = df_read.sample(siz)\n",
    "df_read = df_read.drop(df4.index)\n",
    "df5 = df_read.copy()\n",
    "\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)\n",
    "print(df5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_j19bgqpcbWT",
    "outputId": "e4e9f119-b436-44fe-ab74-572cf6d7bca3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([ 881,  453, 2004, 1353,  281,  941, 1185, 1159, 1138,  599,\n",
      "       ...\n",
      "        834, 1730,  353, 1345, 1190, 1375,  185,  701, 1671, 1982],\n",
      "      dtype='int64', length=415)\n"
     ]
    }
   ],
   "source": [
    "q = list(df1.index)+list(df2.index)+list(df3.index)+list(df4.index)+list(df5.index)\n",
    "print(df1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>881</td>\n",
       "      <td>16.303433</td>\n",
       "      <td>20.173091</td>\n",
       "      <td>25.433043</td>\n",
       "      <td>25.643291</td>\n",
       "      <td>24.152125</td>\n",
       "      <td>22.587444</td>\n",
       "      <td>21.328204</td>\n",
       "      <td>22.817773</td>\n",
       "      <td>26.437254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.921796e-01</td>\n",
       "      <td>1.737764e-01</td>\n",
       "      <td>1.648728e-01</td>\n",
       "      <td>1.425348e-01</td>\n",
       "      <td>1.305925e-01</td>\n",
       "      <td>1.261496e-01</td>\n",
       "      <td>1.147960e-01</td>\n",
       "      <td>1.103482e-01</td>\n",
       "      <td>1.081272e-01</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>453</td>\n",
       "      <td>12.214500</td>\n",
       "      <td>13.047566</td>\n",
       "      <td>12.366274</td>\n",
       "      <td>13.698728</td>\n",
       "      <td>14.184204</td>\n",
       "      <td>13.415470</td>\n",
       "      <td>13.486002</td>\n",
       "      <td>13.617162</td>\n",
       "      <td>13.263748</td>\n",
       "      <td>...</td>\n",
       "      <td>4.224886e-01</td>\n",
       "      <td>4.762110e-01</td>\n",
       "      <td>2.761365e-01</td>\n",
       "      <td>2.889277e-01</td>\n",
       "      <td>2.664903e-01</td>\n",
       "      <td>1.228172e-01</td>\n",
       "      <td>3.699411e-02</td>\n",
       "      <td>3.033411e-02</td>\n",
       "      <td>4.899616e-03</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2004</td>\n",
       "      <td>18.783516</td>\n",
       "      <td>18.783516</td>\n",
       "      <td>18.783516</td>\n",
       "      <td>7.292035</td>\n",
       "      <td>14.739904</td>\n",
       "      <td>18.196108</td>\n",
       "      <td>19.780011</td>\n",
       "      <td>17.251981</td>\n",
       "      <td>18.458265</td>\n",
       "      <td>...</td>\n",
       "      <td>3.272029e-09</td>\n",
       "      <td>3.101699e-09</td>\n",
       "      <td>2.865078e-09</td>\n",
       "      <td>2.662100e-09</td>\n",
       "      <td>2.523615e-09</td>\n",
       "      <td>2.376565e-09</td>\n",
       "      <td>2.315456e-09</td>\n",
       "      <td>1.619550e-09</td>\n",
       "      <td>5.227693e-10</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1353</td>\n",
       "      <td>16.097207</td>\n",
       "      <td>16.342448</td>\n",
       "      <td>19.168090</td>\n",
       "      <td>19.801918</td>\n",
       "      <td>19.517579</td>\n",
       "      <td>17.757652</td>\n",
       "      <td>20.430178</td>\n",
       "      <td>21.263438</td>\n",
       "      <td>21.988878</td>\n",
       "      <td>...</td>\n",
       "      <td>5.808534e-09</td>\n",
       "      <td>5.761238e-09</td>\n",
       "      <td>5.684933e-09</td>\n",
       "      <td>5.637743e-09</td>\n",
       "      <td>5.651255e-09</td>\n",
       "      <td>5.777343e-09</td>\n",
       "      <td>5.980060e-09</td>\n",
       "      <td>5.165057e-09</td>\n",
       "      <td>3.167535e-09</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>281</td>\n",
       "      <td>14.450840</td>\n",
       "      <td>17.514262</td>\n",
       "      <td>20.270981</td>\n",
       "      <td>20.973510</td>\n",
       "      <td>19.283957</td>\n",
       "      <td>18.421601</td>\n",
       "      <td>22.061808</td>\n",
       "      <td>20.789789</td>\n",
       "      <td>20.464257</td>\n",
       "      <td>...</td>\n",
       "      <td>1.310904e+00</td>\n",
       "      <td>1.375666e+00</td>\n",
       "      <td>1.553347e+00</td>\n",
       "      <td>1.856186e+00</td>\n",
       "      <td>1.661794e+00</td>\n",
       "      <td>1.592669e+00</td>\n",
       "      <td>1.580095e+00</td>\n",
       "      <td>9.239505e-01</td>\n",
       "      <td>8.044317e-02</td>\n",
       "      <td>1.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5          6  \\\n",
       "881    881  16.303433  20.173091  25.433043  25.643291  24.152125  22.587444   \n",
       "453    453  12.214500  13.047566  12.366274  13.698728  14.184204  13.415470   \n",
       "2004  2004  18.783516  18.783516  18.783516   7.292035  14.739904  18.196108   \n",
       "1353  1353  16.097207  16.342448  19.168090  19.801918  19.517579  17.757652   \n",
       "281    281  14.450840  17.514262  20.270981  20.973510  19.283957  18.421601   \n",
       "\n",
       "              7          8          9  ...           279           280  \\\n",
       "881   21.328204  22.817773  26.437254  ...  1.921796e-01  1.737764e-01   \n",
       "453   13.486002  13.617162  13.263748  ...  4.224886e-01  4.762110e-01   \n",
       "2004  19.780011  17.251981  18.458265  ...  3.272029e-09  3.101699e-09   \n",
       "1353  20.430178  21.263438  21.988878  ...  5.808534e-09  5.761238e-09   \n",
       "281   22.061808  20.789789  20.464257  ...  1.310904e+00  1.375666e+00   \n",
       "\n",
       "               281           282           283           284           285  \\\n",
       "881   1.648728e-01  1.425348e-01  1.305925e-01  1.261496e-01  1.147960e-01   \n",
       "453   2.761365e-01  2.889277e-01  2.664903e-01  1.228172e-01  3.699411e-02   \n",
       "2004  2.865078e-09  2.662100e-09  2.523615e-09  2.376565e-09  2.315456e-09   \n",
       "1353  5.684933e-09  5.637743e-09  5.651255e-09  5.777343e-09  5.980060e-09   \n",
       "281   1.553347e+00  1.856186e+00  1.661794e+00  1.592669e+00  1.580095e+00   \n",
       "\n",
       "               286           287  class  \n",
       "881   1.103482e-01  1.081272e-01   1.61  \n",
       "453   3.033411e-02  4.899616e-03   1.15  \n",
       "2004  1.619550e-09  5.227693e-10   1.76  \n",
       "1353  5.165057e-09  3.167535e-09   1.24  \n",
       "281   9.239505e-01  8.044317e-02   1.08  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
    "train = train.dropna()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uT0Jrfa5cbZv",
    "outputId": "765d52d9-b8e2-4f6d-cd94-261bb7f7c39d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1660, 1, 284)\n",
      "d_model 284\n",
      "num_heads 4\n",
      "TIME STEPS 1\n",
      "linear\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
      "Epoch 1/1200\n",
      "52/52 [==============================] - 11s 64ms/step - loss: 2.7456 - val_loss: 1.8440\n",
      "Epoch 2/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.9703 - val_loss: 1.7688\n",
      "Epoch 3/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.9189 - val_loss: 1.8185\n",
      "Epoch 4/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.9030 - val_loss: 1.6858\n",
      "Epoch 5/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.8891 - val_loss: 1.7069\n",
      "Epoch 6/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.8084 - val_loss: 1.6509\n",
      "Epoch 7/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7798 - val_loss: 1.6406\n",
      "Epoch 8/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7511 - val_loss: 1.6133\n",
      "Epoch 9/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7150 - val_loss: 1.5680\n",
      "Epoch 10/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7569 - val_loss: 1.5474\n",
      "Epoch 11/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7131 - val_loss: 1.5755\n",
      "Epoch 12/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7155 - val_loss: 1.5610\n",
      "Epoch 13/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7014 - val_loss: 1.5418\n",
      "Epoch 14/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6683 - val_loss: 1.5940\n",
      "Epoch 15/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6815 - val_loss: 1.6117\n",
      "Epoch 16/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.7143 - val_loss: 1.5495\n",
      "Epoch 17/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6309 - val_loss: 1.5377\n",
      "Epoch 18/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6469 - val_loss: 1.5485\n",
      "Epoch 19/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6459 - val_loss: 1.5255\n",
      "Epoch 20/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6378 - val_loss: 1.5726\n",
      "Epoch 21/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6179 - val_loss: 1.5543\n",
      "Epoch 22/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6506 - val_loss: 1.4831\n",
      "Epoch 23/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6698 - val_loss: 1.5071\n",
      "Epoch 24/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6180 - val_loss: 1.5431\n",
      "Epoch 25/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.6457 - val_loss: 1.5426\n",
      "Epoch 26/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.6322 - val_loss: 1.5761\n",
      "Epoch 27/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6229 - val_loss: 1.5234\n",
      "Epoch 28/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6123 - val_loss: 1.5115\n",
      "Epoch 29/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5863 - val_loss: 1.4854\n",
      "Epoch 30/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5770 - val_loss: 1.5347\n",
      "Epoch 31/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5753 - val_loss: 1.5082\n",
      "Epoch 32/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5487 - val_loss: 1.4570\n",
      "Epoch 33/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.6040 - val_loss: 1.4799\n",
      "Epoch 34/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5720 - val_loss: 1.4577\n",
      "Epoch 35/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.5626 - val_loss: 1.5116\n",
      "Epoch 36/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5401 - val_loss: 1.4756\n",
      "Epoch 37/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5366 - val_loss: 1.5318\n",
      "Epoch 38/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5228 - val_loss: 1.4924\n",
      "Epoch 39/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5774 - val_loss: 1.4672\n",
      "Epoch 40/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5412 - val_loss: 1.4641\n",
      "Epoch 41/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5490 - val_loss: 1.4747\n",
      "Epoch 42/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4877 - val_loss: 1.4712\n",
      "Epoch 43/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5152 - val_loss: 1.4520\n",
      "Epoch 44/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5322 - val_loss: 1.5425\n",
      "Epoch 45/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4883 - val_loss: 1.5104\n",
      "Epoch 46/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5033 - val_loss: 1.4406\n",
      "Epoch 47/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4792 - val_loss: 1.5607\n",
      "Epoch 48/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5097 - val_loss: 1.5405\n",
      "Epoch 49/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4985 - val_loss: 1.4881\n",
      "Epoch 50/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.5056 - val_loss: 1.4453\n",
      "Epoch 51/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4630 - val_loss: 1.4601\n",
      "Epoch 52/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4456 - val_loss: 1.4986\n",
      "Epoch 53/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4262 - val_loss: 1.4627\n",
      "Epoch 54/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4587 - val_loss: 1.3992\n",
      "Epoch 55/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4600 - val_loss: 1.5698\n",
      "Epoch 56/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4401 - val_loss: 1.4657\n",
      "Epoch 57/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4384 - val_loss: 1.3826\n",
      "Epoch 58/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4587 - val_loss: 1.4243\n",
      "Epoch 59/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4560 - val_loss: 1.4319\n",
      "Epoch 60/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4424 - val_loss: 1.3643\n",
      "Epoch 61/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4435 - val_loss: 1.4711\n",
      "Epoch 62/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4301 - val_loss: 1.4954\n",
      "Epoch 63/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4311 - val_loss: 1.4252\n",
      "Epoch 64/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4712 - val_loss: 1.4246\n",
      "Epoch 65/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3918 - val_loss: 1.4243\n",
      "Epoch 66/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4046 - val_loss: 1.2985\n",
      "Epoch 67/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4442 - val_loss: 1.3730\n",
      "Epoch 68/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4397 - val_loss: 1.4463\n",
      "Epoch 69/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3806 - val_loss: 1.4045\n",
      "Epoch 70/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4085 - val_loss: 1.3858\n",
      "Epoch 71/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.4309 - val_loss: 1.2936\n",
      "Epoch 72/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3763 - val_loss: 1.2611\n",
      "Epoch 73/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.4362 - val_loss: 1.3837\n",
      "Epoch 74/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3720 - val_loss: 1.3588\n",
      "Epoch 75/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3800 - val_loss: 1.3540\n",
      "Epoch 76/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3386 - val_loss: 1.3255\n",
      "Epoch 77/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3881 - val_loss: 1.2503\n",
      "Epoch 78/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3323 - val_loss: 1.2969\n",
      "Epoch 79/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3433 - val_loss: 1.2337\n",
      "Epoch 80/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3761 - val_loss: 1.2708\n",
      "Epoch 81/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3519 - val_loss: 1.2985\n",
      "Epoch 82/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3266 - val_loss: 1.2863\n",
      "Epoch 83/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3286 - val_loss: 1.2261\n",
      "Epoch 84/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3165 - val_loss: 1.2371\n",
      "Epoch 85/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3008 - val_loss: 1.2745\n",
      "Epoch 86/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3545 - val_loss: 1.2001\n",
      "Epoch 87/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.3577 - val_loss: 1.1826\n",
      "Epoch 88/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3485 - val_loss: 1.1534\n",
      "Epoch 89/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3286 - val_loss: 1.1492\n",
      "Epoch 90/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3090 - val_loss: 1.1758\n",
      "Epoch 91/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3256 - val_loss: 1.1565\n",
      "Epoch 92/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2888 - val_loss: 1.1563\n",
      "Epoch 93/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3160 - val_loss: 1.2113\n",
      "Epoch 94/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3504 - val_loss: 1.1991\n",
      "Epoch 95/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2944 - val_loss: 1.1616\n",
      "Epoch 96/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3385 - val_loss: 1.1767\n",
      "Epoch 97/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2969 - val_loss: 1.1510\n",
      "Epoch 98/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2973 - val_loss: 1.1775\n",
      "Epoch 99/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2927 - val_loss: 1.1319\n",
      "Epoch 100/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2812 - val_loss: 1.0990\n",
      "Epoch 101/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3152 - val_loss: 1.1316\n",
      "Epoch 102/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2656 - val_loss: 1.1130\n",
      "Epoch 103/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3089 - val_loss: 1.1355\n",
      "Epoch 104/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2977 - val_loss: 1.1184\n",
      "Epoch 105/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2884 - val_loss: 1.1438\n",
      "Epoch 106/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2774 - val_loss: 1.1532\n",
      "Epoch 107/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2811 - val_loss: 1.1022\n",
      "Epoch 108/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2563 - val_loss: 1.1218\n",
      "Epoch 109/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3048 - val_loss: 1.1162\n",
      "Epoch 110/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2825 - val_loss: 1.1050\n",
      "Epoch 111/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2778 - val_loss: 1.0948\n",
      "Epoch 112/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2727 - val_loss: 1.0877\n",
      "Epoch 113/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2576 - val_loss: 1.0785\n",
      "Epoch 114/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.3066 - val_loss: 1.0806\n",
      "Epoch 115/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2291 - val_loss: 1.0785\n",
      "Epoch 116/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2514 - val_loss: 1.0659\n",
      "Epoch 117/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2522 - val_loss: 1.0565\n",
      "Epoch 118/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2916 - val_loss: 1.1074\n",
      "Epoch 119/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2721 - val_loss: 1.0793\n",
      "Epoch 120/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2764 - val_loss: 1.0819\n",
      "Epoch 121/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2346 - val_loss: 1.0665\n",
      "Epoch 122/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2316 - val_loss: 1.0303\n",
      "Epoch 123/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2382 - val_loss: 1.0226\n",
      "Epoch 124/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2421 - val_loss: 1.0492\n",
      "Epoch 125/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2252 - val_loss: 1.0847\n",
      "Epoch 126/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2344 - val_loss: 1.0367\n",
      "Epoch 127/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2553 - val_loss: 1.0482\n",
      "Epoch 128/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2653 - val_loss: 1.0289\n",
      "Epoch 129/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2509 - val_loss: 1.0249\n",
      "Epoch 130/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2241 - val_loss: 1.1066\n",
      "Epoch 131/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2183 - val_loss: 1.0520\n",
      "Epoch 132/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2314 - val_loss: 1.0090\n",
      "Epoch 133/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.2173 - val_loss: 1.0322\n",
      "Epoch 134/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2241 - val_loss: 1.0294\n",
      "Epoch 135/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2491 - val_loss: 1.0325\n",
      "Epoch 136/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2471 - val_loss: 1.0283\n",
      "Epoch 137/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2407 - val_loss: 1.0407\n",
      "Epoch 138/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2063 - val_loss: 1.0104\n",
      "Epoch 139/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2048 - val_loss: 1.0175\n",
      "Epoch 140/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2124 - val_loss: 1.0228\n",
      "Epoch 141/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1877 - val_loss: 1.0131\n",
      "Epoch 142/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2079 - val_loss: 1.0252\n",
      "Epoch 143/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1991 - val_loss: 0.9988\n",
      "Epoch 144/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2172 - val_loss: 0.9846\n",
      "Epoch 145/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1876 - val_loss: 1.0010\n",
      "Epoch 146/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1680 - val_loss: 1.0311\n",
      "Epoch 147/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1434 - val_loss: 0.9907\n",
      "Epoch 148/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2182 - val_loss: 0.9737\n",
      "Epoch 149/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2020 - val_loss: 1.0241\n",
      "Epoch 150/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2265 - val_loss: 0.9872\n",
      "Epoch 151/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1919 - val_loss: 1.0042\n",
      "Epoch 152/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1770 - val_loss: 1.0084\n",
      "Epoch 153/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1898 - val_loss: 0.9999\n",
      "Epoch 154/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1842 - val_loss: 0.9828\n",
      "Epoch 155/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1801 - val_loss: 0.9776\n",
      "Epoch 156/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1942 - val_loss: 1.0066\n",
      "Epoch 157/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1752 - val_loss: 0.9795\n",
      "Epoch 158/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1875 - val_loss: 0.9815\n",
      "Epoch 159/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.2111 - val_loss: 0.9870\n",
      "Epoch 160/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1960 - val_loss: 0.9629\n",
      "Epoch 161/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1512 - val_loss: 0.9828\n",
      "Epoch 162/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1771 - val_loss: 0.9696\n",
      "Epoch 163/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1646 - val_loss: 1.0147\n",
      "Epoch 164/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1620 - val_loss: 0.9723\n",
      "Epoch 165/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1779 - val_loss: 0.9692\n",
      "Epoch 166/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1681 - val_loss: 0.9909\n",
      "Epoch 167/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1453 - val_loss: 0.9522\n",
      "Epoch 168/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1923 - val_loss: 0.9843\n",
      "Epoch 169/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1739 - val_loss: 0.9746\n",
      "Epoch 170/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1862 - val_loss: 0.9637\n",
      "Epoch 171/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1529 - val_loss: 0.9631\n",
      "Epoch 172/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1494 - val_loss: 0.9525\n",
      "Epoch 173/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1412 - val_loss: 0.9480\n",
      "Epoch 174/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1459 - val_loss: 0.9321\n",
      "Epoch 175/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1791 - val_loss: 0.9417\n",
      "Epoch 176/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1584 - val_loss: 0.9772\n",
      "Epoch 177/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1428 - val_loss: 0.9610\n",
      "Epoch 178/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1648 - val_loss: 0.9390\n",
      "Epoch 179/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1583 - val_loss: 0.9528\n",
      "Epoch 180/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1618 - val_loss: 0.9542\n",
      "Epoch 181/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1484 - val_loss: 0.9479\n",
      "Epoch 182/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1451 - val_loss: 0.9374\n",
      "Epoch 183/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1267 - val_loss: 0.9375\n",
      "Epoch 184/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1485 - val_loss: 0.9307\n",
      "Epoch 185/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1506 - val_loss: 0.9401\n",
      "Epoch 186/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1650 - val_loss: 0.9388\n",
      "Epoch 187/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1551 - val_loss: 0.9389\n",
      "Epoch 188/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1591 - val_loss: 0.9294\n",
      "Epoch 189/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1216 - val_loss: 0.9269\n",
      "Epoch 190/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1825 - val_loss: 0.9305\n",
      "Epoch 191/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1567 - val_loss: 0.9388\n",
      "Epoch 192/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1547 - val_loss: 0.9491\n",
      "Epoch 193/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1308 - val_loss: 0.9420\n",
      "Epoch 194/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1227 - val_loss: 0.9297\n",
      "Epoch 195/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1368 - val_loss: 0.9271\n",
      "Epoch 196/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1509 - val_loss: 0.9346\n",
      "Epoch 197/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1269 - val_loss: 0.9236\n",
      "Epoch 198/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1495 - val_loss: 0.9514\n",
      "Epoch 199/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1388 - val_loss: 0.9262\n",
      "Epoch 200/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1370 - val_loss: 0.9411\n",
      "Epoch 201/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0966 - val_loss: 0.9229\n",
      "Epoch 202/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.1365 - val_loss: 0.9288\n",
      "Epoch 203/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1496 - val_loss: 0.9159\n",
      "Epoch 204/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1001 - val_loss: 0.9346\n",
      "Epoch 205/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1380 - val_loss: 0.9155\n",
      "Epoch 206/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1080 - val_loss: 0.9523\n",
      "Epoch 207/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1394 - val_loss: 0.9339\n",
      "Epoch 208/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1099 - val_loss: 0.9371\n",
      "Epoch 209/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1377 - val_loss: 0.9192\n",
      "Epoch 210/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1294 - val_loss: 0.9217\n",
      "Epoch 211/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1518 - val_loss: 0.9152\n",
      "Epoch 212/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1113 - val_loss: 0.9132\n",
      "Epoch 213/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0974 - val_loss: 0.9301\n",
      "Epoch 214/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1062 - val_loss: 0.9081\n",
      "Epoch 215/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1360 - val_loss: 0.9425\n",
      "Epoch 216/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0934 - val_loss: 0.9269\n",
      "Epoch 217/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1079 - val_loss: 0.9135\n",
      "Epoch 218/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.1079 - val_loss: 0.9337\n",
      "Epoch 219/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1068 - val_loss: 0.9254\n",
      "Epoch 220/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1469 - val_loss: 0.9027\n",
      "Epoch 221/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1085 - val_loss: 0.9140\n",
      "Epoch 222/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0955 - val_loss: 0.9122\n",
      "Epoch 223/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1007 - val_loss: 0.9278\n",
      "Epoch 224/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0998 - val_loss: 0.9143\n",
      "Epoch 225/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0965 - val_loss: 0.9670\n",
      "Epoch 226/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0995 - val_loss: 0.9107\n",
      "Epoch 227/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0806 - val_loss: 0.9017\n",
      "Epoch 228/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0974 - val_loss: 0.9040\n",
      "Epoch 229/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0930 - val_loss: 0.9124\n",
      "Epoch 230/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.0813 - val_loss: 0.9140\n",
      "Epoch 231/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1054 - val_loss: 0.9265\n",
      "Epoch 232/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1041 - val_loss: 0.9041\n",
      "Epoch 233/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0967 - val_loss: 0.9138\n",
      "Epoch 234/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0825 - val_loss: 0.9068\n",
      "Epoch 235/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0812 - val_loss: 0.9072\n",
      "Epoch 236/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0790 - val_loss: 0.9057\n",
      "Epoch 237/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1077 - val_loss: 0.9178\n",
      "Epoch 238/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1245 - val_loss: 0.9246\n",
      "Epoch 239/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1037 - val_loss: 0.9096\n",
      "Epoch 240/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0726 - val_loss: 0.9057\n",
      "Epoch 241/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0820 - val_loss: 0.9142\n",
      "Epoch 242/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1026 - val_loss: 0.9113\n",
      "Epoch 243/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0831 - val_loss: 0.9514\n",
      "Epoch 244/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0941 - val_loss: 0.9082\n",
      "Epoch 245/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0949 - val_loss: 0.9168\n",
      "Epoch 246/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0901 - val_loss: 0.8888\n",
      "Epoch 247/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0823 - val_loss: 0.8996\n",
      "Epoch 248/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0745 - val_loss: 0.8958\n",
      "Epoch 249/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0815 - val_loss: 0.9041\n",
      "Epoch 250/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0742 - val_loss: 0.9296\n",
      "Epoch 251/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0620 - val_loss: 0.9122\n",
      "Epoch 252/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0727 - val_loss: 0.9118\n",
      "Epoch 253/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0878 - val_loss: 0.9117\n",
      "Epoch 254/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0745 - val_loss: 0.9049\n",
      "Epoch 255/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1121 - val_loss: 0.9199\n",
      "Epoch 256/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0349 - val_loss: 0.8995\n",
      "Epoch 257/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0367 - val_loss: 0.9286\n",
      "Epoch 258/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0599 - val_loss: 0.8879\n",
      "Epoch 259/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0595 - val_loss: 0.9211\n",
      "Epoch 260/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0732 - val_loss: 0.8920\n",
      "Epoch 261/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0509 - val_loss: 0.8774\n",
      "Epoch 262/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0935 - val_loss: 0.8947\n",
      "Epoch 263/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0646 - val_loss: 0.8903\n",
      "Epoch 264/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0939 - val_loss: 0.8997\n",
      "Epoch 265/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0662 - val_loss: 0.8951\n",
      "Epoch 266/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0643 - val_loss: 0.8983\n",
      "Epoch 267/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0823 - val_loss: 0.8834\n",
      "Epoch 268/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0465 - val_loss: 0.8910\n",
      "Epoch 269/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0603 - val_loss: 0.9077\n",
      "Epoch 270/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0652 - val_loss: 0.8950\n",
      "Epoch 271/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0378 - val_loss: 0.8963\n",
      "Epoch 272/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0429 - val_loss: 0.9134\n",
      "Epoch 273/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0233 - val_loss: 0.9037\n",
      "Epoch 274/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0606 - val_loss: 0.8914\n",
      "Epoch 275/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0527 - val_loss: 0.9213\n",
      "Epoch 276/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0381 - val_loss: 0.8888\n",
      "Epoch 277/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0764 - val_loss: 0.8875\n",
      "Epoch 278/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0711 - val_loss: 0.8915\n",
      "Epoch 279/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0840 - val_loss: 0.8901\n",
      "Epoch 280/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0398 - val_loss: 0.9252\n",
      "Epoch 281/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0610 - val_loss: 0.9039\n",
      "Epoch 282/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0828 - val_loss: 0.8921\n",
      "Epoch 283/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0436 - val_loss: 0.8813\n",
      "Epoch 284/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0185 - val_loss: 0.8863\n",
      "Epoch 285/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0629 - val_loss: 0.8775\n",
      "Epoch 286/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0284 - val_loss: 0.8704\n",
      "Epoch 287/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.0358 - val_loss: 0.8761\n",
      "Epoch 288/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0468 - val_loss: 0.8742\n",
      "Epoch 289/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0414 - val_loss: 0.8880\n",
      "Epoch 290/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.0436 - val_loss: 0.8719\n",
      "Epoch 291/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0573 - val_loss: 0.8708\n",
      "Epoch 292/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0104 - val_loss: 0.8852\n",
      "Epoch 293/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9908 - val_loss: 0.8848\n",
      "Epoch 294/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0230 - val_loss: 0.8688\n",
      "Epoch 295/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0600 - val_loss: 0.9039\n",
      "Epoch 296/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9971 - val_loss: 0.8872\n",
      "Epoch 297/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0632 - val_loss: 0.8773\n",
      "Epoch 298/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0396 - val_loss: 0.8853\n",
      "Epoch 299/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0208 - val_loss: 0.8652\n",
      "Epoch 300/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0445 - val_loss: 0.8714\n",
      "Epoch 301/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0230 - val_loss: 0.8772\n",
      "Epoch 302/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0140 - val_loss: 0.8745\n",
      "Epoch 303/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0367 - val_loss: 0.8671\n",
      "Epoch 304/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0260 - val_loss: 0.8844\n",
      "Epoch 305/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0484 - val_loss: 0.8763\n",
      "Epoch 306/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9946 - val_loss: 0.8773\n",
      "Epoch 307/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0168 - val_loss: 0.8806\n",
      "Epoch 308/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0388 - val_loss: 0.8678\n",
      "Epoch 309/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0022 - val_loss: 0.8767\n",
      "Epoch 310/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0187 - val_loss: 0.8625\n",
      "Epoch 311/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0169 - val_loss: 0.8795\n",
      "Epoch 312/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0191 - val_loss: 0.8665\n",
      "Epoch 313/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0229 - val_loss: 0.8642\n",
      "Epoch 314/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0316 - val_loss: 0.8791\n",
      "Epoch 315/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0214 - val_loss: 0.8941\n",
      "Epoch 316/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0205 - val_loss: 0.8749\n",
      "Epoch 317/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0204 - val_loss: 0.8685\n",
      "Epoch 318/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0414 - val_loss: 0.9205\n",
      "Epoch 319/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0155 - val_loss: 0.8677\n",
      "Epoch 320/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0061 - val_loss: 0.8778\n",
      "Epoch 321/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9789 - val_loss: 0.8769\n",
      "Epoch 322/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0085 - val_loss: 0.8576\n",
      "Epoch 323/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0161 - val_loss: 0.9076\n",
      "Epoch 324/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0297 - val_loss: 0.8658\n",
      "Epoch 325/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0045 - val_loss: 0.8887\n",
      "Epoch 326/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0089 - val_loss: 0.8773\n",
      "Epoch 327/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0224 - val_loss: 0.8890\n",
      "Epoch 328/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0136 - val_loss: 0.8804\n",
      "Epoch 329/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9480 - val_loss: 0.8734\n",
      "Epoch 330/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0217 - val_loss: 0.8800\n",
      "Epoch 331/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9884 - val_loss: 0.8977\n",
      "Epoch 332/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0497 - val_loss: 0.8644\n",
      "Epoch 333/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0243 - val_loss: 0.8626\n",
      "Epoch 334/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9958 - val_loss: 0.8623\n",
      "Epoch 335/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0040 - val_loss: 0.8642\n",
      "Epoch 336/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0172 - val_loss: 0.8633\n",
      "Epoch 337/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0017 - val_loss: 0.9046\n",
      "Epoch 338/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9941 - val_loss: 0.8764\n",
      "Epoch 339/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.9841 - val_loss: 0.8663\n",
      "Epoch 340/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9737 - val_loss: 0.8832\n",
      "Epoch 341/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0161 - val_loss: 0.8810\n",
      "Epoch 342/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0165 - val_loss: 0.8496\n",
      "Epoch 343/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0067 - val_loss: 0.8794\n",
      "Epoch 344/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9995 - val_loss: 0.8756\n",
      "Epoch 345/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9889 - val_loss: 0.8664\n",
      "Epoch 346/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9839 - val_loss: 0.8564\n",
      "Epoch 347/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0018 - val_loss: 0.8790\n",
      "Epoch 348/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9640 - val_loss: 0.8650\n",
      "Epoch 349/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9805 - val_loss: 0.8758\n",
      "Epoch 350/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9851 - val_loss: 0.8706\n",
      "Epoch 351/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9754 - val_loss: 0.8794\n",
      "Epoch 352/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9979 - val_loss: 0.8630\n",
      "Epoch 353/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9992 - val_loss: 0.8769\n",
      "Epoch 354/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9687 - val_loss: 0.8844\n",
      "Epoch 355/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9737 - val_loss: 0.8701\n",
      "Epoch 356/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9976 - val_loss: 0.8730\n",
      "Epoch 357/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9867 - val_loss: 0.8675\n",
      "Epoch 358/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9781 - val_loss: 0.8705\n",
      "Epoch 359/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9429 - val_loss: 0.8838\n",
      "Epoch 360/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9706 - val_loss: 0.8854\n",
      "Epoch 361/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9610 - val_loss: 0.8742\n",
      "Epoch 362/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9967 - val_loss: 0.8856\n",
      "Epoch 363/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0042 - val_loss: 0.8545\n",
      "Epoch 364/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9590 - val_loss: 0.8743\n",
      "Epoch 365/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9888 - val_loss: 0.8790\n",
      "Epoch 366/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9652 - val_loss: 0.8663\n",
      "Epoch 367/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9655 - val_loss: 0.8685\n",
      "Epoch 368/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9850 - val_loss: 0.8525\n",
      "Epoch 369/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9571 - val_loss: 0.8553\n",
      "Epoch 370/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9795 - val_loss: 0.8559\n",
      "Epoch 371/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9690 - val_loss: 0.8614\n",
      "Epoch 372/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9415 - val_loss: 0.8660\n",
      "Epoch 373/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9960 - val_loss: 0.8665\n",
      "Epoch 374/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9672 - val_loss: 0.8758\n",
      "Epoch 375/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9848 - val_loss: 0.8608\n",
      "Epoch 376/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0006 - val_loss: 0.8685\n",
      "Epoch 377/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9401 - val_loss: 0.8644\n",
      "Epoch 378/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9644 - val_loss: 0.8640\n",
      "Epoch 379/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.0017 - val_loss: 0.8735\n",
      "Epoch 380/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9860 - val_loss: 0.8959\n",
      "Epoch 381/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9842 - val_loss: 0.8909\n",
      "Epoch 382/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9706 - val_loss: 0.8738\n",
      "Epoch 383/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9861 - val_loss: 0.8501\n",
      "Epoch 384/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9717 - val_loss: 0.8606\n",
      "Epoch 385/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9884 - val_loss: 0.8656\n",
      "Epoch 386/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9900 - val_loss: 0.8548\n",
      "Epoch 387/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9653 - val_loss: 0.8778\n",
      "Epoch 388/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9904 - val_loss: 0.8489\n",
      "Epoch 389/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9847 - val_loss: 0.8612\n",
      "Epoch 390/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9640 - val_loss: 0.8640\n",
      "Epoch 391/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9689 - val_loss: 0.8515\n",
      "Epoch 392/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9746 - val_loss: 0.8524\n",
      "Epoch 393/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.9426 - val_loss: 0.8618\n",
      "Epoch 394/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9761 - val_loss: 0.8611\n",
      "Epoch 395/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9672 - val_loss: 0.8813\n",
      "Epoch 396/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9769 - val_loss: 0.8727\n",
      "Epoch 397/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9601 - val_loss: 0.8529\n",
      "Epoch 398/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9596 - val_loss: 0.8533\n",
      "Epoch 399/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9460 - val_loss: 0.8573\n",
      "Epoch 400/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9708 - val_loss: 0.8464\n",
      "Epoch 401/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9494 - val_loss: 0.8672\n",
      "Epoch 402/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9418 - val_loss: 0.8644\n",
      "Epoch 403/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9588 - val_loss: 0.8906\n",
      "Epoch 404/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9633 - val_loss: 0.9027\n",
      "Epoch 405/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9590 - val_loss: 0.9131\n",
      "Epoch 406/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9655 - val_loss: 0.8546\n",
      "Epoch 407/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9581 - val_loss: 0.8609\n",
      "Epoch 408/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9573 - val_loss: 0.8605\n",
      "Epoch 409/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9485 - val_loss: 0.8610\n",
      "Epoch 410/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9766 - val_loss: 0.8658\n",
      "Epoch 411/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9825 - val_loss: 0.8500\n",
      "Epoch 412/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9807 - val_loss: 0.8511\n",
      "Epoch 413/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9629 - val_loss: 0.8966\n",
      "Epoch 414/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9577 - val_loss: 0.8511\n",
      "Epoch 415/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9888 - val_loss: 0.8571\n",
      "Epoch 416/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9380 - val_loss: 0.9115\n",
      "Epoch 417/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9823 - val_loss: 0.8681\n",
      "Epoch 418/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9650 - val_loss: 0.8588\n",
      "Epoch 419/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9453 - val_loss: 0.8623\n",
      "Epoch 420/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9496 - val_loss: 0.8672\n",
      "Epoch 421/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9766 - val_loss: 0.8706\n",
      "Epoch 422/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9572 - val_loss: 0.8493\n",
      "Epoch 423/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9527 - val_loss: 0.8620\n",
      "Epoch 424/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9413 - val_loss: 0.9024\n",
      "Epoch 425/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9568 - val_loss: 0.8556\n",
      "Epoch 426/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9709 - val_loss: 0.8508\n",
      "Epoch 427/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9310 - val_loss: 0.8532\n",
      "Epoch 428/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9574 - val_loss: 0.8647\n",
      "Epoch 429/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9550 - val_loss: 0.8726\n",
      "Epoch 430/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9363 - val_loss: 0.8595\n",
      "Epoch 431/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9500 - val_loss: 0.8618\n",
      "Epoch 432/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9322 - val_loss: 0.8412\n",
      "Epoch 433/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9232 - val_loss: 0.8502\n",
      "Epoch 434/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9646 - val_loss: 0.8547\n",
      "Epoch 435/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9491 - val_loss: 0.8360\n",
      "Epoch 436/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9074 - val_loss: 0.8541\n",
      "Epoch 437/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9557 - val_loss: 0.8484\n",
      "Epoch 438/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9594 - val_loss: 0.8547\n",
      "Epoch 439/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9682 - val_loss: 0.8461\n",
      "Epoch 440/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9314 - val_loss: 0.8485\n",
      "Epoch 441/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9997 - val_loss: 0.8529\n",
      "Epoch 442/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9519 - val_loss: 0.8551\n",
      "Epoch 443/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8974 - val_loss: 0.8634\n",
      "Epoch 444/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9342 - val_loss: 0.8532\n",
      "Epoch 445/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9443 - val_loss: 0.8492\n",
      "Epoch 446/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9234 - val_loss: 0.8590\n",
      "Epoch 447/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9297 - val_loss: 0.8511\n",
      "Epoch 448/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9255 - val_loss: 0.8496\n",
      "Epoch 449/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9542 - val_loss: 0.8394\n",
      "Epoch 450/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9023 - val_loss: 0.8439\n",
      "Epoch 451/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9343 - val_loss: 0.8534\n",
      "Epoch 452/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9258 - val_loss: 0.8460\n",
      "Epoch 453/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9335 - val_loss: 0.8514\n",
      "Epoch 454/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9540 - val_loss: 0.8426\n",
      "Epoch 455/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9094 - val_loss: 0.8479\n",
      "Epoch 456/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9208 - val_loss: 0.8695\n",
      "Epoch 457/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9202 - val_loss: 0.8452\n",
      "Epoch 458/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8993 - val_loss: 0.8419\n",
      "Epoch 459/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9014 - val_loss: 0.8652\n",
      "Epoch 460/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9359 - val_loss: 0.8469\n",
      "Epoch 461/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9151 - val_loss: 0.8499\n",
      "Epoch 462/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9264 - val_loss: 0.8748\n",
      "Epoch 463/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9156 - val_loss: 0.8653\n",
      "Epoch 464/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9157 - val_loss: 0.8555\n",
      "Epoch 465/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9426 - val_loss: 0.8957\n",
      "Epoch 466/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9208 - val_loss: 0.8470\n",
      "Epoch 467/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8790 - val_loss: 0.8771\n",
      "Epoch 468/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9082 - val_loss: 0.8540\n",
      "Epoch 469/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9458 - val_loss: 0.8770\n",
      "Epoch 470/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9320 - val_loss: 0.8568\n",
      "Epoch 471/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9143 - val_loss: 0.8643\n",
      "Epoch 472/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8911 - val_loss: 0.8555\n",
      "Epoch 473/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9193 - val_loss: 0.8438\n",
      "Epoch 474/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9474 - val_loss: 0.8612\n",
      "Epoch 475/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9060 - val_loss: 0.8604\n",
      "Epoch 476/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9434 - val_loss: 0.8792\n",
      "Epoch 477/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9275 - val_loss: 0.8659\n",
      "Epoch 478/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9239 - val_loss: 0.8793\n",
      "Epoch 479/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9210 - val_loss: 0.8508\n",
      "Epoch 480/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9603 - val_loss: 0.8564\n",
      "Epoch 481/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9243 - val_loss: 0.8377\n",
      "Epoch 482/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9115 - val_loss: 0.8429\n",
      "Epoch 483/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8883 - val_loss: 0.8565\n",
      "Epoch 484/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9082 - val_loss: 0.8552\n",
      "Epoch 485/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9046 - val_loss: 0.8310\n",
      "Epoch 486/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8967 - val_loss: 0.8398\n",
      "Epoch 487/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9362 - val_loss: 0.8478\n",
      "Epoch 488/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9188 - val_loss: 0.8539\n",
      "Epoch 489/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9264 - val_loss: 0.8320\n",
      "Epoch 490/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8994 - val_loss: 0.8703\n",
      "Epoch 491/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9189 - val_loss: 0.8495\n",
      "Epoch 492/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9077 - val_loss: 0.8473\n",
      "Epoch 493/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9108 - val_loss: 0.8557\n",
      "Epoch 494/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9090 - val_loss: 0.8444\n",
      "Epoch 495/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9273 - val_loss: 0.8468\n",
      "Epoch 496/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9220 - val_loss: 0.8404\n",
      "Epoch 497/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9385 - val_loss: 0.8600\n",
      "Epoch 498/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9176 - val_loss: 0.8509\n",
      "Epoch 499/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8764 - val_loss: 0.8571\n",
      "Epoch 500/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9066 - val_loss: 0.8619\n",
      "Epoch 501/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9188 - val_loss: 0.8476\n",
      "Epoch 502/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9067 - val_loss: 0.8410\n",
      "Epoch 503/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8878 - val_loss: 0.8342\n",
      "Epoch 504/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9046 - val_loss: 0.8752\n",
      "Epoch 505/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9026 - val_loss: 0.8418\n",
      "Epoch 506/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9335 - val_loss: 0.8421\n",
      "Epoch 507/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9438 - val_loss: 0.8335\n",
      "Epoch 508/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9355 - val_loss: 0.8372\n",
      "Epoch 509/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9288 - val_loss: 0.8404\n",
      "Epoch 510/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.9070 - val_loss: 0.8501\n",
      "Epoch 511/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9125 - val_loss: 0.8521\n",
      "Epoch 512/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9133 - val_loss: 0.8276\n",
      "Epoch 513/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8960 - val_loss: 0.8466\n",
      "Epoch 514/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9104 - val_loss: 0.8509\n",
      "Epoch 515/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8902 - val_loss: 0.8380\n",
      "Epoch 516/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9086 - val_loss: 0.8416\n",
      "Epoch 517/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9055 - val_loss: 0.8512\n",
      "Epoch 518/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8709 - val_loss: 0.8514\n",
      "Epoch 519/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8719 - val_loss: 0.8413\n",
      "Epoch 520/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9057 - val_loss: 0.8528\n",
      "Epoch 521/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9052 - val_loss: 0.8485\n",
      "Epoch 522/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8914 - val_loss: 0.8498\n",
      "Epoch 523/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9122 - val_loss: 0.8600\n",
      "Epoch 524/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9016 - val_loss: 0.8408\n",
      "Epoch 525/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8883 - val_loss: 0.8595\n",
      "Epoch 526/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8734 - val_loss: 0.8525\n",
      "Epoch 527/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9085 - val_loss: 0.8510\n",
      "Epoch 528/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8934 - val_loss: 0.8507\n",
      "Epoch 529/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8778 - val_loss: 0.8530\n",
      "Epoch 530/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9114 - val_loss: 0.8490\n",
      "Epoch 531/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8916 - val_loss: 0.8473\n",
      "Epoch 532/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9197 - val_loss: 0.8355\n",
      "Epoch 533/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9082 - val_loss: 0.8457\n",
      "Epoch 534/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8863 - val_loss: 0.8456\n",
      "Epoch 535/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8808 - val_loss: 0.8722\n",
      "Epoch 536/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8927 - val_loss: 0.8711\n",
      "Epoch 537/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9130 - val_loss: 0.8316\n",
      "Epoch 538/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8943 - val_loss: 0.8559\n",
      "Epoch 539/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8819 - val_loss: 0.8388\n",
      "Epoch 540/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9041 - val_loss: 0.8410\n",
      "Epoch 541/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9161 - val_loss: 0.8343\n",
      "Epoch 542/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8977 - val_loss: 0.8478\n",
      "Epoch 543/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9008 - val_loss: 0.8528\n",
      "Epoch 544/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8786 - val_loss: 0.8595\n",
      "Epoch 545/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8896 - val_loss: 0.8591\n",
      "Epoch 546/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9048 - val_loss: 0.8341\n",
      "Epoch 547/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8808 - val_loss: 0.8637\n",
      "Epoch 548/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9090 - val_loss: 0.8504\n",
      "Epoch 549/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8897 - val_loss: 0.8531\n",
      "Epoch 550/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8837 - val_loss: 0.8335\n",
      "Epoch 551/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8731 - val_loss: 0.8578\n",
      "Epoch 552/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9012 - val_loss: 0.8345\n",
      "Epoch 553/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8976 - val_loss: 0.8371\n",
      "Epoch 554/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8814 - val_loss: 0.8376\n",
      "Epoch 555/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8881 - val_loss: 0.8642\n",
      "Epoch 556/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8730 - val_loss: 0.8581\n",
      "Epoch 557/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8689 - val_loss: 0.8743\n",
      "Epoch 558/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8977 - val_loss: 0.8453\n",
      "Epoch 559/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8795 - val_loss: 0.8503\n",
      "Epoch 560/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8861 - val_loss: 0.8582\n",
      "Epoch 561/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8917 - val_loss: 0.8412\n",
      "Epoch 562/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8916 - val_loss: 0.8492\n",
      "Epoch 563/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8829 - val_loss: 0.8440\n",
      "Epoch 564/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9044 - val_loss: 0.8427\n",
      "Epoch 565/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8880 - val_loss: 0.8351\n",
      "Epoch 566/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8797 - val_loss: 0.8460\n",
      "Epoch 567/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8545 - val_loss: 0.8317\n",
      "Epoch 568/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8918 - val_loss: 0.8407\n",
      "Epoch 569/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8767 - val_loss: 0.8353\n",
      "Epoch 570/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8890 - val_loss: 0.8600\n",
      "Epoch 571/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9033 - val_loss: 0.8397\n",
      "Epoch 572/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8637 - val_loss: 0.8488\n",
      "Epoch 573/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8509 - val_loss: 0.8404\n",
      "Epoch 574/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8953 - val_loss: 0.8367\n",
      "Epoch 575/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8722 - val_loss: 0.8532\n",
      "Epoch 576/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8573 - val_loss: 0.8620\n",
      "Epoch 577/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8710 - val_loss: 0.8478\n",
      "Epoch 578/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8848 - val_loss: 0.8355\n",
      "Epoch 579/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8958 - val_loss: 0.8519\n",
      "Epoch 580/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.9061 - val_loss: 0.8344\n",
      "Epoch 581/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8693 - val_loss: 0.8341\n",
      "Epoch 582/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8533 - val_loss: 0.8335\n",
      "Epoch 583/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8578 - val_loss: 0.8252\n",
      "Epoch 584/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8747 - val_loss: 0.8737\n",
      "Epoch 585/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8834 - val_loss: 0.8198\n",
      "Epoch 586/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8634 - val_loss: 0.8298\n",
      "Epoch 587/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8655 - val_loss: 0.8534\n",
      "Epoch 588/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8689 - val_loss: 0.8463\n",
      "Epoch 589/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8885 - val_loss: 0.8304\n",
      "Epoch 590/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8412 - val_loss: 0.8532\n",
      "Epoch 591/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8823 - val_loss: 0.8523\n",
      "Epoch 592/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8322 - val_loss: 0.8411\n",
      "Epoch 593/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8644 - val_loss: 0.8509\n",
      "Epoch 594/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8725 - val_loss: 0.8505\n",
      "Epoch 595/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8524 - val_loss: 0.8294\n",
      "Epoch 596/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8524 - val_loss: 0.9007\n",
      "Epoch 597/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8619 - val_loss: 0.8416\n",
      "Epoch 598/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8699 - val_loss: 0.8141\n",
      "Epoch 599/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8462 - val_loss: 0.8225\n",
      "Epoch 600/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8459 - val_loss: 0.8431\n",
      "Epoch 601/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8520 - val_loss: 0.8418\n",
      "Epoch 602/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8450 - val_loss: 0.8539\n",
      "Epoch 603/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8727 - val_loss: 0.8664\n",
      "Epoch 604/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8612 - val_loss: 0.8416\n",
      "Epoch 605/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8798 - val_loss: 0.8543\n",
      "Epoch 606/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8669 - val_loss: 0.8573\n",
      "Epoch 607/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8760 - val_loss: 0.8516\n",
      "Epoch 608/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8673 - val_loss: 0.8306\n",
      "Epoch 609/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8293 - val_loss: 0.8278\n",
      "Epoch 610/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8383 - val_loss: 0.8257\n",
      "Epoch 611/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8349 - val_loss: 0.8300\n",
      "Epoch 612/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8670 - val_loss: 0.8513\n",
      "Epoch 613/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8707 - val_loss: 0.8603\n",
      "Epoch 614/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8388 - val_loss: 0.8289\n",
      "Epoch 615/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8673 - val_loss: 0.8419\n",
      "Epoch 616/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8498 - val_loss: 0.8381\n",
      "Epoch 617/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8585 - val_loss: 0.8609\n",
      "Epoch 618/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8537 - val_loss: 0.8473\n",
      "Epoch 619/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8477 - val_loss: 0.8547\n",
      "Epoch 620/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8663 - val_loss: 0.8421\n",
      "Epoch 621/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8782 - val_loss: 0.8248\n",
      "Epoch 622/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8726 - val_loss: 0.8402\n",
      "Epoch 623/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8637 - val_loss: 0.8411\n",
      "Epoch 624/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8472 - val_loss: 0.8445\n",
      "Epoch 625/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8595 - val_loss: 0.8493\n",
      "Epoch 626/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8535 - val_loss: 0.8377\n",
      "Epoch 627/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8474 - val_loss: 0.8255\n",
      "Epoch 628/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8553 - val_loss: 0.8481\n",
      "Epoch 629/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8426 - val_loss: 0.8569\n",
      "Epoch 630/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8523 - val_loss: 0.8497\n",
      "Epoch 631/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8285 - val_loss: 0.8393\n",
      "Epoch 632/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8671 - val_loss: 0.8442\n",
      "Epoch 633/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8503 - val_loss: 0.8332\n",
      "Epoch 634/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8229 - val_loss: 0.8310\n",
      "Epoch 635/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8560 - val_loss: 0.8601\n",
      "Epoch 636/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8469 - val_loss: 0.8637\n",
      "Epoch 637/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8626 - val_loss: 0.8609\n",
      "Epoch 638/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8618 - val_loss: 0.8588\n",
      "Epoch 639/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8517 - val_loss: 0.8275\n",
      "Epoch 640/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8423 - val_loss: 0.8321\n",
      "Epoch 641/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8227 - val_loss: 0.8224\n",
      "Epoch 642/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8392 - val_loss: 0.8513\n",
      "Epoch 643/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8766 - val_loss: 0.8254\n",
      "Epoch 644/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8408 - val_loss: 0.8351\n",
      "Epoch 645/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8376 - val_loss: 0.8576\n",
      "Epoch 646/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8268 - val_loss: 0.8415\n",
      "Epoch 647/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8269 - val_loss: 0.8512\n",
      "Epoch 648/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8581 - val_loss: 0.8496\n",
      "Epoch 649/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8366 - val_loss: 0.8275\n",
      "Epoch 650/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8379 - val_loss: 0.8635\n",
      "Epoch 651/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8558 - val_loss: 0.8395\n",
      "Epoch 652/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8754 - val_loss: 0.8272\n",
      "Epoch 653/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8318 - val_loss: 0.8631\n",
      "Epoch 654/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8632 - val_loss: 0.8375\n",
      "Epoch 655/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8245 - val_loss: 0.8450\n",
      "Epoch 656/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8443 - val_loss: 0.8309\n",
      "Epoch 657/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8633 - val_loss: 0.8465\n",
      "Epoch 658/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8271 - val_loss: 0.8566\n",
      "Epoch 659/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8738 - val_loss: 0.8261\n",
      "Epoch 660/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8352 - val_loss: 0.8349\n",
      "Epoch 661/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8648 - val_loss: 0.8710\n",
      "Epoch 662/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8490 - val_loss: 0.8480\n",
      "Epoch 663/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.8211 - val_loss: 0.8389\n",
      "Epoch 664/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8316 - val_loss: 0.8348\n",
      "Epoch 665/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8127 - val_loss: 0.8305\n",
      "Epoch 666/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8273 - val_loss: 0.8237\n",
      "Epoch 667/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8534 - val_loss: 0.8139\n",
      "Epoch 668/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8377 - val_loss: 0.8230\n",
      "Epoch 669/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.8175 - val_loss: 0.8225\n",
      "Epoch 670/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8199 - val_loss: 0.8753\n",
      "Epoch 671/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8315 - val_loss: 0.8437\n",
      "Epoch 672/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8217 - val_loss: 0.8249\n",
      "Epoch 673/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8297 - val_loss: 0.8166\n",
      "Epoch 674/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8368 - val_loss: 0.8568\n",
      "Epoch 675/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8422 - val_loss: 0.8139\n",
      "Epoch 676/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8377 - val_loss: 0.8494\n",
      "Epoch 677/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8097 - val_loss: 0.8168\n",
      "Epoch 678/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8235 - val_loss: 0.8280\n",
      "Epoch 679/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8266 - val_loss: 0.8361\n",
      "Epoch 680/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8323 - val_loss: 0.8485\n",
      "Epoch 681/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8359 - val_loss: 0.8464\n",
      "Epoch 682/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8325 - val_loss: 0.8249\n",
      "Epoch 683/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8345 - val_loss: 0.8291\n",
      "Epoch 684/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8494 - val_loss: 0.8786\n",
      "Epoch 685/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8464 - val_loss: 0.8376\n",
      "Epoch 686/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8299 - val_loss: 0.8332\n",
      "Epoch 687/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8351 - val_loss: 0.8275\n",
      "Epoch 688/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8338 - val_loss: 0.8208\n",
      "Epoch 689/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8350 - val_loss: 0.8849\n",
      "Epoch 690/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8389 - val_loss: 0.8499\n",
      "Epoch 691/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8597 - val_loss: 0.8363\n",
      "Epoch 692/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8423 - val_loss: 0.8527\n",
      "Epoch 693/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7972 - val_loss: 0.8261\n",
      "Epoch 694/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8001 - val_loss: 0.8353\n",
      "Epoch 695/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8308 - val_loss: 0.8183\n",
      "Epoch 696/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8461 - val_loss: 0.8154\n",
      "Epoch 697/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8252 - val_loss: 0.8311\n",
      "Epoch 698/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8295 - val_loss: 0.8289\n",
      "Epoch 699/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8429 - val_loss: 0.8184\n",
      "Epoch 700/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8148 - val_loss: 0.8366\n",
      "Epoch 701/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8350 - val_loss: 0.8278\n",
      "Epoch 702/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8582 - val_loss: 0.8387\n",
      "Epoch 703/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8125 - val_loss: 0.8197\n",
      "Epoch 704/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8376 - val_loss: 0.8587\n",
      "Epoch 705/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8010 - val_loss: 0.8177\n",
      "Epoch 706/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8106 - val_loss: 0.8436\n",
      "Epoch 707/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8070 - val_loss: 0.8226\n",
      "Epoch 708/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8045 - val_loss: 0.8298\n",
      "Epoch 709/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8220 - val_loss: 0.8474\n",
      "Epoch 710/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8207 - val_loss: 0.8213\n",
      "Epoch 711/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.8026 - val_loss: 0.8152\n",
      "Epoch 712/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8126 - val_loss: 0.8168\n",
      "Epoch 713/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8274 - val_loss: 0.8174\n",
      "Epoch 714/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8454 - val_loss: 0.8441\n",
      "Epoch 715/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8150 - val_loss: 0.8242\n",
      "Epoch 716/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8282 - val_loss: 0.8147\n",
      "Epoch 717/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8380 - val_loss: 0.8082\n",
      "Epoch 718/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8262 - val_loss: 0.8201\n",
      "Epoch 719/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8091 - val_loss: 0.8246\n",
      "Epoch 720/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8406 - val_loss: 0.8103\n",
      "Epoch 721/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8246 - val_loss: 0.8396\n",
      "Epoch 722/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8237 - val_loss: 0.8299\n",
      "Epoch 723/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8057 - val_loss: 0.8342\n",
      "Epoch 724/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8311 - val_loss: 0.8156\n",
      "Epoch 725/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8388 - val_loss: 0.8137\n",
      "Epoch 726/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8261 - val_loss: 0.8273\n",
      "Epoch 727/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8191 - val_loss: 0.8267\n",
      "Epoch 728/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8072 - val_loss: 0.8573\n",
      "Epoch 729/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8094 - val_loss: 0.8095\n",
      "Epoch 730/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8128 - val_loss: 0.8183\n",
      "Epoch 731/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8152 - val_loss: 0.8120\n",
      "Epoch 732/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8094 - val_loss: 0.8164\n",
      "Epoch 733/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8110 - val_loss: 0.8118\n",
      "Epoch 734/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8308 - val_loss: 0.8103\n",
      "Epoch 735/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8212 - val_loss: 0.8462\n",
      "Epoch 736/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8205 - val_loss: 0.8266\n",
      "Epoch 737/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7960 - val_loss: 0.8153\n",
      "Epoch 738/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8166 - val_loss: 0.8192\n",
      "Epoch 739/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8001 - val_loss: 0.8217\n",
      "Epoch 740/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7937 - val_loss: 0.8154\n",
      "Epoch 741/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7904 - val_loss: 0.8069\n",
      "Epoch 742/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7991 - val_loss: 0.8333\n",
      "Epoch 743/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8253 - val_loss: 0.8061\n",
      "Epoch 744/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8053 - val_loss: 0.8349\n",
      "Epoch 745/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8390 - val_loss: 0.8097\n",
      "Epoch 746/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8311 - val_loss: 0.8126\n",
      "Epoch 747/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8344 - val_loss: 0.8272\n",
      "Epoch 748/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8041 - val_loss: 0.8213\n",
      "Epoch 749/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8059 - val_loss: 0.8220\n",
      "Epoch 750/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8169 - val_loss: 0.8195\n",
      "Epoch 751/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7843 - val_loss: 0.8309\n",
      "Epoch 752/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7602 - val_loss: 0.8240\n",
      "Epoch 753/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8057 - val_loss: 0.8195\n",
      "Epoch 754/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7812 - val_loss: 0.8374\n",
      "Epoch 755/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8170 - val_loss: 0.8051\n",
      "Epoch 756/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8012 - val_loss: 0.8318\n",
      "Epoch 757/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7901 - val_loss: 0.8511\n",
      "Epoch 758/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8116 - val_loss: 0.8045\n",
      "Epoch 759/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8262 - val_loss: 0.8307\n",
      "Epoch 760/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7984 - val_loss: 0.8595\n",
      "Epoch 761/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7861 - val_loss: 0.8282\n",
      "Epoch 762/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7834 - val_loss: 0.8232\n",
      "Epoch 763/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7825 - val_loss: 0.8201\n",
      "Epoch 764/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8030 - val_loss: 0.8271\n",
      "Epoch 765/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8226 - val_loss: 0.8366\n",
      "Epoch 766/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.8094 - val_loss: 0.8133\n",
      "Epoch 767/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8210 - val_loss: 0.8187\n",
      "Epoch 768/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7910 - val_loss: 0.8236\n",
      "Epoch 769/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8132 - val_loss: 0.8394\n",
      "Epoch 770/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8050 - val_loss: 0.8041\n",
      "Epoch 771/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7890 - val_loss: 0.8192\n",
      "Epoch 772/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7705 - val_loss: 0.8211\n",
      "Epoch 773/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7868 - val_loss: 0.8162\n",
      "Epoch 774/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7966 - val_loss: 0.8038\n",
      "Epoch 775/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8003 - val_loss: 0.8252\n",
      "Epoch 776/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7746 - val_loss: 0.8323\n",
      "Epoch 777/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7957 - val_loss: 0.7935\n",
      "Epoch 778/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7957 - val_loss: 0.8074\n",
      "Epoch 779/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7667 - val_loss: 0.8073\n",
      "Epoch 780/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7818 - val_loss: 0.8225\n",
      "Epoch 781/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7717 - val_loss: 0.8155\n",
      "Epoch 782/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8269 - val_loss: 0.8224\n",
      "Epoch 783/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8299 - val_loss: 0.7996\n",
      "Epoch 784/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8121 - val_loss: 0.8069\n",
      "Epoch 785/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7989 - val_loss: 0.8277\n",
      "Epoch 786/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8128 - val_loss: 0.8397\n",
      "Epoch 787/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7841 - val_loss: 0.8055\n",
      "Epoch 788/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7857 - val_loss: 0.7933\n",
      "Epoch 789/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7915 - val_loss: 0.7950\n",
      "Epoch 790/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7944 - val_loss: 0.8291\n",
      "Epoch 791/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7737 - val_loss: 0.8257\n",
      "Epoch 792/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7771 - val_loss: 0.8135\n",
      "Epoch 793/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8107 - val_loss: 0.7845\n",
      "Epoch 794/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7957 - val_loss: 0.8025\n",
      "Epoch 795/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7991 - val_loss: 0.7956\n",
      "Epoch 796/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7796 - val_loss: 0.7953\n",
      "Epoch 797/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8051 - val_loss: 0.8191\n",
      "Epoch 798/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7786 - val_loss: 0.8462\n",
      "Epoch 799/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7909 - val_loss: 0.8169\n",
      "Epoch 800/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8085 - val_loss: 0.8026\n",
      "Epoch 801/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8031 - val_loss: 0.8058\n",
      "Epoch 802/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7704 - val_loss: 0.8056\n",
      "Epoch 803/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7685 - val_loss: 0.8025\n",
      "Epoch 804/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7739 - val_loss: 0.8135\n",
      "Epoch 805/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7983 - val_loss: 0.7966\n",
      "Epoch 806/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7918 - val_loss: 0.8013\n",
      "Epoch 807/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7933 - val_loss: 0.7930\n",
      "Epoch 808/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.8269 - val_loss: 0.8200\n",
      "Epoch 809/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7410 - val_loss: 0.8349\n",
      "Epoch 810/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7853 - val_loss: 0.8116\n",
      "Epoch 811/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7831 - val_loss: 0.8161\n",
      "Epoch 812/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7755 - val_loss: 0.8101\n",
      "Epoch 813/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7886 - val_loss: 0.8004\n",
      "Epoch 814/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7775 - val_loss: 0.8162\n",
      "Epoch 815/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7964 - val_loss: 0.8219\n",
      "Epoch 816/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7920 - val_loss: 0.7969\n",
      "Epoch 817/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7851 - val_loss: 0.8440\n",
      "Epoch 818/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7815 - val_loss: 0.8079\n",
      "Epoch 819/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7994 - val_loss: 0.8290\n",
      "Epoch 820/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7674 - val_loss: 0.8229\n",
      "Epoch 821/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7838 - val_loss: 0.7913\n",
      "Epoch 822/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7978 - val_loss: 0.8108\n",
      "Epoch 823/1200\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.7903 - val_loss: 0.8113\n",
      "Epoch 824/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7819 - val_loss: 0.8082\n",
      "Epoch 825/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.8059 - val_loss: 0.8019\n",
      "Epoch 826/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7538 - val_loss: 0.8204\n",
      "Epoch 827/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7874 - val_loss: 0.8410\n",
      "Epoch 828/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7562 - val_loss: 0.8165\n",
      "Epoch 829/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7998 - val_loss: 0.8001\n",
      "Epoch 830/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7633 - val_loss: 0.8022\n",
      "Epoch 831/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7813 - val_loss: 0.8023\n",
      "Epoch 832/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7759 - val_loss: 0.8075\n",
      "Epoch 833/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7949 - val_loss: 0.8055\n",
      "Epoch 834/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7541 - val_loss: 0.8089\n",
      "Epoch 835/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7869 - val_loss: 0.8011\n",
      "Epoch 836/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7866 - val_loss: 0.8246\n",
      "Epoch 837/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7847 - val_loss: 0.8208\n",
      "Epoch 838/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7943 - val_loss: 0.7940\n",
      "Epoch 839/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7688 - val_loss: 0.8123\n",
      "Epoch 840/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7606 - val_loss: 0.8070\n",
      "Epoch 841/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7696 - val_loss: 0.8211\n",
      "Epoch 842/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7811 - val_loss: 0.8128\n",
      "Epoch 843/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7671 - val_loss: 0.8300\n",
      "Epoch 844/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7936 - val_loss: 0.7974\n",
      "Epoch 845/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.8011 - val_loss: 0.8153\n",
      "Epoch 846/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7749 - val_loss: 0.8133\n",
      "Epoch 847/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7919 - val_loss: 0.8099\n",
      "Epoch 848/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7895 - val_loss: 0.8172\n",
      "Epoch 849/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7837 - val_loss: 0.7844\n",
      "Epoch 850/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7755 - val_loss: 0.8192\n",
      "Epoch 851/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7801 - val_loss: 0.8109\n",
      "Epoch 852/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7770 - val_loss: 0.8135\n",
      "Epoch 853/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7518 - val_loss: 0.8167\n",
      "Epoch 854/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7627 - val_loss: 0.8053\n",
      "Epoch 855/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7580 - val_loss: 0.8268\n",
      "Epoch 856/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7626 - val_loss: 0.8108\n",
      "Epoch 857/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7620 - val_loss: 0.8065\n",
      "Epoch 858/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7867 - val_loss: 0.7988\n",
      "Epoch 859/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7840 - val_loss: 0.8142\n",
      "Epoch 860/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7661 - val_loss: 0.8130\n",
      "Epoch 861/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7648 - val_loss: 0.8105\n",
      "Epoch 862/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7724 - val_loss: 0.8085\n",
      "Epoch 863/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7924 - val_loss: 0.8120\n",
      "Epoch 864/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7661 - val_loss: 0.7999\n",
      "Epoch 865/1200\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.7849 - val_loss: 0.8151\n",
      "Epoch 866/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7542 - val_loss: 0.8404\n",
      "Epoch 867/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7644 - val_loss: 0.8019\n",
      "Epoch 868/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7642 - val_loss: 0.8236\n",
      "Epoch 869/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7767 - val_loss: 0.7980\n",
      "Epoch 870/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7558 - val_loss: 0.8205\n",
      "Epoch 871/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7604 - val_loss: 0.8110\n",
      "Epoch 872/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7853 - val_loss: 0.8500\n",
      "Epoch 873/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7672 - val_loss: 0.8537\n",
      "Epoch 874/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7804 - val_loss: 0.8242\n",
      "Epoch 875/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7686 - val_loss: 0.7869\n",
      "Epoch 876/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7722 - val_loss: 0.8186\n",
      "Epoch 877/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7531 - val_loss: 0.7937\n",
      "Epoch 878/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7713 - val_loss: 0.8046\n",
      "Epoch 879/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7679 - val_loss: 0.8093\n",
      "Epoch 880/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7570 - val_loss: 0.7913\n",
      "Epoch 881/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7722 - val_loss: 0.7946\n",
      "Epoch 882/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7706 - val_loss: 0.7910\n",
      "Epoch 883/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7367 - val_loss: 0.7962\n",
      "Epoch 884/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7624 - val_loss: 0.7986\n",
      "Epoch 885/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7795 - val_loss: 0.8067\n",
      "Epoch 886/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7585 - val_loss: 0.8239\n",
      "Epoch 887/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7733 - val_loss: 0.8035\n",
      "Epoch 888/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7763 - val_loss: 0.7900\n",
      "Epoch 889/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7670 - val_loss: 0.8046\n",
      "Epoch 890/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7507 - val_loss: 0.8286\n",
      "Epoch 891/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7476 - val_loss: 0.8080\n",
      "Epoch 892/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7555 - val_loss: 0.7913\n",
      "Epoch 893/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7471 - val_loss: 0.8088\n",
      "Epoch 894/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7434 - val_loss: 0.8221\n",
      "Epoch 895/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7539 - val_loss: 0.8052\n",
      "Epoch 896/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7615 - val_loss: 0.7858\n",
      "Epoch 897/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7553 - val_loss: 0.7934\n",
      "Epoch 898/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7650 - val_loss: 0.8057\n",
      "Epoch 899/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7587 - val_loss: 0.7809\n",
      "Epoch 900/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7727 - val_loss: 0.8090\n",
      "Epoch 901/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7562 - val_loss: 0.8131\n",
      "Epoch 902/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7763 - val_loss: 0.8076\n",
      "Epoch 903/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7665 - val_loss: 0.8008\n",
      "Epoch 904/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7527 - val_loss: 0.7903\n",
      "Epoch 905/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7447 - val_loss: 0.8149\n",
      "Epoch 906/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7632 - val_loss: 0.7958\n",
      "Epoch 907/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7259 - val_loss: 0.7847\n",
      "Epoch 908/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7339 - val_loss: 0.7975\n",
      "Epoch 909/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7336 - val_loss: 0.8105\n",
      "Epoch 910/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7414 - val_loss: 0.7875\n",
      "Epoch 911/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7643 - val_loss: 0.7855\n",
      "Epoch 912/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7659 - val_loss: 0.8088\n",
      "Epoch 913/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7647 - val_loss: 0.8363\n",
      "Epoch 914/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7467 - val_loss: 0.7947\n",
      "Epoch 915/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7850 - val_loss: 0.7872\n",
      "Epoch 916/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7463 - val_loss: 0.8038\n",
      "Epoch 917/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7650 - val_loss: 0.8046\n",
      "Epoch 918/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7680 - val_loss: 0.8081\n",
      "Epoch 919/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7540 - val_loss: 0.8013\n",
      "Epoch 920/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7276 - val_loss: 0.8049\n",
      "Epoch 921/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7420 - val_loss: 0.7900\n",
      "Epoch 922/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7613 - val_loss: 0.8036\n",
      "Epoch 923/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7415 - val_loss: 0.8205\n",
      "Epoch 924/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7496 - val_loss: 0.8209\n",
      "Epoch 925/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7525 - val_loss: 0.7905\n",
      "Epoch 926/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7845 - val_loss: 0.7820\n",
      "Epoch 927/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7134 - val_loss: 0.7949\n",
      "Epoch 928/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7377 - val_loss: 0.7835\n",
      "Epoch 929/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7342 - val_loss: 0.7948\n",
      "Epoch 930/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7454 - val_loss: 0.8134\n",
      "Epoch 931/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7168 - val_loss: 0.8198\n",
      "Epoch 932/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7318 - val_loss: 0.8373\n",
      "Epoch 933/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7362 - val_loss: 0.7899\n",
      "Epoch 934/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7349 - val_loss: 0.7969\n",
      "Epoch 935/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7089 - val_loss: 0.7904\n",
      "Epoch 936/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7323 - val_loss: 0.8045\n",
      "Epoch 937/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7554 - val_loss: 0.8108\n",
      "Epoch 938/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7225 - val_loss: 0.8285\n",
      "Epoch 939/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7286 - val_loss: 0.8157\n",
      "Epoch 940/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7605 - val_loss: 0.7910\n",
      "Epoch 941/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7142 - val_loss: 0.8328\n",
      "Epoch 942/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7121 - val_loss: 0.7805\n",
      "Epoch 943/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7101 - val_loss: 0.7988\n",
      "Epoch 944/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7482 - val_loss: 0.7995\n",
      "Epoch 945/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7491 - val_loss: 0.7809\n",
      "Epoch 946/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7439 - val_loss: 0.7930\n",
      "Epoch 947/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7627 - val_loss: 0.8067\n",
      "Epoch 948/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7320 - val_loss: 0.7897\n",
      "Epoch 949/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7356 - val_loss: 0.8038\n",
      "Epoch 950/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7385 - val_loss: 0.7834\n",
      "Epoch 951/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7517 - val_loss: 0.7862\n",
      "Epoch 952/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7150 - val_loss: 0.7992\n",
      "Epoch 953/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7503 - val_loss: 0.8222\n",
      "Epoch 954/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7439 - val_loss: 0.7732\n",
      "Epoch 955/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7271 - val_loss: 0.8134\n",
      "Epoch 956/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7291 - val_loss: 0.7911\n",
      "Epoch 957/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7490 - val_loss: 0.7963\n",
      "Epoch 958/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7089 - val_loss: 0.8014\n",
      "Epoch 959/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7333 - val_loss: 0.7937\n",
      "Epoch 960/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7246 - val_loss: 0.7962\n",
      "Epoch 961/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7174 - val_loss: 0.7809\n",
      "Epoch 962/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7363 - val_loss: 0.7841\n",
      "Epoch 963/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7392 - val_loss: 0.7780\n",
      "Epoch 964/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7271 - val_loss: 0.7875\n",
      "Epoch 965/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7205 - val_loss: 0.7990\n",
      "Epoch 966/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7443 - val_loss: 0.7876\n",
      "Epoch 967/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7272 - val_loss: 0.7906\n",
      "Epoch 968/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7293 - val_loss: 0.7931\n",
      "Epoch 969/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7275 - val_loss: 0.8178\n",
      "Epoch 970/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7346 - val_loss: 0.8082\n",
      "Epoch 971/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7352 - val_loss: 0.7912\n",
      "Epoch 972/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7297 - val_loss: 0.7998\n",
      "Epoch 973/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7108 - val_loss: 0.8062\n",
      "Epoch 974/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7208 - val_loss: 0.7955\n",
      "Epoch 975/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7157 - val_loss: 0.8063\n",
      "Epoch 976/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7292 - val_loss: 0.8011\n",
      "Epoch 977/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7127 - val_loss: 0.8164\n",
      "Epoch 978/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7189 - val_loss: 0.7950\n",
      "Epoch 979/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7277 - val_loss: 0.7758\n",
      "Epoch 980/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7431 - val_loss: 0.8109\n",
      "Epoch 981/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7208 - val_loss: 0.8109\n",
      "Epoch 982/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7242 - val_loss: 0.8138\n",
      "Epoch 983/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7245 - val_loss: 0.7917\n",
      "Epoch 984/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7161 - val_loss: 0.7790\n",
      "Epoch 985/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7031 - val_loss: 0.7796\n",
      "Epoch 986/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7294 - val_loss: 0.8123\n",
      "Epoch 987/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7351 - val_loss: 0.8056\n",
      "Epoch 988/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7377 - val_loss: 0.7785\n",
      "Epoch 989/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7176 - val_loss: 0.7903\n",
      "Epoch 990/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7204 - val_loss: 0.7847\n",
      "Epoch 991/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7245 - val_loss: 0.7715\n",
      "Epoch 992/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7430 - val_loss: 0.7715\n",
      "Epoch 993/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7360 - val_loss: 0.7883\n",
      "Epoch 994/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7628 - val_loss: 0.7852\n",
      "Epoch 995/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7375 - val_loss: 0.7687\n",
      "Epoch 996/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7360 - val_loss: 0.7861\n",
      "Epoch 997/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7275 - val_loss: 0.7940\n",
      "Epoch 998/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7128 - val_loss: 0.8049\n",
      "Epoch 999/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7185 - val_loss: 0.8008\n",
      "Epoch 1000/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7268 - val_loss: 0.7859\n",
      "Epoch 1001/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7034 - val_loss: 0.7841\n",
      "Epoch 1002/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7447 - val_loss: 0.8043\n",
      "Epoch 1003/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7336 - val_loss: 0.7868\n",
      "Epoch 1004/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7154 - val_loss: 0.8238\n",
      "Epoch 1005/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7101 - val_loss: 0.8007\n",
      "Epoch 1006/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7181 - val_loss: 0.8155\n",
      "Epoch 1007/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7265 - val_loss: 0.8054\n",
      "Epoch 1008/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7015 - val_loss: 0.8060\n",
      "Epoch 1009/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7112 - val_loss: 0.7952\n",
      "Epoch 1010/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6972 - val_loss: 0.7998\n",
      "Epoch 1011/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7243 - val_loss: 0.7795\n",
      "Epoch 1012/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7308 - val_loss: 0.7772\n",
      "Epoch 1013/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7190 - val_loss: 0.7919\n",
      "Epoch 1014/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7563 - val_loss: 0.7763\n",
      "Epoch 1015/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7429 - val_loss: 0.7816\n",
      "Epoch 1016/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7047 - val_loss: 0.7743\n",
      "Epoch 1017/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7338 - val_loss: 0.7867\n",
      "Epoch 1018/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7006 - val_loss: 0.7928\n",
      "Epoch 1019/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7143 - val_loss: 0.8011\n",
      "Epoch 1020/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7367 - val_loss: 0.8123\n",
      "Epoch 1021/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6957 - val_loss: 0.7837\n",
      "Epoch 1022/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7026 - val_loss: 0.8052\n",
      "Epoch 1023/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7203 - val_loss: 0.7833\n",
      "Epoch 1024/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7060 - val_loss: 0.7933\n",
      "Epoch 1025/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7299 - val_loss: 0.8199\n",
      "Epoch 1026/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7298 - val_loss: 0.8024\n",
      "Epoch 1027/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7295 - val_loss: 0.8144\n",
      "Epoch 1028/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7050 - val_loss: 0.8167\n",
      "Epoch 1029/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6995 - val_loss: 0.7916\n",
      "Epoch 1030/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6882 - val_loss: 0.8473\n",
      "Epoch 1031/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7016 - val_loss: 0.8057\n",
      "Epoch 1032/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7095 - val_loss: 0.8035\n",
      "Epoch 1033/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7173 - val_loss: 0.7932\n",
      "Epoch 1034/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7196 - val_loss: 0.7641\n",
      "Epoch 1035/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7196 - val_loss: 0.7941\n",
      "Epoch 1036/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6911 - val_loss: 0.8099\n",
      "Epoch 1037/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7024 - val_loss: 0.7737\n",
      "Epoch 1038/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7426 - val_loss: 0.7864\n",
      "Epoch 1039/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7108 - val_loss: 0.8025\n",
      "Epoch 1040/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6877 - val_loss: 0.7802\n",
      "Epoch 1041/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7165 - val_loss: 0.7752\n",
      "Epoch 1042/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7181 - val_loss: 0.7918\n",
      "Epoch 1043/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6940 - val_loss: 0.8035\n",
      "Epoch 1044/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7214 - val_loss: 0.7875\n",
      "Epoch 1045/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6719 - val_loss: 0.7742\n",
      "Epoch 1046/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7199 - val_loss: 0.7925\n",
      "Epoch 1047/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7131 - val_loss: 0.8013\n",
      "Epoch 1048/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7163 - val_loss: 0.7987\n",
      "Epoch 1049/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6978 - val_loss: 0.7883\n",
      "Epoch 1050/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7186 - val_loss: 0.7973\n",
      "Epoch 1051/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6946 - val_loss: 0.7783\n",
      "Epoch 1052/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7119 - val_loss: 0.7920\n",
      "Epoch 1053/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7007 - val_loss: 0.7788\n",
      "Epoch 1054/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7087 - val_loss: 0.8056\n",
      "Epoch 1055/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6908 - val_loss: 0.7874\n",
      "Epoch 1056/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7062 - val_loss: 0.7965\n",
      "Epoch 1057/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6852 - val_loss: 0.7764\n",
      "Epoch 1058/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6996 - val_loss: 0.7743\n",
      "Epoch 1059/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7086 - val_loss: 0.7818\n",
      "Epoch 1060/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6977 - val_loss: 0.7802\n",
      "Epoch 1061/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7171 - val_loss: 0.7641\n",
      "Epoch 1062/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7145 - val_loss: 0.7871\n",
      "Epoch 1063/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6671 - val_loss: 0.7861\n",
      "Epoch 1064/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7270 - val_loss: 0.7938\n",
      "Epoch 1065/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7177 - val_loss: 0.7937\n",
      "Epoch 1066/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7293 - val_loss: 0.7687\n",
      "Epoch 1067/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6947 - val_loss: 0.7864\n",
      "Epoch 1068/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7179 - val_loss: 0.7831\n",
      "Epoch 1069/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6930 - val_loss: 0.7815\n",
      "Epoch 1070/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6991 - val_loss: 0.7818\n",
      "Epoch 1071/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7037 - val_loss: 0.8095\n",
      "Epoch 1072/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7239 - val_loss: 0.7987\n",
      "Epoch 1073/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6869 - val_loss: 0.7867\n",
      "Epoch 1074/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6831 - val_loss: 0.8103\n",
      "Epoch 1075/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6745 - val_loss: 0.8047\n",
      "Epoch 1076/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7038 - val_loss: 0.7854\n",
      "Epoch 1077/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7046 - val_loss: 0.7884\n",
      "Epoch 1078/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6999 - val_loss: 0.7778\n",
      "Epoch 1079/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7098 - val_loss: 0.7833\n",
      "Epoch 1080/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6821 - val_loss: 0.8095\n",
      "Epoch 1081/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7073 - val_loss: 0.7897\n",
      "Epoch 1082/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6980 - val_loss: 0.8084\n",
      "Epoch 1083/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7236 - val_loss: 0.7735\n",
      "Epoch 1084/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6950 - val_loss: 0.7817\n",
      "Epoch 1085/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6906 - val_loss: 0.7914\n",
      "Epoch 1086/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7139 - val_loss: 0.7695\n",
      "Epoch 1087/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6859 - val_loss: 0.7745\n",
      "Epoch 1088/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6884 - val_loss: 0.7866\n",
      "Epoch 1089/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6911 - val_loss: 0.7892\n",
      "Epoch 1090/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7137 - val_loss: 0.7728\n",
      "Epoch 1091/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6991 - val_loss: 0.7965\n",
      "Epoch 1092/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6933 - val_loss: 0.7921\n",
      "Epoch 1093/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7104 - val_loss: 0.7824\n",
      "Epoch 1094/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7031 - val_loss: 0.7799\n",
      "Epoch 1095/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6928 - val_loss: 0.7802\n",
      "Epoch 1096/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7027 - val_loss: 0.7925\n",
      "Epoch 1097/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6935 - val_loss: 0.7781\n",
      "Epoch 1098/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6988 - val_loss: 0.7971\n",
      "Epoch 1099/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7008 - val_loss: 0.7921\n",
      "Epoch 1100/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6981 - val_loss: 0.8062\n",
      "Epoch 1101/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6939 - val_loss: 0.7875\n",
      "Epoch 1102/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6737 - val_loss: 0.7881\n",
      "Epoch 1103/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6808 - val_loss: 0.7903\n",
      "Epoch 1104/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6789 - val_loss: 0.7804\n",
      "Epoch 1105/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6862 - val_loss: 0.7810\n",
      "Epoch 1106/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7055 - val_loss: 0.7849\n",
      "Epoch 1107/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6902 - val_loss: 0.7888\n",
      "Epoch 1108/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6857 - val_loss: 0.7793\n",
      "Epoch 1109/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6820 - val_loss: 0.7822\n",
      "Epoch 1110/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7142 - val_loss: 0.8085\n",
      "Epoch 1111/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6835 - val_loss: 0.7933\n",
      "Epoch 1112/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7136 - val_loss: 0.7957\n",
      "Epoch 1113/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7102 - val_loss: 0.7738\n",
      "Epoch 1114/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6710 - val_loss: 0.7713\n",
      "Epoch 1115/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6824 - val_loss: 0.7784\n",
      "Epoch 1116/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6901 - val_loss: 0.7876\n",
      "Epoch 1117/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7063 - val_loss: 0.8073\n",
      "Epoch 1118/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7121 - val_loss: 0.7921\n",
      "Epoch 1119/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6873 - val_loss: 0.7770\n",
      "Epoch 1120/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6914 - val_loss: 0.8018\n",
      "Epoch 1121/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6883 - val_loss: 0.8117\n",
      "Epoch 1122/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6749 - val_loss: 0.7733\n",
      "Epoch 1123/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6800 - val_loss: 0.7785\n",
      "Epoch 1124/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6819 - val_loss: 0.7667\n",
      "Epoch 1125/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6815 - val_loss: 0.8035\n",
      "Epoch 1126/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6539 - val_loss: 0.7952\n",
      "Epoch 1127/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6623 - val_loss: 0.7646\n",
      "Epoch 1128/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7000 - val_loss: 0.7566\n",
      "Epoch 1129/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6966 - val_loss: 0.7928\n",
      "Epoch 1130/1200\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.6657 - val_loss: 0.7709\n",
      "Epoch 1131/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6694 - val_loss: 0.7983\n",
      "Epoch 1132/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6799 - val_loss: 0.7763\n",
      "Epoch 1133/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6902 - val_loss: 0.7740\n",
      "Epoch 1134/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6869 - val_loss: 0.7737\n",
      "Epoch 1135/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7055 - val_loss: 0.8250\n",
      "Epoch 1136/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6801 - val_loss: 0.7541\n",
      "Epoch 1137/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6831 - val_loss: 0.7888\n",
      "Epoch 1138/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6651 - val_loss: 0.7919\n",
      "Epoch 1139/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6619 - val_loss: 0.7677\n",
      "Epoch 1140/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6910 - val_loss: 0.7569\n",
      "Epoch 1141/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6796 - val_loss: 0.7751\n",
      "Epoch 1142/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6996 - val_loss: 0.7943\n",
      "Epoch 1143/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6704 - val_loss: 0.7772\n",
      "Epoch 1144/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.7138 - val_loss: 0.7955\n",
      "Epoch 1145/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6713 - val_loss: 0.7815\n",
      "Epoch 1146/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6653 - val_loss: 0.7788\n",
      "Epoch 1147/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6374 - val_loss: 0.7811\n",
      "Epoch 1148/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7016 - val_loss: 0.7817\n",
      "Epoch 1149/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6759 - val_loss: 0.7949\n",
      "Epoch 1150/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6911 - val_loss: 0.7875\n",
      "Epoch 1151/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6837 - val_loss: 0.7808\n",
      "Epoch 1152/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6709 - val_loss: 0.7808\n",
      "Epoch 1153/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6930 - val_loss: 0.7663\n",
      "Epoch 1154/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6765 - val_loss: 0.7825\n",
      "Epoch 1155/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6889 - val_loss: 0.7719\n",
      "Epoch 1156/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6735 - val_loss: 0.7845\n",
      "Epoch 1157/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6788 - val_loss: 0.7846\n",
      "Epoch 1158/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6572 - val_loss: 0.8088\n",
      "Epoch 1159/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6823 - val_loss: 0.8073\n",
      "Epoch 1160/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6856 - val_loss: 0.7785\n",
      "Epoch 1161/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7028 - val_loss: 0.7627\n",
      "Epoch 1162/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6695 - val_loss: 0.7700\n",
      "Epoch 1163/1200\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.6639 - val_loss: 0.7825\n",
      "Epoch 1164/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6777 - val_loss: 0.8066\n",
      "Epoch 1165/1200\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.6881 - val_loss: 0.7603\n",
      "Epoch 1166/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6804 - val_loss: 0.7686\n",
      "Epoch 1167/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6679 - val_loss: 0.7856\n",
      "Epoch 1168/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6604 - val_loss: 0.7805\n",
      "Epoch 1169/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6869 - val_loss: 0.7977\n",
      "Epoch 1170/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6855 - val_loss: 0.7925\n",
      "Epoch 1171/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6797 - val_loss: 0.7798\n",
      "Epoch 1172/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6557 - val_loss: 0.7759\n",
      "Epoch 1173/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6694 - val_loss: 0.7546\n",
      "Epoch 1174/1200\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6837 - val_loss: 0.7584\n",
      "Epoch 1175/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6550 - val_loss: 0.7653\n",
      "Epoch 1176/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6861 - val_loss: 0.7829\n",
      "Epoch 1177/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6802 - val_loss: 0.7863\n",
      "Epoch 1178/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6632 - val_loss: 0.7861\n",
      "Epoch 1179/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6775 - val_loss: 0.7738\n",
      "Epoch 1180/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6542 - val_loss: 0.7952\n",
      "Epoch 1181/1200\n",
      "52/52 [==============================] - 2s 38ms/step - loss: 0.6631 - val_loss: 0.7791\n",
      "Epoch 1182/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6986 - val_loss: 0.7966\n",
      "Epoch 1183/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6761 - val_loss: 0.7843\n",
      "Epoch 1184/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6587 - val_loss: 0.7901\n",
      "Epoch 1185/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6522 - val_loss: 0.8048\n",
      "Epoch 1186/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6760 - val_loss: 0.7927\n",
      "Epoch 1187/1200\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.6507 - val_loss: 0.7649\n",
      "Epoch 1188/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6708 - val_loss: 0.7884\n",
      "Epoch 1189/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6761 - val_loss: 0.7878\n",
      "Epoch 1190/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6685 - val_loss: 0.7724\n",
      "Epoch 1191/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6463 - val_loss: 0.7828\n",
      "Epoch 1192/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6586 - val_loss: 0.7676\n",
      "Epoch 1193/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6779 - val_loss: 0.7882\n",
      "Epoch 1194/1200\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.6685 - val_loss: 0.7719\n",
      "Epoch 1195/1200\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 0.6600 - val_loss: 0.7988\n",
      "Epoch 1196/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.7015 - val_loss: 0.7665\n",
      "Epoch 1197/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6410 - val_loss: 0.7779\n",
      "Epoch 1198/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6657 - val_loss: 0.7827\n",
      "Epoch 1199/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6644 - val_loss: 0.7699\n",
      "Epoch 1200/1200\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.6686 - val_loss: 0.7750\n"
     ]
    }
   ],
   "source": [
    "#run call the transformer model\n",
    "#df_read = df.copy()\n",
    "#df_read=(df_read-df_read.mean())/df_read.std()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, test = train_test_split(df_read, test_size=0.2)\n",
    "\n",
    "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
    "test = df5.copy()\n",
    "\n",
    "Y_train = np.array(train['class'])\n",
    "X_train= np.array(train.drop(['0','5','9','108', 'class'],axis=1))\n",
    "X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
    "print(X_train.shape[1])\n",
    "print(X_train.shape)\n",
    "\n",
    "Y_val=np.array(test['class'])\n",
    "X_val = np.array(test.drop(['0','5','9','108', 'class'],axis=1))\n",
    "X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])\n",
    "NUM_LAYERS =  4\n",
    "\n",
    "D_MODEL = X_train.shape[2]\n",
    "print('d_model', D_MODEL)\n",
    "NUM_HEADS =  4\n",
    "print('num_heads', NUM_HEADS)\n",
    "UNITS =  2048\n",
    "DROPOUT = 0.1 #0.1\n",
    "TIME_STEPS= X_train.shape[1]\n",
    "print('TIME STEPS', TIME_STEPS)\n",
    "OUTPUT_SIZE=1\n",
    "batch_size=64\n",
    "\n",
    "model = transformer(\n",
    "  time_steps=TIME_STEPS,\n",
    "  num_layers=NUM_LAYERS,\n",
    "  units=UNITS,\n",
    "  d_model=D_MODEL,\n",
    "  num_heads=NUM_HEADS,\n",
    "  dropout=DROPOUT,\n",
    "  output_size=OUTPUT_SIZE,\n",
    "  projection='linear')\n",
    "\n",
    "#run\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='mae') #org\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss='mse')\n",
    "# model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mae')\n",
    "#\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=80, restore_best_weights=True)\n",
    "# history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[callback])\n",
    "history = model.fit(X_train,Y_train, epochs=1200, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UxMyaAwpcqdf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004332117287509413\n",
      "(1660, 1, 284)\n",
      "(415, 1, 284)\n"
     ]
    }
   ],
   "source": [
    "#run\n",
    "import time\n",
    "st = time.time()\n",
    "p1 = np.array(model(X_val)).flatten()\n",
    "end = time.time()\n",
    "# print(end, st, len(p1))\n",
    "print((end-st)/len(p1))\n",
    "p2 = np.array(model(X_train)).flatten()\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Say3gnZgi331",
    "outputId": "8cb662ec-46e3-428c-a3dc-5f3adeb2ff3a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "[[1.         0.82558073]\n",
      " [0.82558073 1.        ]] SignificanceResult(statistic=0.8200834426645004, pvalue=0.0) 0.6307726011213807\n",
      "Validation\n",
      "0.7463958278425061 0.7378729330739575 0.542058127040542\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import kendalltau\n",
    "print(\"training\")\n",
    "print(np.corrcoef(p2, Y_train), stats.spearmanr(p2, Y_train), kendalltau(p2,Y_train).correlation)\n",
    "print(\"Validation\")\n",
    "print(np.corrcoef(p1, Y_val)[1][0], stats.spearmanr(p1, Y_val).correlation, kendalltau(p1,Y_val).correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ST5WiH4KoeOh",
    "outputId": "b995e4a2-4e2a-412c-9c59-0ee632503d82",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1660, 1, 284)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ksXNLzMScqg6",
    "outputId": "218ba87c-4fe6-43c8-bf76-19bc5d9ce9b8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\"audio_dm_3_featu_.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
