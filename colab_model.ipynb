{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVAa1tdOE-5-",
        "outputId": "11440f54-2b15-430a-f6b3-93ddc01a1917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/home/ashutosh/Desktop/ugmqa_project/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q librosa matplotlib spafe torch pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pknZQa7wt4L3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QqdzhPSpFJ5z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nHXfPJFBGX63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        assert (self.head_dim * num_heads ==\n",
        "                embed_size), \"Embed size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n",
        "\n",
        "        # Split embedding into self.num_heads pieces\n",
        "        value = value.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "        key = key.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        values = self.values(value)\n",
        "        keys = self.keys(key)\n",
        "        queries = self.queries(query)\n",
        "        energy = torch.einsum(\n",
        "            \"nqhd,nkhd->nhqk\", [queries, keys])  # MatMul Q and K\n",
        "        # queries shape: (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, query_len, heads, heads_dim)\n",
        "        # energy shape: (N, heads, query_len, key_len)\n",
        "        # print(\"Mask\", mask.shape)\n",
        "        # print(\"Energy\", energy.shape)\n",
        "\n",
        "        # energy = torch.zeros((N, self.num_heads, query_len, key_len)).to(device)\n",
        "\n",
        "        # mask = torch.zeros((1, 1, 1, key_len)).to(device)\n",
        "\n",
        "        if mask is not None:\n",
        "            # print(mask)\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "            # print(energy[0][0][0])\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
        "        # print(\"output_out:\", out.shape)\n",
        "        # print('Out shape', out.shape)\n",
        "        out = out.reshape(\n",
        "            N, query_len, self.num_heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uBb-2No1IHlL"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, dropout, max_len = 5000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
        "    self.position_encoding = torch.zeros(max_len, embed_size).to(device)\n",
        "    self.position_encoding[:, 0::2] = torch.sin(position * div_term).to(device)\n",
        "    self.position_encoding[:, 1::2] = torch.cos(position * div_term).to(device)\n",
        "    self.register_buffer('pe', self.position_encoding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print('pe_x', x.shape)\n",
        "    # print('pe', self.position_encoding.shape)\n",
        "    x = x + self.position_encoding[:x.size(0)]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "m6FIvPEgIJib"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        x = self.dropout(self.norm1((attention + query)))\n",
        "\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(x + forward))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mVmNcXqTILWw"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    src_vocab_size,\n",
        "    embed_size,\n",
        "    num_layers,\n",
        "    heads,\n",
        "    device,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_length\n",
        "  ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device\n",
        "    # self.position_embedding = PositionalEncoding(embed_size, dropout, src_vocab_size)\n",
        "    self.position_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "      [\n",
        "        TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    N, seq_length = x.shape\n",
        "    # print(N, seq_length)\n",
        "\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "    # print('positions shape', positions.shape)\n",
        "    out = self.dropout(self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, out, out, mask)\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4-NbKpF1ITI-"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    src_vocab_size,\n",
        "    src_pad_index,\n",
        "    embed_size = 256,\n",
        "    num_layers = 6,\n",
        "    forward_expansion = 4,\n",
        "    heads = 8,\n",
        "    dropout = 0,\n",
        "    device = \"cuda\",\n",
        "    max_length = 100\n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "      src_vocab_size, embed_size, num_layers, heads,\n",
        "      device, forward_expansion, dropout, max_length)\n",
        "\n",
        "    self.src_pad_index = src_pad_index\n",
        "    self.output = nn.Linear(src_vocab_size * embed_size, 1)\n",
        "    self.device = device\n",
        "    self.embed_size = embed_size\n",
        "    self.src_vocab_size = src_vocab_size\n",
        "\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_index).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # (N, 1, 1, src_len)\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self, src):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    out = self.encoder(src, src_mask)\n",
        "\n",
        "    out = out.reshape(-1, self.src_vocab_size * self.embed_size)\n",
        "\n",
        "    return self.output(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "soinXdqjMxj5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "\n",
        "class AudioFeatureDataset(Dataset):\n",
        "    def __init__(self, annotations_file, mode='train'):\n",
        "        self.data = pd.read_csv(annotations_file)\n",
        "        self.data = self.data.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "        # Splitting the dataset into train and validation sets\n",
        "        total_samples = len(self.data)\n",
        "        train_size = int(0.8 * total_samples)\n",
        "        valid_size = total_samples - train_size\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.data = self.data.iloc[:train_size]\n",
        "        else:\n",
        "            self.data = self.data.iloc[train_size:]\n",
        "\n",
        "        self.features = torch.Tensor(self.data.drop(['class'], axis=1).values)\n",
        "        self.labels = torch.Tensor(self.data['class'].values)\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        # print(self.features.shape)\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "dataset = AudioFeatureDataset('./working_dataset.csv', mode='train')\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# for batch in dataloader:\n",
        "#     x, y = batch\n",
        "#     # Your training code here\n",
        "# x_train = dataset[0][0]\n",
        "# x_target = dataset[0][1]\n",
        "\n",
        "# print(len(dataset))\n",
        "# print(len(dataset[0][0]))\n",
        "\n",
        "dataset_val = AudioFeatureDataset('./working_dataset.csv', mode=\"val\")\n",
        "dataloader_val = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "# print(len(dataset_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JKBauT7rV9dJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "eL44nAxMPBBV",
        "outputId": "2c6fa457-7d18-468d-d2ff-f3bb5ae04d8c"
      },
      "outputs": [],
      "source": [
        "num_layers = 1\n",
        "src_vocab_size = 297  # TIME-STEPS\n",
        "src_pad_index = 0\n",
        "embed_size = 256 #D-Model\n",
        "num_heads = 1\n",
        "dropout = 0.1\n",
        "output_size = 1\n",
        "forward_expansion = 4\n",
        "\n",
        "model = Transformer(\n",
        "  src_vocab_size,\n",
        "  src_pad_index,\n",
        "  embed_size = embed_size,\n",
        "  dropout = dropout,\n",
        "  heads = num_heads,\n",
        "  num_layers = num_layers,\n",
        "  forward_expansion = forward_expansion\n",
        "  ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 297])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.3572,  0.3436,  0.6759, -0.0480], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "X = torch.Tensor([[0,124.46949,-21.54154,0,16.483715,36.6015,0,-25.834284,0,10.178377,0,-7.345142,10.264649,16.472473,7.895236,5.5313177,6.2877584,-4.182223,-14.012452,-7.7094264,0.4547759,-5.8861904,-13.388045,-6.9823565,1.1916866,-1.7707733,-5.276302,-0.35934502,1.7799568,-4.779073,-6.8897967,0.79337484,4.2686896,-2.5059521,-6.496715,-1.0943556,3.1531215,0.25546533,-2.272549,-0.73324627,0.0020391261,0.012858815,0.062261246,0.030906683,0.033696823,0.06754137,0.054984007,0.36406443,0.18963988,0.10330482,0.12989527,0.42003468,0.48301712,0.11749379,0.06369407,0.12233541,0.2568086,0.5382474,1.106481,1.4184383,0.71715385,0.35170192,0.09534879,0.0366585,0.102917545,0.087631606,0.03625296,0.06454905,0.095770694,0.12915519,0.25286278,0.2482152,0.20051777,0.39578456,0.28170255,0.3759659,0.4401238,0.90044194,0.8636782,1.1747689,0.7714911,0.818637,0.37247756,0.3380456,0.7661559,2.4523528,2.7824976,8.302962,16.734638,8.80889,12.927655,13.964713,7.212821,7.622612,17.17792,10.23075,8.3349285,5.6397433,3.6112895,0.6386233,0.24162434,0.34277353,0.36261582,0.3141538,0.38225174,0.47966504,0.03040914,0.0012386683,9.3100425e-05,7.8610115e-05,7.8027195e-05,5.490173e-06,1.6225702e-05,5.6767494e-06,4.4215603e-06,9.099585e-06,1.2576394e-06,2.412565e-06,7.926269e-07,2.0946077e-06,1.3263315e-06,6.3015705e-07,4.1206027e-07,7.42592e-07,5.666581e-07,2.967201e-07,1.7622109e-07,4.3961313e-07,1.7293216e-07,1.4194566e-07,1.8899503e-07,1.5307475e-07,9.427612e-08,1.06292724e-07,1.01943236e-07,6.155833e-08,7.990511e-08,6.0735445e-08,4.232562e-08,6.248923e-08,3.7052516e-08,4.0408686e-08,3.633701e-08,2.8342097e-08,3.3516038e-08,2.3205962e-08,2.8330943e-08,2.0632546e-08,2.3197451e-08,1.7732157e-08,1.965794e-08,1.6046194e-08,1.7022142e-08,1.4855194e-08,1.4529622e-08,1.42051055e-08,1.3379163e-08,1.3088586e-08,1.2451093e-08,1.209253e-08,1.2065797e-08,1.1807311e-08,1.1646068e-08,1.16453025e-08,1.1936379e-08,1.2163221e-08,1.1026455e-08,7.149512e-09,0.36461177,0.34337756,0.27882302,0.48984087,0.58728004,0.33111906,0.35057455,0.40294,0.37982342,0.38546938,0.35827973,0.3449619,4098.515885613107,4394.679597751644,2173.6066930992697,660.1920797101604,666.3801569895414,679.7044596914801,646.1064276646537,678.6052827762849,636.4525179151398,588.8034579300204,588.6927126149499,605.9566149564441,554.6815422905305,643.55633138673,635.2709118421336,593.1425591116601,689.9515513706316,753.8378337006548,713.6116050230352,665.575526379199,639.8702521661144,566.4381100383742,602.769675543065,636.9758223614031,838.8391505866864,1478.874805344335,1459.149044006308,1387.6915345139455,1353.654060604915,1380.012526068448,1383.4946564371012,1341.579689123074,1427.5678622884227,1397.7808753654772,1344.037783237732,1257.285128280657,1297.332690017431,1316.4388778935704,1307.883347941082,1283.9768778672214,1273.1843530634096,1278.0824064357296,1321.431115766408,1354.4814820287468,1333.5878041563171,1357.2018216795293,1347.5298328353635,1317.862385602503,1316.7824410928667,1363.7911817467896,1421.45053482153,1473.7295655749122,1513.3380612007554,1527.07320879706,1520.3705478366887,1481.8238543145394,1500.151441619609,1508.9669344602714,1481.524474097877,1445.1450034130887,1417.4826755078072,1453.824676109189,1476.554974221759,1457.7835067613787,1442.5007903703054,1446.138078746579,1480.3479277869212,1474.320222966424,1423.3878323139418,1360.8577255891676,1337.3912372750488,1400.7434254248976,1406.7530196210892,1371.0172331395806,1360.0256467642882,1360.1123709007084,1359.7692618475817,1349.6076963128105,1345.1738370383607,1343.4491732619308,1328.2974945469014,1314.617505626696,1321.0909928662404,1335.4841595848136,1343.4321393541222,1344.775088467436,1333.9444077975136,1305.7116252799508,1309.2780904215947,1308.5759110007473,1306.0999714339998,1355.6284484769956,1452.5165920413374,1475.6366023948713,1447.970350106245,1406.542142884293,1377.6039021203871,1278.4930177581896,1189.0178172699625,1205.2225452548764,1179.4867044106006,881.8538957920834,1197.5584167943596,1324.3785722370592,1361.4066076471468,1263.928152316742,1005.8203163981548,759.0834395666938,679.2875862507892,708.2743155303299,712.1759249973968,604.9816438504809,659.412993337054,789.7184528406086,815.1398626985261,915.8414552062778,773.3202934324213],\n",
        "                  [-397.86472,124.46949,0,-48.849564,16.483715,36.6015,-6.4991527,-25.834284,-1.2522386,10.178377,-5.218914,-7.345142,10.264649,16.472473,7.895236,5.5313177,6.2877584,-4.182223,-14.012452,-7.7094264,0.4547759,-5.8861904,-13.388045,-6.9823565,1.1916866,-1.7707733,-5.276302,-0.35934502,1.7799568,-4.779073,-6.8897967,0.79337484,4.2686896,-2.5059521,-6.496715,-1.0943556,3.1531215,0.25546533,-2.272549,-0.73324627,0.0020391261,0.012858815,0.062261246,0.030906683,0.033696823,0.06754137,0.054984007,0.36406443,0.18963988,0.10330482,0.12989527,0.42003468,0.48301712,0.11749379,0.06369407,0.12233541,0.2568086,0.5382474,1.106481,1.4184383,0.71715385,0.35170192,0.09534879,0.0366585,0.102917545,0.087631606,0.03625296,0.06454905,0.095770694,0.12915519,0.25286278,0.2482152,0.20051777,0.39578456,0.28170255,0.3759659,0.4401238,0.90044194,0.8636782,1.1747689,0.7714911,0.818637,0.37247756,0.3380456,0.7661559,2.4523528,2.7824976,8.302962,16.734638,8.80889,12.927655,13.964713,7.212821,7.622612,17.17792,10.23075,8.3349285,5.6397433,3.6112895,0.6386233,0.24162434,0.34277353,0.36261582,0.3141538,0.38225174,0.47966504,0.03040914,0.0012386683,9.3100425e-05,7.8610115e-05,7.8027195e-05,5.490173e-06,1.6225702e-05,5.6767494e-06,4.4215603e-06,9.099585e-06,1.2576394e-06,2.412565e-06,7.926269e-07,2.0946077e-06,1.3263315e-06,6.3015705e-07,4.1206027e-07,7.42592e-07,5.666581e-07,2.967201e-07,1.7622109e-07,4.3961313e-07,1.7293216e-07,1.4194566e-07,1.8899503e-07,1.5307475e-07,9.427612e-08,1.06292724e-07,1.01943236e-07,6.155833e-08,7.990511e-08,6.0735445e-08,4.232562e-08,6.248923e-08,3.7052516e-08,4.0408686e-08,3.633701e-08,2.8342097e-08,3.3516038e-08,2.3205962e-08,2.8330943e-08,2.0632546e-08,2.3197451e-08,1.7732157e-08,1.965794e-08,1.6046194e-08,1.7022142e-08,1.4855194e-08,1.4529622e-08,1.42051055e-08,1.3379163e-08,1.3088586e-08,1.2451093e-08,1.209253e-08,1.2065797e-08,1.1807311e-08,1.1646068e-08,1.16453025e-08,1.1936379e-08,1.2163221e-08,1.1026455e-08,7.149512e-09,0.36461177,0.34337756,0.27882302,0.48984087,0.58728004,0.33111906,0.35057455,0.40294,0.37982342,0.38546938,0.35827973,0.3449619,4098.515885613107,4394.679597751644,2173.6066930992697,660.1920797101604,666.3801569895414,679.7044596914801,646.1064276646537,678.6052827762849,636.4525179151398,588.8034579300204,588.6927126149499,605.9566149564441,554.6815422905305,643.55633138673,635.2709118421336,593.1425591116601,689.9515513706316,753.8378337006548,713.6116050230352,665.575526379199,639.8702521661144,566.4381100383742,602.769675543065,636.9758223614031,838.8391505866864,1478.874805344335,1459.149044006308,1387.6915345139455,1353.654060604915,1380.012526068448,1383.4946564371012,1341.579689123074,1427.5678622884227,1397.7808753654772,1344.037783237732,1257.285128280657,1297.332690017431,1316.4388778935704,1307.883347941082,1283.9768778672214,1273.1843530634096,1278.0824064357296,1321.431115766408,1354.4814820287468,1333.5878041563171,1357.2018216795293,1347.5298328353635,1317.862385602503,1316.7824410928667,1363.7911817467896,1421.45053482153,1473.7295655749122,1513.3380612007554,1527.07320879706,1520.3705478366887,1481.8238543145394,1500.151441619609,1508.9669344602714,1481.524474097877,1445.1450034130887,1417.4826755078072,1453.824676109189,1476.554974221759,1457.7835067613787,1442.5007903703054,1446.138078746579,1480.3479277869212,1474.320222966424,1423.3878323139418,1360.8577255891676,1337.3912372750488,1400.7434254248976,1406.7530196210892,1371.0172331395806,1360.0256467642882,1360.1123709007084,1359.7692618475817,1349.6076963128105,1345.1738370383607,1343.4491732619308,1328.2974945469014,1314.617505626696,1321.0909928662404,1335.4841595848136,1343.4321393541222,1344.775088467436,1333.9444077975136,1305.7116252799508,1309.2780904215947,1308.5759110007473,1306.0999714339998,1355.6284484769956,1452.5165920413374,1475.6366023948713,1447.970350106245,1406.542142884293,1377.6039021203871,1278.4930177581896,1189.0178172699625,1205.2225452548764,1179.4867044106006,881.8538957920834,1197.5584167943596,1324.3785722370592,1361.4066076471468,1263.928152316742,1005.8203163981548,759.0834395666938,679.2875862507892,708.2743155303299,712.1759249973968,604.9816438504809,659.412993337054,789.7184528406086,815.1398626985261,915.8414552062778,773.3202934324213],\n",
        "                  [-397.86472,0,-21.54154,-48.849564,16.483715,36.6015,-6.4991527,-25.834284,-1.2522386,10.178377,-5.218914,-7.345142,10.264649,16.472473,7.895236,5.5313177,6.2877584,-4.182223,-14.012452,-7.7094264,0.4547759,-5.8861904,-13.388045,-6.9823565,1.1916866,-1.7707733,-5.276302,-0.35934502,1.7799568,-4.779073,-6.8897967,0.79337484,4.2686896,-2.5059521,-6.496715,-1.0943556,3.1531215,0.25546533,-2.272549,-0.73324627,0.0020391261,0.012858815,0.062261246,0.030906683,0.033696823,0.06754137,0.054984007,0.36406443,0.18963988,0.10330482,0.12989527,0.42003468,0.48301712,0.11749379,0.06369407,0.12233541,0.2568086,0.5382474,1.106481,1.4184383,0.71715385,0.35170192,0.09534879,0.0366585,0.102917545,0.087631606,0.03625296,0.06454905,0.095770694,0.12915519,0.25286278,0.2482152,0.20051777,0.39578456,0.28170255,0.3759659,0.4401238,0.90044194,0.8636782,1.1747689,0.7714911,0.818637,0.37247756,0.3380456,0.7661559,2.4523528,2.7824976,8.302962,16.734638,8.80889,12.927655,13.964713,7.212821,7.622612,17.17792,10.23075,8.3349285,5.6397433,3.6112895,0.6386233,0.24162434,0.34277353,0.36261582,0.3141538,0.38225174,0.47966504,0.03040914,0.0012386683,9.3100425e-05,7.8610115e-05,7.8027195e-05,5.490173e-06,1.6225702e-05,5.6767494e-06,4.4215603e-06,9.099585e-06,1.2576394e-06,2.412565e-06,7.926269e-07,2.0946077e-06,1.3263315e-06,6.3015705e-07,4.1206027e-07,7.42592e-07,5.666581e-07,2.967201e-07,1.7622109e-07,4.3961313e-07,1.7293216e-07,1.4194566e-07,1.8899503e-07,1.5307475e-07,9.427612e-08,1.06292724e-07,1.01943236e-07,6.155833e-08,7.990511e-08,6.0735445e-08,4.232562e-08,6.248923e-08,3.7052516e-08,4.0408686e-08,3.633701e-08,2.8342097e-08,3.3516038e-08,2.3205962e-08,2.8330943e-08,2.0632546e-08,2.3197451e-08,1.7732157e-08,1.965794e-08,1.6046194e-08,1.7022142e-08,1.4855194e-08,1.4529622e-08,1.42051055e-08,1.3379163e-08,1.3088586e-08,1.2451093e-08,1.209253e-08,1.2065797e-08,1.1807311e-08,1.1646068e-08,1.16453025e-08,1.1936379e-08,1.2163221e-08,1.1026455e-08,7.149512e-09,0.36461177,0.34337756,0.27882302,0.48984087,0.58728004,0.33111906,0.35057455,0.40294,0.37982342,0.38546938,0.35827973,0.3449619,4098.515885613107,4394.679597751644,2173.6066930992697,660.1920797101604,666.3801569895414,679.7044596914801,646.1064276646537,678.6052827762849,636.4525179151398,588.8034579300204,588.6927126149499,605.9566149564441,554.6815422905305,643.55633138673,635.2709118421336,593.1425591116601,689.9515513706316,753.8378337006548,713.6116050230352,665.575526379199,639.8702521661144,566.4381100383742,602.769675543065,636.9758223614031,838.8391505866864,1478.874805344335,1459.149044006308,1387.6915345139455,1353.654060604915,1380.012526068448,1383.4946564371012,1341.579689123074,1427.5678622884227,1397.7808753654772,1344.037783237732,1257.285128280657,1297.332690017431,1316.4388778935704,1307.883347941082,1283.9768778672214,1273.1843530634096,1278.0824064357296,1321.431115766408,1354.4814820287468,1333.5878041563171,1357.2018216795293,1347.5298328353635,1317.862385602503,1316.7824410928667,1363.7911817467896,1421.45053482153,1473.7295655749122,1513.3380612007554,1527.07320879706,1520.3705478366887,1481.8238543145394,1500.151441619609,1508.9669344602714,1481.524474097877,1445.1450034130887,1417.4826755078072,1453.824676109189,1476.554974221759,1457.7835067613787,1442.5007903703054,1446.138078746579,1480.3479277869212,1474.320222966424,1423.3878323139418,1360.8577255891676,1337.3912372750488,1400.7434254248976,1406.7530196210892,1371.0172331395806,1360.0256467642882,1360.1123709007084,1359.7692618475817,1349.6076963128105,1345.1738370383607,1343.4491732619308,1328.2974945469014,1314.617505626696,1321.0909928662404,1335.4841595848136,1343.4321393541222,1344.775088467436,1333.9444077975136,1305.7116252799508,1309.2780904215947,1308.5759110007473,1306.0999714339998,1355.6284484769956,1452.5165920413374,1475.6366023948713,1447.970350106245,1406.542142884293,1377.6039021203871,1278.4930177581896,1189.0178172699625,1205.2225452548764,1179.4867044106006,881.8538957920834,1197.5584167943596,1324.3785722370592,1361.4066076471468,1263.928152316742,1005.8203163981548,759.0834395666938,679.2875862507892,708.2743155303299,712.1759249973968,604.9816438504809,659.412993337054,789.7184528406086,815.1398626985261,915.8414552062778,773.3202934324213],\n",
        "                  [0,124.46949,-21.54154,-48.849564,16.483715,36.6015,-6.4991527,-25.834284,-1.2522386,10.178377,-5.218914,-7.345142,10.264649,16.472473,7.895236,5.5313177,6.2877584,-4.182223,-14.012452,-7.7094264,0.4547759,-5.8861904,-13.388045,-6.9823565,1.1916866,-1.7707733,-5.276302,-0.35934502,1.7799568,-4.779073,-6.8897967,0.79337484,4.2686896,-2.5059521,-6.496715,-1.0943556,3.1531215,0.25546533,-2.272549,-0.73324627,0.0020391261,0.012858815,0.062261246,0.030906683,0.033696823,0.06754137,0.054984007,0.36406443,0.18963988,0.10330482,0.12989527,0.42003468,0.48301712,0.11749379,0.06369407,0.12233541,0.2568086,0.5382474,1.106481,1.4184383,0.71715385,0.35170192,0.09534879,0.0366585,0.102917545,0.087631606,0.03625296,0.06454905,0.095770694,0.12915519,0.25286278,0.2482152,0.20051777,0.39578456,0.28170255,0.3759659,0.4401238,0.90044194,0.8636782,1.1747689,0.7714911,0.818637,0.37247756,0.3380456,0.7661559,2.4523528,2.7824976,8.302962,16.734638,8.80889,12.927655,13.964713,7.212821,7.622612,17.17792,10.23075,8.3349285,5.6397433,3.6112895,0.6386233,0.24162434,0.34277353,0.36261582,0.3141538,0.38225174,0.47966504,0.03040914,0.0012386683,9.3100425e-05,7.8610115e-05,7.8027195e-05,5.490173e-06,1.6225702e-05,5.6767494e-06,4.4215603e-06,9.099585e-06,1.2576394e-06,2.412565e-06,7.926269e-07,2.0946077e-06,1.3263315e-06,6.3015705e-07,4.1206027e-07,7.42592e-07,5.666581e-07,2.967201e-07,1.7622109e-07,4.3961313e-07,1.7293216e-07,1.4194566e-07,1.8899503e-07,1.5307475e-07,9.427612e-08,1.06292724e-07,1.01943236e-07,6.155833e-08,7.990511e-08,6.0735445e-08,4.232562e-08,6.248923e-08,3.7052516e-08,4.0408686e-08,3.633701e-08,2.8342097e-08,3.3516038e-08,2.3205962e-08,2.8330943e-08,2.0632546e-08,2.3197451e-08,1.7732157e-08,1.965794e-08,1.6046194e-08,1.7022142e-08,1.4855194e-08,1.4529622e-08,1.42051055e-08,1.3379163e-08,1.3088586e-08,1.2451093e-08,1.209253e-08,1.2065797e-08,1.1807311e-08,1.1646068e-08,1.16453025e-08,1.1936379e-08,1.2163221e-08,1.1026455e-08,7.149512e-09,0.36461177,0.34337756,0.27882302,0.48984087,0.58728004,0.33111906,0.35057455,0.40294,0.37982342,0.38546938,0.35827973,0.3449619,4098.515885613107,4394.679597751644,2173.6066930992697,660.1920797101604,666.3801569895414,679.7044596914801,646.1064276646537,678.6052827762849,636.4525179151398,588.8034579300204,588.6927126149499,605.9566149564441,554.6815422905305,643.55633138673,635.2709118421336,593.1425591116601,689.9515513706316,753.8378337006548,713.6116050230352,665.575526379199,639.8702521661144,566.4381100383742,602.769675543065,636.9758223614031,838.8391505866864,1478.874805344335,1459.149044006308,1387.6915345139455,1353.654060604915,1380.012526068448,1383.4946564371012,1341.579689123074,1427.5678622884227,1397.7808753654772,1344.037783237732,1257.285128280657,1297.332690017431,1316.4388778935704,1307.883347941082,1283.9768778672214,1273.1843530634096,1278.0824064357296,1321.431115766408,1354.4814820287468,1333.5878041563171,1357.2018216795293,1347.5298328353635,1317.862385602503,1316.7824410928667,1363.7911817467896,1421.45053482153,1473.7295655749122,1513.3380612007554,1527.07320879706,1520.3705478366887,1481.8238543145394,1500.151441619609,1508.9669344602714,1481.524474097877,1445.1450034130887,1417.4826755078072,1453.824676109189,1476.554974221759,1457.7835067613787,1442.5007903703054,1446.138078746579,1480.3479277869212,1474.320222966424,1423.3878323139418,1360.8577255891676,1337.3912372750488,1400.7434254248976,1406.7530196210892,1371.0172331395806,1360.0256467642882,1360.1123709007084,1359.7692618475817,1349.6076963128105,1345.1738370383607,1343.4491732619308,1328.2974945469014,1314.617505626696,1321.0909928662404,1335.4841595848136,1343.4321393541222,1344.775088467436,1333.9444077975136,1305.7116252799508,1309.2780904215947,1308.5759110007473,1306.0999714339998,1355.6284484769956,1452.5165920413374,1475.6366023948713,1447.970350106245,1406.542142884293,1377.6039021203871,1278.4930177581896,1189.0178172699625,1205.2225452548764,1179.4867044106006,881.8538957920834,1197.5584167943596,1324.3785722370592,1361.4066076471468,1263.928152316742,1005.8203163981548,759.0834395666938,679.2875862507892,708.2743155303299,712.1759249973968,604.9816438504809,659.412993337054,789.7184528406086,815.1398626985261,915.8414552062778,773.3202934324213]])\n",
        "\n",
        "X = X.to(device)\n",
        "\n",
        "print(X.shape)\n",
        "# y = torch.Tensor([3.21]).to(device)\n",
        "\n",
        "pred = model(X).squeeze()\n",
        "print(pred)\n",
        "# loss_fn = nn.MSELoss()\n",
        "# print(loss_fn(pred, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Ys2SVUuyQZou"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "batch 1 loss: 0.023027770519256592\n",
            "batch 101 loss: 1.9786805713587092\n",
            "batch 201 loss: 1.5805484951034305\n",
            "batch 301 loss: 1.3721672656244481\n",
            "batch 401 loss: 1.7587486047115641\n",
            "batch 501 loss: 1.852455121179737\n",
            "batch 601 loss: 1.8957205086108297\n",
            "batch 701 loss: 1.7571566923510182\n",
            "batch 801 loss: 1.8496282738227636\n",
            "batch 901 loss: 2.1162181313836483\n",
            "batch 1001 loss: 1.811686961103842\n",
            "batch 1101 loss: 1.8808767113705107\n",
            "batch 1201 loss: 1.6269929672434227\n",
            "batch 1301 loss: 1.885086389207281\n",
            "batch 1401 loss: 1.839956363287056\n",
            "batch 1501 loss: 1.5422802571079228\n",
            "batch 1601 loss: 2.207083491367521\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ashutosh/Desktop/ugmqa_project/venv/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.903329 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0000553726220824\n",
            "batch 101 loss: 1.9174738237958495\n",
            "batch 201 loss: 1.971653355185408\n",
            "batch 301 loss: 1.4395726550975814\n",
            "batch 401 loss: 1.695589261383866\n",
            "batch 501 loss: 1.8864454291993753\n",
            "batch 601 loss: 1.4790758796362207\n",
            "batch 701 loss: 1.4804863199685496\n",
            "batch 801 loss: 1.7045763987497775\n",
            "batch 901 loss: 1.7497560768265976\n",
            "batch 1001 loss: 1.5563001579639968\n",
            "batch 1101 loss: 1.7807734946039273\n",
            "batch 1201 loss: 1.836237637930317\n",
            "batch 1301 loss: 1.8351398984236584\n",
            "batch 1401 loss: 1.845583249393967\n",
            "batch 1501 loss: 1.6678031766178174\n",
            "batch 1601 loss: 1.7779404591408092\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.730898 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "batch 1 loss: 1.278724195251707\n",
            "batch 101 loss: 1.9067457352718338\n",
            "batch 201 loss: 1.8182069416792774\n",
            "batch 301 loss: 1.807286435350825\n",
            "batch 401 loss: 1.6951495494888513\n",
            "batch 501 loss: 1.4791419703132123\n",
            "batch 601 loss: 1.852342631787178\n",
            "batch 701 loss: 1.4737579261732754\n",
            "batch 801 loss: 1.6831552543654107\n",
            "batch 901 loss: 1.7337375425826758\n",
            "batch 1001 loss: 1.9053942251999978\n",
            "batch 1101 loss: 1.9473828504466655\n",
            "batch 1201 loss: 1.8324023969619885\n",
            "batch 1301 loss: 2.0001432008264235\n",
            "batch 1401 loss: 1.6138143535871858\n",
            "batch 1501 loss: 1.5135206026333616\n",
            "batch 1601 loss: 1.404701267138007\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.621541 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1236187249375507\n",
            "batch 101 loss: 1.6950106110694834\n",
            "batch 201 loss: 1.7556201150012203\n",
            "batch 301 loss: 1.6653122116849612\n",
            "batch 401 loss: 2.0172431603243446\n",
            "batch 501 loss: 1.7553699930905713\n",
            "batch 601 loss: 1.822683552915878\n",
            "batch 701 loss: 1.9177766790639725\n",
            "batch 801 loss: 1.6729696927385158\n",
            "batch 901 loss: 1.8302722409856507\n",
            "batch 1001 loss: 1.7327592904376798\n",
            "batch 1101 loss: 1.8781310469099117\n",
            "batch 1201 loss: 1.8232523078340455\n",
            "batch 1301 loss: 1.8929948837834671\n",
            "batch 1401 loss: 1.7939858526695753\n",
            "batch 1501 loss: 1.710874791293172\n",
            "batch 1601 loss: 1.5676035908357882\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.619958 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0105089903250337\n",
            "batch 101 loss: 1.6256224016845227\n",
            "batch 201 loss: 1.813837527862197\n",
            "batch 301 loss: 1.6007128323057622\n",
            "batch 401 loss: 1.7503973069615313\n",
            "batch 501 loss: 1.7822842816430784\n",
            "batch 601 loss: 1.6572767260490218\n",
            "batch 701 loss: 1.320199548367391\n",
            "batch 801 loss: 1.6543230445950758\n",
            "batch 901 loss: 1.764806694094441\n",
            "batch 1001 loss: 1.8430566816055216\n",
            "batch 1101 loss: 2.0373091462992305\n",
            "batch 1201 loss: 1.8411906516901217\n",
            "batch 1301 loss: 1.4731917188132229\n",
            "batch 1401 loss: 1.8747887187660672\n",
            "batch 1501 loss: 1.6703638398693874\n",
            "batch 1601 loss: 2.0665149006084538\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.691454 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2296994201766211\n",
            "batch 101 loss: 1.5593305782923563\n",
            "batch 201 loss: 1.794661589981988\n",
            "batch 301 loss: 1.745336474372016\n",
            "batch 401 loss: 1.7593348512036027\n",
            "batch 501 loss: 1.7100279393838718\n",
            "batch 601 loss: 1.6743329698444678\n",
            "batch 701 loss: 1.9353028871980495\n",
            "batch 801 loss: 1.789344106381759\n",
            "batch 901 loss: 1.5922971060173585\n",
            "batch 1001 loss: 1.6993801397149217\n",
            "batch 1101 loss: 1.6773399992019404\n",
            "batch 1201 loss: 1.912900651712698\n",
            "batch 1301 loss: 1.6341392417292808\n",
            "batch 1401 loss: 1.8434418193460442\n",
            "batch 1501 loss: 1.6959873796568719\n",
            "batch 1601 loss: 1.55061481060824\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.616270 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9902671252854635\n",
            "batch 101 loss: 1.8964575719983259\n",
            "batch 201 loss: 1.63849720424274\n",
            "batch 301 loss: 1.727794604190858\n",
            "batch 401 loss: 1.71069314220661\n",
            "batch 501 loss: 1.7629742365446872\n",
            "batch 601 loss: 2.012198009819258\n",
            "batch 701 loss: 1.3652467728978082\n",
            "batch 801 loss: 1.5843838916682216\n",
            "batch 901 loss: 1.306097624256654\n",
            "batch 1001 loss: 1.8263735502735243\n",
            "batch 1101 loss: 1.7626152540871407\n",
            "batch 1201 loss: 1.9553721645381301\n",
            "batch 1301 loss: 1.697742647826799\n",
            "batch 1401 loss: 1.8116765185433905\n",
            "batch 1501 loss: 1.45780269354349\n",
            "batch 1601 loss: 1.713432570971345\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.618910 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2626045827691268\n",
            "batch 101 loss: 1.6992681540572085\n",
            "batch 201 loss: 1.7668536377490092\n",
            "batch 301 loss: 1.8503185257056338\n",
            "batch 401 loss: 1.6231558517052327\n",
            "batch 501 loss: 1.8619564829650335\n",
            "batch 601 loss: 1.7021991136390715\n",
            "batch 701 loss: 1.6370370737822213\n",
            "batch 801 loss: 1.8063224838657335\n",
            "batch 901 loss: 1.7173017968673958\n",
            "batch 1001 loss: 1.939477663487196\n",
            "batch 1101 loss: 1.8341925662284484\n",
            "batch 1201 loss: 1.7323456456814892\n",
            "batch 1301 loss: 1.6278840761663378\n",
            "batch 1401 loss: 1.5752702135033905\n",
            "batch 1501 loss: 1.8142392147914506\n",
            "batch 1601 loss: 1.5981066684506369\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.666244 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9981591064203531\n",
            "batch 101 loss: 1.862187450458623\n",
            "batch 201 loss: 1.6810934388331953\n",
            "batch 301 loss: 1.8620166609981157\n",
            "batch 401 loss: 1.9131005811818704\n",
            "batch 501 loss: 1.6517282865138259\n",
            "batch 601 loss: 1.6237737962300889\n",
            "batch 701 loss: 1.8502332300913986\n",
            "batch 801 loss: 1.731701310206954\n",
            "batch 901 loss: 1.735867684351979\n",
            "batch 1001 loss: 1.472844413372768\n",
            "batch 1101 loss: 1.6508873290347401\n",
            "batch 1201 loss: 1.745139127962177\n",
            "batch 1301 loss: 1.7907012099681014\n",
            "batch 1401 loss: 1.7573578637254832\n",
            "batch 1501 loss: 1.553915620726475\n",
            "batch 1601 loss: 1.9287071586295497\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.701011 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0431865373626352\n",
            "batch 101 loss: 2.0528310030465944\n",
            "batch 201 loss: 1.5471497360238573\n",
            "batch 301 loss: 1.9471072367420128\n",
            "batch 401 loss: 1.7247467407793737\n",
            "batch 501 loss: 1.3613565397860656\n",
            "batch 601 loss: 1.4874603651612415\n",
            "batch 701 loss: 1.7566774704471755\n",
            "batch 801 loss: 1.9092244889066934\n",
            "batch 901 loss: 1.6577295585058165\n",
            "batch 1001 loss: 1.9976083949836902\n",
            "batch 1101 loss: 1.7736613800979102\n",
            "batch 1201 loss: 1.8213168709824095\n",
            "batch 1301 loss: 1.7237841776187997\n",
            "batch 1401 loss: 1.9108992960520208\n",
            "batch 1501 loss: 1.7016453217397793\n",
            "batch 1601 loss: 1.7408789279035408\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.617912 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1620802887600075\n",
            "batch 101 loss: 1.5841012930014404\n",
            "batch 201 loss: 2.0164426187401934\n",
            "batch 301 loss: 1.5520137620452532\n",
            "batch 401 loss: 1.8343534752138202\n",
            "batch 501 loss: 1.431537942199502\n",
            "batch 601 loss: 1.6624592098174618\n",
            "batch 701 loss: 1.4402571566189\n",
            "batch 801 loss: 1.8527228464472227\n",
            "batch 901 loss: 1.546398647211572\n",
            "batch 1001 loss: 1.494807554369827\n",
            "batch 1101 loss: 1.7136588475804455\n",
            "batch 1201 loss: 1.7908549516863423\n",
            "batch 1301 loss: 1.818810318851065\n",
            "batch 1401 loss: 1.7642816016706637\n",
            "batch 1501 loss: 1.9002430308581097\n",
            "batch 1601 loss: 1.712043728052522\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.641262 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0193099257798166\n",
            "batch 101 loss: 1.8720763893687036\n",
            "batch 201 loss: 1.7147908492886927\n",
            "batch 301 loss: 1.7476482673104328\n",
            "batch 401 loss: 1.6835856140384566\n",
            "batch 501 loss: 2.1265932677680395\n",
            "batch 601 loss: 1.6755331418663264\n",
            "batch 701 loss: 1.7127293112897315\n",
            "batch 801 loss: 1.6172125553133856\n",
            "batch 901 loss: 1.6478213864817917\n",
            "batch 1001 loss: 1.7061780646175613\n",
            "batch 1101 loss: 1.7714372548297979\n",
            "batch 1201 loss: 1.6027953848584184\n",
            "batch 1301 loss: 1.8365022083139046\n",
            "batch 1401 loss: 1.6534592173877172\n",
            "batch 1501 loss: 1.9371971430024133\n",
            "batch 1601 loss: 1.8012548903549759\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.769380 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8215218497189926\n",
            "batch 101 loss: 1.6880232530352077\n",
            "batch 201 loss: 1.7003842077322406\n",
            "batch 301 loss: 1.8643062743404881\n",
            "batch 401 loss: 1.7328344242839375\n",
            "batch 501 loss: 1.7228613699984272\n",
            "batch 601 loss: 1.531131534570959\n",
            "batch 701 loss: 1.654564385206322\n",
            "batch 801 loss: 1.7413067565891105\n",
            "batch 901 loss: 1.9575431391045277\n",
            "batch 1001 loss: 1.6597635238868964\n",
            "batch 1101 loss: 2.173084300442133\n",
            "batch 1201 loss: 1.61122098037682\n",
            "batch 1301 loss: 1.8772763814078643\n",
            "batch 1401 loss: 1.864365854020034\n",
            "batch 1501 loss: 1.8378702672268263\n",
            "batch 1601 loss: 1.646648917736702\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.619283 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0372763359709642\n",
            "batch 101 loss: 1.541914730266435\n",
            "batch 201 loss: 2.14245487654116\n",
            "batch 301 loss: 1.9067261853093804\n",
            "batch 401 loss: 2.015208617390599\n",
            "batch 501 loss: 1.8510955309582642\n",
            "batch 601 loss: 1.9004017366241897\n",
            "batch 701 loss: 1.7967975693722837\n",
            "batch 801 loss: 1.781467377752997\n",
            "batch 901 loss: 1.5077152530476452\n",
            "batch 1001 loss: 1.7952437606738385\n",
            "batch 1101 loss: 1.6632948785269763\n",
            "batch 1201 loss: 1.815059545607801\n",
            "batch 1301 loss: 1.5746306675707455\n",
            "batch 1401 loss: 1.7237300152878743\n",
            "batch 1501 loss: 1.635205393875144\n",
            "batch 1601 loss: 1.6575543441461247\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.692124 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0551475565004875\n",
            "batch 101 loss: 1.5270497945578245\n",
            "batch 201 loss: 1.808937745699368\n",
            "batch 301 loss: 1.8816387762874365\n",
            "batch 401 loss: 1.462436319955923\n",
            "batch 501 loss: 1.8403845571786228\n",
            "batch 601 loss: 1.849865590883419\n",
            "batch 701 loss: 1.93111741527493\n",
            "batch 801 loss: 1.8857218317822844\n",
            "batch 901 loss: 1.7346428387099877\n",
            "batch 1001 loss: 1.6323030942203332\n",
            "batch 1101 loss: 1.7546043185115558\n",
            "batch 1201 loss: 1.6853021817095577\n",
            "batch 1301 loss: 1.7908377095442847\n",
            "batch 1401 loss: 2.0426893773324264\n",
            "batch 1501 loss: 1.5790812843994262\n",
            "batch 1601 loss: 1.8524921286464087\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.893591 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9463267996354261\n",
            "batch 101 loss: 1.9201749611925334\n",
            "batch 201 loss: 1.8283663985900056\n",
            "batch 301 loss: 1.7606828565476462\n",
            "batch 401 loss: 1.5100101430495851\n",
            "batch 501 loss: 1.7157227105047788\n",
            "batch 601 loss: 2.0490887080226092\n",
            "batch 701 loss: 1.687834391795841\n",
            "batch 801 loss: 1.6200723890098743\n",
            "batch 901 loss: 1.631563107978436\n",
            "batch 1001 loss: 1.6969991253121406\n",
            "batch 1101 loss: 1.4978767821239307\n",
            "batch 1201 loss: 1.9240106250098323\n",
            "batch 1301 loss: 1.5362008692012932\n",
            "batch 1401 loss: 1.7173020443542737\n",
            "batch 1501 loss: 1.707675640062953\n",
            "batch 1601 loss: 1.7268369817262283\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.753673 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9829939662119551\n",
            "batch 101 loss: 1.5020188542337565\n",
            "batch 201 loss: 1.6881162245268932\n",
            "batch 301 loss: 1.5803029789244283\n",
            "batch 401 loss: 2.064699734034948\n",
            "batch 501 loss: 1.855484239700636\n",
            "batch 601 loss: 1.585451383717591\n",
            "batch 701 loss: 1.8838692493003328\n",
            "batch 801 loss: 1.7371681289765684\n",
            "batch 901 loss: 2.0525579678374926\n",
            "batch 1001 loss: 1.7899847882241011\n",
            "batch 1101 loss: 1.7976603997203666\n",
            "batch 1201 loss: 1.820999882130909\n",
            "batch 1301 loss: 1.8280209929402917\n",
            "batch 1401 loss: 1.6047807366623965\n",
            "batch 1501 loss: 1.600487813520722\n",
            "batch 1601 loss: 1.7064914893358947\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.672535 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0566301872156327\n",
            "batch 101 loss: 1.708800435994408\n",
            "batch 201 loss: 1.8583323331663268\n",
            "batch 301 loss: 1.499340799820202\n",
            "batch 401 loss: 1.6817486319571617\n",
            "batch 501 loss: 1.764715133423524\n",
            "batch 601 loss: 1.545140827217142\n",
            "batch 701 loss: 1.939825831832186\n",
            "batch 801 loss: 1.9346436404743872\n",
            "batch 901 loss: 1.8086789703229442\n",
            "batch 1001 loss: 1.6215870002142037\n",
            "batch 1101 loss: 1.5614821039962408\n",
            "batch 1201 loss: 1.9266865274120937\n",
            "batch 1301 loss: 1.815087683936581\n",
            "batch 1401 loss: 1.6772125507087912\n",
            "batch 1501 loss: 1.8006687253445852\n",
            "batch 1601 loss: 1.713948666927172\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.738649 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1472650627268013\n",
            "batch 101 loss: 1.6308846267880859\n",
            "batch 201 loss: 1.665368703272991\n",
            "batch 301 loss: 1.6777735045272857\n",
            "batch 401 loss: 1.9130781639780616\n",
            "batch 501 loss: 2.0028101101377977\n",
            "batch 601 loss: 2.0370707752375163\n",
            "batch 701 loss: 1.5968966204368917\n",
            "batch 801 loss: 1.5680489347766706\n",
            "batch 901 loss: 1.6030976635357366\n",
            "batch 1001 loss: 1.6865789835011697\n",
            "batch 1101 loss: 1.4674714756083267\n",
            "batch 1201 loss: 1.645744614424184\n",
            "batch 1301 loss: 1.6462402415087127\n",
            "batch 1401 loss: 1.658479887462454\n",
            "batch 1501 loss: 1.7935386536783335\n",
            "batch 1601 loss: 1.7671310626006016\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.648164 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "batch 1 loss: 1.3563814257644118\n",
            "batch 101 loss: 1.7827065766369197\n",
            "batch 201 loss: 1.7532928509172052\n",
            "batch 301 loss: 1.929428289928328\n",
            "batch 401 loss: 1.9295940879439877\n",
            "batch 501 loss: 1.7585521177889314\n",
            "batch 601 loss: 1.6877611574064941\n",
            "batch 701 loss: 1.4176195104647196\n",
            "batch 801 loss: 1.9148793773492798\n",
            "batch 901 loss: 1.733607203773754\n",
            "batch 1001 loss: 1.5188301247720666\n",
            "batch 1101 loss: 1.4698265580675798\n",
            "batch 1201 loss: 1.7473899241193431\n",
            "batch 1301 loss: 1.6531109995830775\n",
            "batch 1401 loss: 1.8069774003133352\n",
            "batch 1501 loss: 1.783034329222246\n",
            "batch 1601 loss: 2.0075211005588063\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.628558 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2207059998437761\n",
            "batch 101 loss: 1.988263144435041\n",
            "batch 201 loss: 1.860906709817209\n",
            "batch 301 loss: 1.8433200328225758\n",
            "batch 401 loss: 1.76945776591936\n",
            "batch 501 loss: 1.7369041398097762\n",
            "batch 601 loss: 1.4712001876396517\n",
            "batch 701 loss: 1.964174521339628\n",
            "batch 801 loss: 1.7850750575873826\n",
            "batch 901 loss: 1.4855421960460444\n",
            "batch 1001 loss: 1.7085203078537052\n",
            "batch 1101 loss: 1.9912812149637467\n",
            "batch 1201 loss: 1.6415982086063194\n",
            "batch 1301 loss: 1.7345363767427626\n",
            "batch 1401 loss: 1.4374594843387605\n",
            "batch 1501 loss: 1.7204346886507118\n",
            "batch 1601 loss: 1.7038910301368742\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.668094 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9898635369730096\n",
            "batch 101 loss: 1.6576589379366486\n",
            "batch 201 loss: 1.6799842386506498\n",
            "batch 301 loss: 1.8431483702198603\n",
            "batch 401 loss: 1.6956379008479416\n",
            "batch 501 loss: 1.7256211355212145\n",
            "batch 601 loss: 1.8508048197755125\n",
            "batch 701 loss: 1.5972394962608814\n",
            "batch 801 loss: 1.586337276531176\n",
            "batch 901 loss: 1.8881436401116662\n",
            "batch 1001 loss: 1.5957093273539795\n",
            "batch 1101 loss: 1.4612369392905384\n",
            "batch 1201 loss: 1.4313946739445236\n",
            "batch 1301 loss: 1.8661301546632603\n",
            "batch 1401 loss: 1.8275327361340168\n",
            "batch 1501 loss: 1.9212803788500605\n",
            "batch 1601 loss: 1.9445569452545897\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.617831 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1386329198090244\n",
            "batch 101 loss: 1.6804770652600565\n",
            "batch 201 loss: 1.947880342202261\n",
            "batch 301 loss: 1.8404925826241378\n",
            "batch 401 loss: 1.657075978280409\n",
            "batch 501 loss: 1.739574791684281\n",
            "batch 601 loss: 1.4445613644557307\n",
            "batch 701 loss: 2.011896036305261\n",
            "batch 801 loss: 1.9078904985148073\n",
            "batch 901 loss: 1.8229282639059603\n",
            "batch 1001 loss: 1.9459440044191434\n",
            "batch 1101 loss: 1.6051060655643232\n",
            "batch 1201 loss: 1.704610874413047\n",
            "batch 1301 loss: 1.5311285646210855\n",
            "batch 1401 loss: 1.576773401688393\n",
            "batch 1501 loss: 1.738922201260002\n",
            "batch 1601 loss: 1.81344634704059\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.689806 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9503414333704859\n",
            "batch 101 loss: 1.7661281101615167\n",
            "batch 201 loss: 1.745518161940854\n",
            "batch 301 loss: 1.6022034242702647\n",
            "batch 401 loss: 1.843775863760675\n",
            "batch 501 loss: 1.722224774472852\n",
            "batch 601 loss: 2.182221255078912\n",
            "batch 701 loss: 1.6639615307358326\n",
            "batch 801 loss: 1.7650692854744556\n",
            "batch 901 loss: 1.7011441353324335\n",
            "batch 1001 loss: 1.6377337005425943\n",
            "batch 1101 loss: 1.7779877030744684\n",
            "batch 1201 loss: 1.6698526621772907\n",
            "batch 1301 loss: 1.5075391326592944\n",
            "batch 1401 loss: 1.7643333054048709\n",
            "batch 1501 loss: 1.5559854793007253\n",
            "batch 1601 loss: 1.8394275382181513\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.829115 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9368052313337103\n",
            "batch 101 loss: 1.7772118161339312\n",
            "batch 201 loss: 1.868144684722356\n",
            "batch 301 loss: 1.5811656688719813\n",
            "batch 401 loss: 1.6934154792966\n",
            "batch 501 loss: 1.7943531093164347\n",
            "batch 601 loss: 1.6173212649240214\n",
            "batch 701 loss: 1.9191431100302725\n",
            "batch 801 loss: 1.7803703794951524\n",
            "batch 901 loss: 1.7419295899604912\n",
            "batch 1001 loss: 1.8957854913105257\n",
            "batch 1101 loss: 1.6475947064955836\n",
            "batch 1201 loss: 1.6315219686191995\n",
            "batch 1301 loss: 1.7129763623467307\n",
            "batch 1401 loss: 1.6587657246459275\n",
            "batch 1501 loss: 1.7976550596684684\n",
            "batch 1601 loss: 1.5807925112667727\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.689003 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1533239779621363\n",
            "batch 101 loss: 1.73279784381245\n",
            "batch 201 loss: 1.8581017581722699\n",
            "batch 301 loss: 1.9543609931092942\n",
            "batch 401 loss: 2.048029055772349\n",
            "batch 501 loss: 1.6653675978421234\n",
            "batch 601 loss: 1.8814819815113242\n",
            "batch 701 loss: 1.457175265896076\n",
            "batch 801 loss: 1.6764576524184667\n",
            "batch 901 loss: 1.5818386311922223\n",
            "batch 1001 loss: 1.7698609332513298\n",
            "batch 1101 loss: 1.6486975535582316\n",
            "batch 1201 loss: 1.7140413880283631\n",
            "batch 1301 loss: 1.8043390891927993\n",
            "batch 1401 loss: 1.6192567066207995\n",
            "batch 1501 loss: 1.4564604308991693\n",
            "batch 1601 loss: 1.5746441786334617\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.700479 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0693247876642271\n",
            "batch 101 loss: 1.496701966658584\n",
            "batch 201 loss: 1.6420844547497109\n",
            "batch 301 loss: 1.5221016302382486\n",
            "batch 401 loss: 1.8886170918173593\n",
            "batch 501 loss: 1.753763562607346\n",
            "batch 601 loss: 1.7899690714245662\n",
            "batch 701 loss: 1.8307647402764087\n",
            "batch 801 loss: 1.9846702048655789\n",
            "batch 901 loss: 1.8560240479453933\n",
            "batch 1001 loss: 1.488163417118776\n",
            "batch 1101 loss: 1.4100821075964995\n",
            "batch 1201 loss: 1.6499421665829141\n",
            "batch 1301 loss: 1.9315951895155012\n",
            "batch 1401 loss: 1.6404698147112504\n",
            "batch 1501 loss: 1.7824858709749243\n",
            "batch 1601 loss: 1.8190252544971737\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.767667 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0206671505980194\n",
            "batch 101 loss: 1.8603638516192587\n",
            "batch 201 loss: 1.650850538056984\n",
            "batch 301 loss: 1.9656626244867221\n",
            "batch 401 loss: 1.4642319116543512\n",
            "batch 501 loss: 1.6854266154242215\n",
            "batch 601 loss: 1.94786796476139\n",
            "batch 701 loss: 1.9349202945131765\n",
            "batch 801 loss: 1.596107187386515\n",
            "batch 901 loss: 1.5729987262995564\n",
            "batch 1001 loss: 1.5670906295860185\n",
            "batch 1101 loss: 1.7637214440177196\n",
            "batch 1201 loss: 1.7558713674524915\n",
            "batch 1301 loss: 1.5295260355059872\n",
            "batch 1401 loss: 1.593143776189536\n",
            "batch 1501 loss: 1.944338078773435\n",
            "batch 1601 loss: 1.5749142625564128\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.635426 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0932498080737423\n",
            "batch 101 loss: 1.6807633686008194\n",
            "batch 201 loss: 1.6420877633952113\n",
            "batch 301 loss: 1.678372145195608\n",
            "batch 401 loss: 1.8729157426399796\n",
            "batch 501 loss: 1.7329480607098957\n",
            "batch 601 loss: 1.5062545485973533\n",
            "batch 701 loss: 1.6359781578567345\n",
            "batch 801 loss: 2.1364525837303883\n",
            "batch 901 loss: 1.735984979919158\n",
            "batch 1001 loss: 1.7042396714526695\n",
            "batch 1101 loss: 1.7293029283167562\n",
            "batch 1201 loss: 1.9314154685442917\n",
            "batch 1301 loss: 1.5676260147523136\n",
            "batch 1401 loss: 1.7123331755585969\n",
            "batch 1501 loss: 1.7706708409302518\n",
            "batch 1601 loss: 1.5829666071387327\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.619645 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0605901519628242\n",
            "batch 101 loss: 1.8230925449351707\n",
            "batch 201 loss: 1.8689985919580794\n",
            "batch 301 loss: 1.7454299772484227\n",
            "batch 401 loss: 1.4907442049693782\n",
            "batch 501 loss: 1.636539968892887\n",
            "batch 601 loss: 1.7773830702342093\n",
            "batch 701 loss: 1.5640531676852334\n",
            "batch 801 loss: 1.9098225690328399\n",
            "batch 901 loss: 1.770410922721494\n",
            "batch 1001 loss: 1.940659246619325\n",
            "batch 1101 loss: 1.850681074364111\n",
            "batch 1201 loss: 1.9120664468478208\n",
            "batch 1301 loss: 1.4821548193343914\n",
            "batch 1401 loss: 1.5288456963071075\n",
            "batch 1501 loss: 1.6225643590063554\n",
            "batch 1601 loss: 1.4937352789872238\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.620410 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2779335950456152\n",
            "batch 101 loss: 1.6893201682018115\n",
            "batch 201 loss: 1.6766388425626064\n",
            "batch 301 loss: 2.139691480631227\n",
            "batch 401 loss: 1.6567514746965026\n",
            "batch 501 loss: 1.659276495584054\n",
            "batch 601 loss: 1.633509013876901\n",
            "batch 701 loss: 1.81930531644859\n",
            "batch 801 loss: 1.7747133698995459\n",
            "batch 901 loss: 1.758784630097216\n",
            "batch 1001 loss: 1.5567675282078925\n",
            "batch 1101 loss: 1.592038821115857\n",
            "batch 1201 loss: 1.8292164553326438\n",
            "batch 1301 loss: 1.796522488705814\n",
            "batch 1401 loss: 1.7788904329418438\n",
            "batch 1501 loss: 1.5387085744336946\n",
            "batch 1601 loss: 2.069370573181659\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.618571 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0999189197667874\n",
            "batch 101 loss: 1.9294350508030038\n",
            "batch 201 loss: 1.7367854474345221\n",
            "batch 301 loss: 1.438158444436267\n",
            "batch 401 loss: 1.8230396060098428\n",
            "batch 501 loss: 1.8605513121042168\n",
            "batch 601 loss: 1.8919200260646176\n",
            "batch 701 loss: 1.82925604226999\n",
            "batch 801 loss: 1.9073502566528624\n",
            "batch 901 loss: 1.7603482178458945\n",
            "batch 1001 loss: 1.5667046302295058\n",
            "batch 1101 loss: 1.5286755474191158\n",
            "batch 1201 loss: 1.5070093517405894\n",
            "batch 1301 loss: 1.8106191686762758\n",
            "batch 1401 loss: 1.8657525466871447\n",
            "batch 1501 loss: 1.7328783789655744\n",
            "batch 1601 loss: 1.634120870921593\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.651994 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0119722208403983\n",
            "batch 101 loss: 1.8565086436318234\n",
            "batch 201 loss: 1.8566109868447263\n",
            "batch 301 loss: 1.729876968227909\n",
            "batch 401 loss: 1.4955954042822122\n",
            "batch 501 loss: 1.7397635350553902\n",
            "batch 601 loss: 1.4258848395523092\n",
            "batch 701 loss: 1.544844391825609\n",
            "batch 801 loss: 1.762325647121761\n",
            "batch 901 loss: 1.9423670572694391\n",
            "batch 1001 loss: 1.8026632891455665\n",
            "batch 1101 loss: 1.5678318135688096\n",
            "batch 1201 loss: 1.61084581526462\n",
            "batch 1301 loss: 1.6871809033478349\n",
            "batch 1401 loss: 1.856686672664946\n",
            "batch 1501 loss: 1.7566599328785377\n",
            "batch 1601 loss: 1.9801085668453016\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.635044 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0253649494149977\n",
            "batch 101 loss: 1.8780306919140275\n",
            "batch 201 loss: 1.5305082675791346\n",
            "batch 301 loss: 1.9013866571872495\n",
            "batch 401 loss: 1.615904247729486\n",
            "batch 501 loss: 1.9093898763996549\n",
            "batch 601 loss: 1.6828802759991959\n",
            "batch 701 loss: 1.7526777104603024\n",
            "batch 801 loss: 1.5423700686730444\n",
            "batch 901 loss: 1.5615499296801862\n",
            "batch 1001 loss: 1.8278176768717822\n",
            "batch 1101 loss: 1.5388130953705694\n",
            "batch 1201 loss: 2.0162125314376316\n",
            "batch 1301 loss: 1.6797646360192449\n",
            "batch 1401 loss: 1.7228002056723926\n",
            "batch 1501 loss: 2.0782351780089083\n",
            "batch 1601 loss: 1.813049294525058\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.659646 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "batch 1 loss: 1.3187871016615782\n",
            "batch 101 loss: 1.645434518682273\n",
            "batch 201 loss: 1.9714205784314254\n",
            "batch 301 loss: 1.5607947451906512\n",
            "batch 401 loss: 2.121870634314182\n",
            "batch 501 loss: 1.6703425141649495\n",
            "batch 601 loss: 1.7519154696920305\n",
            "batch 701 loss: 1.8035584160441067\n",
            "batch 801 loss: 1.7680612276579268\n",
            "batch 901 loss: 1.7727676080644597\n",
            "batch 1001 loss: 1.7127727794111707\n",
            "batch 1101 loss: 1.789528203404043\n",
            "batch 1201 loss: 1.8351316422130912\n",
            "batch 1301 loss: 1.483909333295669\n",
            "batch 1401 loss: 1.85295170012163\n",
            "batch 1501 loss: 1.5415832127734028\n",
            "batch 1601 loss: 1.470001155068312\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.672688 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9983442187588661\n",
            "batch 101 loss: 1.9279912460806372\n",
            "batch 201 loss: 1.8006412735420598\n",
            "batch 301 loss: 1.4821878234841188\n",
            "batch 401 loss: 1.8475859221469728\n",
            "batch 501 loss: 1.748568636137061\n",
            "batch 601 loss: 1.5928039842576254\n",
            "batch 701 loss: 1.7836271762172737\n",
            "batch 801 loss: 2.0079907969385387\n",
            "batch 901 loss: 1.7324419812022098\n",
            "batch 1001 loss: 1.5388454980513437\n",
            "batch 1101 loss: 1.6747133307147306\n",
            "batch 1201 loss: 1.569679093840532\n",
            "batch 1301 loss: 1.7774239385598047\n",
            "batch 1401 loss: 1.9146211184421553\n",
            "batch 1501 loss: 1.3907717778004007\n",
            "batch 1601 loss: 1.8362952926760772\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.619742 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9881504834722727\n",
            "batch 101 loss: 1.6386569856099231\n",
            "batch 201 loss: 1.8765612017295665\n",
            "batch 301 loss: 2.0359452392509776\n",
            "batch 401 loss: 1.7368992806121242\n",
            "batch 501 loss: 1.4868494635128082\n",
            "batch 601 loss: 1.7950345348918928\n",
            "batch 701 loss: 1.7147742611344439\n",
            "batch 801 loss: 1.5826157317397884\n",
            "batch 901 loss: 1.7549721742909241\n",
            "batch 1001 loss: 1.6666501900623552\n",
            "batch 1101 loss: 1.7753043382481337\n",
            "batch 1201 loss: 1.8687154613477468\n",
            "batch 1301 loss: 1.852940398938954\n",
            "batch 1401 loss: 1.9146462480863555\n",
            "batch 1501 loss: 1.5744705822994\n",
            "batch 1601 loss: 1.7788090440735687\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.616889 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9770674852642697\n",
            "batch 101 loss: 1.7591182611323892\n",
            "batch 201 loss: 1.747197495883447\n",
            "batch 301 loss: 1.6915208780041575\n",
            "batch 401 loss: 1.6715944509021938\n",
            "batch 501 loss: 1.6965480663254857\n",
            "batch 601 loss: 1.6764861525118613\n",
            "batch 701 loss: 1.5563665549499275\n",
            "batch 801 loss: 1.8157635242748074\n",
            "batch 901 loss: 1.8005085365213018\n",
            "batch 1001 loss: 1.8718230902641175\n",
            "batch 1101 loss: 1.597224303108669\n",
            "batch 1201 loss: 1.8145548217790202\n",
            "batch 1301 loss: 1.7960528925537074\n",
            "batch 1401 loss: 1.8782251233857823\n",
            "batch 1501 loss: 1.5525621607417588\n",
            "batch 1601 loss: 1.69788787088386\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.627551 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "batch 1 loss: 1.225904353346632\n",
            "batch 101 loss: 1.6106605423788278\n",
            "batch 201 loss: 1.597549624509993\n",
            "batch 301 loss: 1.6492116819915827\n",
            "batch 401 loss: 1.9591634429536497\n",
            "batch 501 loss: 1.7837513257039246\n",
            "batch 601 loss: 1.6677180421912816\n",
            "batch 701 loss: 1.8489827059814707\n",
            "batch 801 loss: 1.632402035657069\n",
            "batch 901 loss: 1.8706185844202992\n",
            "batch 1001 loss: 1.6567464335705153\n",
            "batch 1101 loss: 1.5767184452184666\n",
            "batch 1201 loss: 1.881706678273622\n",
            "batch 1301 loss: 1.7271328814374374\n",
            "batch 1401 loss: 1.953821634481428\n",
            "batch 1501 loss: 1.5649430305726129\n",
            "batch 1601 loss: 1.5427573281028755\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.640944 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1883131322608824\n",
            "batch 101 loss: 1.9317982796253637\n",
            "batch 201 loss: 1.7763617349381458\n",
            "batch 301 loss: 1.720790871244899\n",
            "batch 401 loss: 1.7218811957038997\n",
            "batch 501 loss: 1.4827617419997114\n",
            "batch 601 loss: 1.4622271640392501\n",
            "batch 701 loss: 1.684572704989114\n",
            "batch 801 loss: 1.8746049879770725\n",
            "batch 901 loss: 1.6394202572730137\n",
            "batch 1001 loss: 1.988038490969775\n",
            "batch 1101 loss: 1.7047053586901166\n",
            "batch 1201 loss: 1.6440552479424513\n",
            "batch 1301 loss: 1.948686304766452\n",
            "batch 1401 loss: 2.0356477199029177\n",
            "batch 1501 loss: 1.9698286459398515\n",
            "batch 1601 loss: 1.6188863436080283\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.616246 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "batch 1 loss: 0.7950404346824144\n",
            "batch 101 loss: 1.636455103423907\n",
            "batch 201 loss: 1.6043163933804316\n",
            "batch 301 loss: 1.6338459954410791\n",
            "batch 401 loss: 1.6857148019616215\n",
            "batch 501 loss: 1.703348682618416\n",
            "batch 601 loss: 1.782897636468988\n",
            "batch 701 loss: 1.851319325223285\n",
            "batch 801 loss: 1.6406820443170727\n",
            "batch 901 loss: 1.6269959845430275\n",
            "batch 1001 loss: 1.9049785880683976\n",
            "batch 1101 loss: 1.7842644012172242\n",
            "batch 1201 loss: 1.9532235174439847\n",
            "batch 1301 loss: 1.7361533809266985\n",
            "batch 1401 loss: 1.8770082926757459\n",
            "batch 1501 loss: 1.4748464029625756\n",
            "batch 1601 loss: 1.7763197706604843\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.616842 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9324046157079283\n",
            "batch 101 loss: 1.3834725034731674\n",
            "batch 201 loss: 2.0111394871585073\n",
            "batch 301 loss: 1.9882980807009154\n",
            "batch 401 loss: 1.8190165240009082\n",
            "batch 501 loss: 1.977956072430825\n",
            "batch 601 loss: 1.614461919508176\n",
            "batch 701 loss: 1.742860262408267\n",
            "batch 801 loss: 1.8256069414490275\n",
            "batch 901 loss: 1.5455291436144034\n",
            "batch 1001 loss: 1.5490672154678031\n",
            "batch 1101 loss: 1.6474244203709532\n",
            "batch 1201 loss: 1.6351615455071442\n",
            "batch 1301 loss: 1.8819752710212925\n",
            "batch 1401 loss: 2.1703054226748644\n",
            "batch 1501 loss: 1.8018362717947456\n",
            "batch 1601 loss: 2.1012186474547367\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.943742 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8357588028660393\n",
            "batch 101 loss: 1.9843319418022929\n",
            "batch 201 loss: 1.5667131531570204\n",
            "batch 301 loss: 1.6567675629719452\n",
            "batch 401 loss: 1.5777328698663131\n",
            "batch 501 loss: 1.8096941416489427\n",
            "batch 601 loss: 1.626400915409904\n",
            "batch 701 loss: 1.5859670632013332\n",
            "batch 801 loss: 1.7253440097621024\n",
            "batch 901 loss: 1.6659460391904577\n",
            "batch 1001 loss: 1.9501169056259096\n",
            "batch 1101 loss: 1.8436200594394905\n",
            "batch 1201 loss: 1.6273488668704\n",
            "batch 1301 loss: 1.6743547791268794\n",
            "batch 1401 loss: 1.901652274269145\n",
            "batch 1501 loss: 1.7046485062755528\n",
            "batch 1601 loss: 1.7852215938881273\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.671356 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9881348932348192\n",
            "batch 101 loss: 1.6158324700778894\n",
            "batch 201 loss: 1.7624825268861606\n",
            "batch 301 loss: 1.7692349201210549\n",
            "batch 401 loss: 1.7652022822065192\n",
            "batch 501 loss: 1.832193322004241\n",
            "batch 601 loss: 1.7171484394877916\n",
            "batch 701 loss: 1.7786303062643856\n",
            "batch 801 loss: 1.710840910510742\n",
            "batch 901 loss: 1.8096428468106023\n",
            "batch 1001 loss: 1.7035571404333678\n",
            "batch 1101 loss: 1.8241071205734534\n",
            "batch 1201 loss: 1.937737813636195\n",
            "batch 1301 loss: 1.7108648348180577\n",
            "batch 1401 loss: 1.8370693382574246\n",
            "batch 1501 loss: 1.8072305386107472\n",
            "batch 1601 loss: 1.5124336152835167\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.633180 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1471686518844217\n",
            "batch 101 loss: 1.604183733859502\n",
            "batch 201 loss: 1.5413553907041204\n",
            "batch 301 loss: 1.5415508808376035\n",
            "batch 401 loss: 1.5051629520137795\n",
            "batch 501 loss: 2.0672757954674763\n",
            "batch 601 loss: 1.8460374278319067\n",
            "batch 701 loss: 1.6749280032340903\n",
            "batch 801 loss: 1.8179538271764613\n",
            "batch 901 loss: 1.7719667497937917\n",
            "batch 1001 loss: 1.6983909491472877\n",
            "batch 1101 loss: 1.577169456196716\n",
            "batch 1201 loss: 1.6090688763235812\n",
            "batch 1301 loss: 1.6607912020305957\n",
            "batch 1401 loss: 1.8201469037728384\n",
            "batch 1501 loss: 1.8233719955292604\n",
            "batch 1601 loss: 1.89654793014226\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.655967 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "batch 1 loss: 1.074416609707673\n",
            "batch 101 loss: 1.867751236933982\n",
            "batch 201 loss: 1.5787863921371172\n",
            "batch 301 loss: 1.569038452803029\n",
            "batch 401 loss: 1.7499968956573866\n",
            "batch 501 loss: 1.7009874817734818\n",
            "batch 601 loss: 2.044113713639672\n",
            "batch 701 loss: 1.8364998220821507\n",
            "batch 801 loss: 1.6358754448476247\n",
            "batch 901 loss: 1.6092491309300885\n",
            "batch 1001 loss: 1.7992627420753706\n",
            "batch 1101 loss: 1.7737710283044725\n",
            "batch 1201 loss: 1.4988372661283937\n",
            "batch 1301 loss: 1.6796499360445887\n",
            "batch 1401 loss: 1.6525024731802114\n",
            "batch 1501 loss: 1.7909227013809141\n",
            "batch 1601 loss: 2.023633596110196\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.681800 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "batch 1 loss: 1.048360489467159\n",
            "batch 101 loss: 1.8159175431542098\n",
            "batch 201 loss: 1.666851964169182\n",
            "batch 301 loss: 1.5327378851886897\n",
            "batch 401 loss: 1.679532010339899\n",
            "batch 501 loss: 1.9149321562568025\n",
            "batch 601 loss: 1.7663965545804239\n",
            "batch 701 loss: 1.7178889556089416\n",
            "batch 801 loss: 2.0122041766019536\n",
            "batch 901 loss: 1.8405921963142464\n",
            "batch 1001 loss: 1.9310825237754035\n",
            "batch 1101 loss: 1.9299984785114066\n",
            "batch 1201 loss: 1.5987290196295363\n",
            "batch 1301 loss: 1.857202612543224\n",
            "batch 1401 loss: 1.7370759346114937\n",
            "batch 1501 loss: 1.4191921765370716\n",
            "batch 1601 loss: 1.7743597103469073\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.723024 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9460455189605637\n",
            "batch 101 loss: 2.0777331750513985\n",
            "batch 201 loss: 1.864513157372712\n",
            "batch 301 loss: 1.4478362150953035\n",
            "batch 401 loss: 1.960812131408602\n",
            "batch 501 loss: 1.5449053904158063\n",
            "batch 601 loss: 1.836772518272046\n",
            "batch 701 loss: 1.735167383590924\n",
            "batch 801 loss: 1.8915856353859999\n",
            "batch 901 loss: 1.8013121082354338\n",
            "batch 1001 loss: 1.747917933689896\n",
            "batch 1101 loss: 1.6384373528457945\n",
            "batch 1201 loss: 1.8030348333372967\n",
            "batch 1301 loss: 1.775979067199587\n",
            "batch 1401 loss: 1.8327140782109927\n",
            "batch 1501 loss: 1.832935355516238\n",
            "batch 1601 loss: 1.5634929208399262\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.684074 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9697652634748011\n",
            "batch 101 loss: 1.4615335935365874\n",
            "batch 201 loss: 1.6416867955680936\n",
            "batch 301 loss: 1.574898995283911\n",
            "batch 401 loss: 1.8076697301089553\n",
            "batch 501 loss: 1.9732792548375437\n",
            "batch 601 loss: 1.4200743925564348\n",
            "batch 701 loss: 2.060768276746385\n",
            "batch 801 loss: 1.652533268998377\n",
            "batch 901 loss: 1.9417805735912408\n",
            "batch 1001 loss: 1.7764773801906266\n",
            "batch 1101 loss: 1.4484013159098685\n",
            "batch 1201 loss: 1.7750309552859107\n",
            "batch 1301 loss: 1.8364908707397989\n",
            "batch 1401 loss: 2.1296122811525127\n",
            "batch 1501 loss: 1.9268886690400542\n",
            "batch 1601 loss: 1.7059152881032786\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.632019 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8488668660233089\n",
            "batch 101 loss: 1.916939612301685\n",
            "batch 201 loss: 1.6255362752414657\n",
            "batch 301 loss: 1.5533424735291579\n",
            "batch 401 loss: 1.8473233095929027\n",
            "batch 501 loss: 1.838280398298375\n",
            "batch 601 loss: 1.5494184711598291\n",
            "batch 701 loss: 1.7668818925274536\n",
            "batch 801 loss: 1.9694326728497982\n",
            "batch 901 loss: 1.69902250402989\n",
            "batch 1001 loss: 2.0091489206207918\n",
            "batch 1101 loss: 1.6162372995383338\n",
            "batch 1201 loss: 1.8008528529340402\n",
            "batch 1301 loss: 1.5952951622944966\n",
            "batch 1401 loss: 1.5861414487520233\n",
            "batch 1501 loss: 1.6778215432388242\n",
            "batch 1601 loss: 1.607280008068119\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.621039 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0331422755867243\n",
            "batch 101 loss: 1.7305964696323644\n",
            "batch 201 loss: 1.794866303744202\n",
            "batch 301 loss: 1.7756544552011473\n",
            "batch 401 loss: 1.7068789650825784\n",
            "batch 501 loss: 1.7693798198894364\n",
            "batch 601 loss: 1.5932375624208361\n",
            "batch 701 loss: 1.81892084584033\n",
            "batch 801 loss: 1.6346370090826532\n",
            "batch 901 loss: 1.8199083569437788\n",
            "batch 1001 loss: 1.651996737687732\n",
            "batch 1101 loss: 1.904662601817763\n",
            "batch 1201 loss: 1.4861510654428276\n",
            "batch 1301 loss: 2.051759883903433\n",
            "batch 1401 loss: 1.6103487143304664\n",
            "batch 1501 loss: 1.7259538580716094\n",
            "batch 1601 loss: 1.6759839083146653\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.628449 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0014437490272394\n",
            "batch 101 loss: 1.8776195021043531\n",
            "batch 201 loss: 1.563817125163041\n",
            "batch 301 loss: 1.6763246226403863\n",
            "batch 401 loss: 1.8784433985460782\n",
            "batch 501 loss: 1.7313181658985384\n",
            "batch 601 loss: 1.704818128015995\n",
            "batch 701 loss: 1.7148295422487445\n",
            "batch 801 loss: 1.7056997804390266\n",
            "batch 901 loss: 1.6632009380950352\n",
            "batch 1001 loss: 1.6683674287619579\n",
            "batch 1101 loss: 1.6807464254985098\n",
            "batch 1201 loss: 1.9431425670837053\n",
            "batch 1301 loss: 1.885155040104146\n",
            "batch 1401 loss: 1.6808865756096203\n",
            "batch 1501 loss: 1.8401164370344487\n",
            "batch 1601 loss: 1.6831770174193297\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.743053 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "batch 1 loss: 1.058839962054335\n",
            "batch 101 loss: 1.6787621564991242\n",
            "batch 201 loss: 1.6716104856437597\n",
            "batch 301 loss: 1.8410577787087095\n",
            "batch 401 loss: 1.948925074543804\n",
            "batch 501 loss: 2.157754902783199\n",
            "batch 601 loss: 1.641932568901757\n",
            "batch 701 loss: 1.5560655621497426\n",
            "batch 801 loss: 1.7235503642680123\n",
            "batch 901 loss: 1.687985506777477\n",
            "batch 1001 loss: 1.5586473638791358\n",
            "batch 1101 loss: 1.7869443423138727\n",
            "batch 1201 loss: 1.7273153511458077\n",
            "batch 1301 loss: 1.4874244012587587\n",
            "batch 1401 loss: 1.6313181245303712\n",
            "batch 1501 loss: 1.8470999052000117\n",
            "batch 1601 loss: 1.6346828394035766\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.663942 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9009621413098649\n",
            "batch 101 loss: 1.8227329117245972\n",
            "batch 201 loss: 1.6340239396299967\n",
            "batch 301 loss: 1.6979466582319946\n",
            "batch 401 loss: 1.6898105459356156\n",
            "batch 501 loss: 1.7966927557135932\n",
            "batch 601 loss: 1.5540374589397106\n",
            "batch 701 loss: 1.704496131632186\n",
            "batch 801 loss: 2.042035565460101\n",
            "batch 901 loss: 1.7141859035089146\n",
            "batch 1001 loss: 1.5517424336160912\n",
            "batch 1101 loss: 1.7248106369323795\n",
            "batch 1201 loss: 1.7673903329367748\n",
            "batch 1301 loss: 2.069631565017626\n",
            "batch 1401 loss: 1.7147254206698563\n",
            "batch 1501 loss: 1.6907263367716223\n",
            "batch 1601 loss: 1.7580296457023359\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.726894 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "batch 1 loss: 0.7933804295468281\n",
            "batch 101 loss: 1.6388421995632985\n",
            "batch 201 loss: 1.714677869513398\n",
            "batch 301 loss: 1.9627485236362554\n",
            "batch 401 loss: 1.6491709426350372\n",
            "batch 501 loss: 2.0899636870750693\n",
            "batch 601 loss: 1.4918467271030387\n",
            "batch 701 loss: 1.938580016780179\n",
            "batch 801 loss: 1.8025282563447218\n",
            "batch 901 loss: 1.6764294228679502\n",
            "batch 1001 loss: 1.741115751156758\n",
            "batch 1101 loss: 1.5256101417075842\n",
            "batch 1201 loss: 1.6243908386080381\n",
            "batch 1301 loss: 1.7127649072692293\n",
            "batch 1401 loss: 1.6632951966218343\n",
            "batch 1501 loss: 1.7975509864429478\n",
            "batch 1601 loss: 1.9425045303057413\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.825143 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8801181619102135\n",
            "batch 101 loss: 1.7087773762224243\n",
            "batch 201 loss: 1.7740188029262935\n",
            "batch 301 loss: 1.7094470733414346\n",
            "batch 401 loss: 1.732454876555712\n",
            "batch 501 loss: 1.8336766302358591\n",
            "batch 601 loss: 1.814041407883924\n",
            "batch 701 loss: 1.6681443879872677\n",
            "batch 801 loss: 1.7370269560767337\n",
            "batch 901 loss: 1.5712586372644364\n",
            "batch 1001 loss: 1.5809940954187187\n",
            "batch 1101 loss: 1.5741857849784815\n",
            "batch 1201 loss: 1.8679220751814136\n",
            "batch 1301 loss: 1.7177696134138387\n",
            "batch 1401 loss: 1.483777444564621\n",
            "batch 1501 loss: 1.392413003866677\n",
            "batch 1601 loss: 1.6705667273224936\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.623412 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "batch 1 loss: 1.170793064357713\n",
            "batch 101 loss: 1.722735993845854\n",
            "batch 201 loss: 1.7270126654044724\n",
            "batch 301 loss: 1.5371463771545677\n",
            "batch 401 loss: 1.7104206147044898\n",
            "batch 501 loss: 1.8853653650920774\n",
            "batch 601 loss: 1.985121218845452\n",
            "batch 701 loss: 1.8425669901946093\n",
            "batch 801 loss: 1.7828418147970615\n",
            "batch 901 loss: 1.9391934733989182\n",
            "batch 1001 loss: 1.7754754872500984\n",
            "batch 1101 loss: 1.7238039436697\n",
            "batch 1201 loss: 1.5602396344846783\n",
            "batch 1301 loss: 1.7725910178145567\n",
            "batch 1401 loss: 1.7625280546027353\n",
            "batch 1501 loss: 1.3279662615733105\n",
            "batch 1601 loss: 1.7852843734773343\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.660611 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "batch 1 loss: 1.014122264358666\n",
            "batch 101 loss: 1.7508188144216548\n",
            "batch 201 loss: 1.7270528834592551\n",
            "batch 301 loss: 1.8725376187171787\n",
            "batch 401 loss: 1.8481873384251957\n",
            "batch 501 loss: 1.6211696917537666\n",
            "batch 601 loss: 1.7883345070295036\n",
            "batch 701 loss: 1.6516534354398027\n",
            "batch 801 loss: 1.9653215366369112\n",
            "batch 901 loss: 1.831287067849189\n",
            "batch 1001 loss: 1.7485222800748397\n",
            "batch 1101 loss: 1.9129216233034185\n",
            "batch 1201 loss: 1.6214484091561463\n",
            "batch 1301 loss: 1.8099851580216273\n",
            "batch 1401 loss: 1.6167295749182813\n",
            "batch 1501 loss: 1.5508331694209483\n",
            "batch 1601 loss: 1.7397088581789284\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.615884 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8522711185319349\n",
            "batch 101 loss: 1.7135466817533598\n",
            "batch 201 loss: 2.0150477559131104\n",
            "batch 301 loss: 1.78741115962388\n",
            "batch 401 loss: 1.8118620102679415\n",
            "batch 501 loss: 1.4295394401415251\n",
            "batch 601 loss: 1.946535142196226\n",
            "batch 701 loss: 1.791168571690214\n",
            "batch 801 loss: 1.3999099694652615\n",
            "batch 901 loss: 1.8365124408859992\n",
            "batch 1001 loss: 1.6133259274740703\n",
            "batch 1101 loss: 1.8131836378085426\n",
            "batch 1201 loss: 1.6709893343783915\n",
            "batch 1301 loss: 1.7369455358503911\n",
            "batch 1401 loss: 2.0378036225712277\n",
            "batch 1501 loss: 1.7845168444281443\n",
            "batch 1601 loss: 1.6169725099229253\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.615771 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8436003665705999\n",
            "batch 101 loss: 1.6922275148998596\n",
            "batch 201 loss: 1.6944680999475532\n",
            "batch 301 loss: 1.9156128191356607\n",
            "batch 401 loss: 1.9479021012637896\n",
            "batch 501 loss: 1.7579755896692222\n",
            "batch 601 loss: 1.798812915722956\n",
            "batch 701 loss: 1.3383555915523722\n",
            "batch 801 loss: 1.5236411237546053\n",
            "batch 901 loss: 1.7901333844009786\n",
            "batch 1001 loss: 1.5011452709643844\n",
            "batch 1101 loss: 1.6016598558278565\n",
            "batch 1201 loss: 1.9328086019319017\n",
            "batch 1301 loss: 1.805457264052739\n",
            "batch 1401 loss: 1.8431499669654294\n",
            "batch 1501 loss: 1.7513038000950474\n",
            "batch 1601 loss: 1.722127671726048\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.714499 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9840716565179173\n",
            "batch 101 loss: 1.4501321085475503\n",
            "batch 201 loss: 1.7427524767245632\n",
            "batch 301 loss: 1.75239096774043\n",
            "batch 401 loss: 1.8420444169957773\n",
            "batch 501 loss: 1.844568473703839\n",
            "batch 601 loss: 1.8531881544092266\n",
            "batch 701 loss: 1.7507956089524668\n",
            "batch 801 loss: 1.7298052110197022\n",
            "batch 901 loss: 1.6478537243569735\n",
            "batch 1001 loss: 1.7768512940424261\n",
            "batch 1101 loss: 1.954680053582415\n",
            "batch 1201 loss: 1.6640457924635847\n",
            "batch 1301 loss: 1.8620547495855135\n",
            "batch 1401 loss: 1.5994402079712382\n",
            "batch 1501 loss: 1.4589484854289185\n",
            "batch 1601 loss: 1.6746994492481462\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.926949 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "batch 1 loss: 0.7603304716071579\n",
            "batch 101 loss: 1.9736074874456973\n",
            "batch 201 loss: 1.7628785061859527\n",
            "batch 301 loss: 1.7061187442147638\n",
            "batch 401 loss: 1.7916741410877148\n",
            "batch 501 loss: 2.0304265159135686\n",
            "batch 601 loss: 1.8032324031181632\n",
            "batch 701 loss: 1.7756053386571966\n",
            "batch 801 loss: 1.8682753147254698\n",
            "batch 901 loss: 1.7529437837392106\n",
            "batch 1001 loss: 1.6848415455844952\n",
            "batch 1101 loss: 1.6544342419577152\n",
            "batch 1201 loss: 1.3721939881655272\n",
            "batch 1301 loss: 1.7148007838210377\n",
            "batch 1401 loss: 2.0438213883445133\n",
            "batch 1501 loss: 1.7339699491289189\n",
            "batch 1601 loss: 1.6409850514613413\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.933720 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1092973150499166\n",
            "batch 101 loss: 1.7210626491054426\n",
            "batch 201 loss: 1.6690164805497625\n",
            "batch 301 loss: 1.6721741205316563\n",
            "batch 401 loss: 1.733583709606901\n",
            "batch 501 loss: 1.6763333411555505\n",
            "batch 601 loss: 1.883900949065719\n",
            "batch 701 loss: 1.8265166055224835\n",
            "batch 801 loss: 1.8399272927854349\n",
            "batch 901 loss: 1.8087496530210774\n",
            "batch 1001 loss: 1.7597175601124764\n",
            "batch 1101 loss: 1.531353373855818\n",
            "batch 1201 loss: 1.591390143937897\n",
            "batch 1301 loss: 1.546974489007116\n",
            "batch 1401 loss: 1.8277695117672557\n",
            "batch 1501 loss: 1.6243171946619988\n",
            "batch 1601 loss: 1.7926284189935542\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.808156 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1882257484644652\n",
            "batch 101 loss: 1.6555897949763676\n",
            "batch 201 loss: 1.5591529669371085\n",
            "batch 301 loss: 1.7165394778445262\n",
            "batch 401 loss: 1.4411228445080633\n",
            "batch 501 loss: 2.066239622288267\n",
            "batch 601 loss: 1.6594431900174822\n",
            "batch 701 loss: 1.7739848245781018\n",
            "batch 801 loss: 1.7050190313498024\n",
            "batch 901 loss: 1.8606078184833676\n",
            "batch 1001 loss: 1.8276500398141797\n",
            "batch 1101 loss: 1.776685191229517\n",
            "batch 1201 loss: 1.5787540365172026\n",
            "batch 1301 loss: 1.762857107471209\n",
            "batch 1401 loss: 1.9903923211549408\n",
            "batch 1501 loss: 1.6210750481821015\n",
            "batch 1601 loss: 1.728451422425569\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.627812 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9485615346553096\n",
            "batch 101 loss: 1.63370479258243\n",
            "batch 201 loss: 1.626482450271724\n",
            "batch 301 loss: 1.9735494832289988\n",
            "batch 401 loss: 1.6137840038142166\n",
            "batch 501 loss: 1.8467017189052422\n",
            "batch 601 loss: 1.7262804136320482\n",
            "batch 701 loss: 1.6580331755802036\n",
            "batch 801 loss: 1.6490668977124734\n",
            "batch 901 loss: 1.5956207746628206\n",
            "batch 1001 loss: 2.14047977202099\n",
            "batch 1101 loss: 1.4765394911469776\n",
            "batch 1201 loss: 1.7882028824836016\n",
            "batch 1301 loss: 1.7560133932824464\n",
            "batch 1401 loss: 1.6861401734675747\n",
            "batch 1501 loss: 1.6178711436560844\n",
            "batch 1601 loss: 1.7094071601331233\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.658618 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0315209575090558\n",
            "batch 101 loss: 1.9739584967726842\n",
            "batch 201 loss: 1.8050614928500728\n",
            "batch 301 loss: 1.8124977129185573\n",
            "batch 401 loss: 1.5034175230469555\n",
            "batch 501 loss: 1.788877519495436\n",
            "batch 601 loss: 1.5941610051481985\n",
            "batch 701 loss: 1.8234690732319359\n",
            "batch 801 loss: 1.5616672632552218\n",
            "batch 901 loss: 1.6347275689948584\n",
            "batch 1001 loss: 1.6718852642168713\n",
            "batch 1101 loss: 1.7263170612463727\n",
            "batch 1201 loss: 1.4488328063885274\n",
            "batch 1301 loss: 1.7128157294169069\n",
            "batch 1401 loss: 2.158529764347477\n",
            "batch 1501 loss: 2.001349209020846\n",
            "batch 1601 loss: 1.6905770476500037\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.843385 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "batch 1 loss: 0.962761097769835\n",
            "batch 101 loss: 1.8410468616880826\n",
            "batch 201 loss: 2.0432593254954554\n",
            "batch 301 loss: 1.5123008401978586\n",
            "batch 401 loss: 1.9001470560440794\n",
            "batch 501 loss: 1.824406865275232\n",
            "batch 601 loss: 1.742079762921203\n",
            "batch 701 loss: 1.6712847876921295\n",
            "batch 801 loss: 1.7256383449200074\n",
            "batch 901 loss: 1.8090961274370057\n",
            "batch 1001 loss: 1.4498667487081547\n",
            "batch 1101 loss: 1.7123410107381642\n",
            "batch 1201 loss: 1.68129256732371\n",
            "batch 1301 loss: 1.523921766281128\n",
            "batch 1401 loss: 1.6188104701516568\n",
            "batch 1501 loss: 1.7240944683062844\n",
            "batch 1601 loss: 1.636222606600495\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.763096 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9787147696732427\n",
            "batch 101 loss: 1.8112887695514654\n",
            "batch 201 loss: 1.519556920529576\n",
            "batch 301 loss: 1.808137303039732\n",
            "batch 401 loss: 1.716894850594399\n",
            "batch 501 loss: 1.8592711104542103\n",
            "batch 601 loss: 1.6517469255882316\n",
            "batch 701 loss: 1.8263426328855075\n",
            "batch 801 loss: 1.6861110884998924\n",
            "batch 901 loss: 1.768857080389862\n",
            "batch 1001 loss: 1.7602630296342636\n",
            "batch 1101 loss: 1.605721290833917\n",
            "batch 1201 loss: 1.494577881511068\n",
            "batch 1301 loss: 1.4856322405516402\n",
            "batch 1401 loss: 1.622743937669011\n",
            "batch 1501 loss: 1.8101819318544585\n",
            "batch 1601 loss: 2.0243094565236244\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.623702 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9125552222365514\n",
            "batch 101 loss: 1.7578964484995232\n",
            "batch 201 loss: 1.6263804669631645\n",
            "batch 301 loss: 1.7398906400448413\n",
            "batch 401 loss: 1.7662372228816274\n",
            "batch 501 loss: 1.969679058418842\n",
            "batch 601 loss: 1.6782924755028217\n",
            "batch 701 loss: 1.9621248639025726\n",
            "batch 801 loss: 1.5506846567930188\n",
            "batch 901 loss: 1.772209029983478\n",
            "batch 1001 loss: 1.8000224780887948\n",
            "batch 1101 loss: 1.4690104483060713\n",
            "batch 1201 loss: 1.7327926735999062\n",
            "batch 1301 loss: 1.918396474844667\n",
            "batch 1401 loss: 1.6468536374883025\n",
            "batch 1501 loss: 1.6853409317226034\n",
            "batch 1601 loss: 1.8424578829364144\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.838914 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2323623418582519\n",
            "batch 101 loss: 1.8119690976682432\n",
            "batch 201 loss: 1.6721553888730705\n",
            "batch 301 loss: 1.6518274591007502\n",
            "batch 401 loss: 1.7301011575199663\n",
            "batch 501 loss: 1.8145344712585212\n",
            "batch 601 loss: 1.760215654615895\n",
            "batch 701 loss: 1.7530294275810592\n",
            "batch 801 loss: 1.8098907787082226\n",
            "batch 901 loss: 1.751037560268851\n",
            "batch 1001 loss: 1.867517281571341\n",
            "batch 1101 loss: 1.8993081023331615\n",
            "batch 1201 loss: 1.6840874204318335\n",
            "batch 1301 loss: 1.4808965245375294\n",
            "batch 1401 loss: 1.6156569552794826\n",
            "batch 1501 loss: 1.5832225453748834\n",
            "batch 1601 loss: 1.8330519812877173\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 2.534131 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "batch 1 loss: 0.7689025718155699\n",
            "batch 101 loss: 1.827748443974415\n",
            "batch 201 loss: 1.695294982419591\n",
            "batch 301 loss: 1.6596800060518353\n",
            "batch 401 loss: 1.7683423083365073\n",
            "batch 501 loss: 1.5660761730303057\n",
            "batch 601 loss: 1.9998156170453876\n",
            "batch 701 loss: 1.8481598617893178\n",
            "batch 801 loss: 1.6928088913520332\n",
            "batch 901 loss: 1.5392071555012081\n",
            "batch 1001 loss: 1.518695180413779\n",
            "batch 1101 loss: 1.5047060542028339\n",
            "batch 1201 loss: 1.6861621159685456\n",
            "batch 1301 loss: 1.8134851565747523\n",
            "batch 1401 loss: 1.8604862920753658\n",
            "batch 1501 loss: 1.8643968682858394\n",
            "batch 1601 loss: 1.8525761221749781\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.627793 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0057029270776547\n",
            "batch 101 loss: 1.546070510968566\n",
            "batch 201 loss: 1.5958288787920538\n",
            "batch 301 loss: 1.9374559791659705\n",
            "batch 401 loss: 1.7575352127756922\n",
            "batch 501 loss: 1.7173654485843872\n",
            "batch 601 loss: 1.668428493528918\n",
            "batch 701 loss: 1.8433763967026606\n",
            "batch 801 loss: 1.7155125041812425\n",
            "batch 901 loss: 1.7424421764662839\n",
            "batch 1001 loss: 1.9019570908992318\n",
            "batch 1101 loss: 1.813997475055803\n",
            "batch 1201 loss: 1.5856748378858903\n",
            "batch 1301 loss: 1.592694046862307\n",
            "batch 1401 loss: 1.8836689233261859\n",
            "batch 1501 loss: 1.7165364820579998\n",
            "batch 1601 loss: 2.092391028207494\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.661449 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0820411906403025\n",
            "batch 101 loss: 1.6567390811321092\n",
            "batch 201 loss: 1.8272029650164767\n",
            "batch 301 loss: 1.8924478861829266\n",
            "batch 401 loss: 1.70947268611053\n",
            "batch 501 loss: 1.67601957114588\n",
            "batch 601 loss: 1.8183091642730869\n",
            "batch 701 loss: 1.8792435755685437\n",
            "batch 801 loss: 1.899557949238806\n",
            "batch 901 loss: 1.9351122461864725\n",
            "batch 1001 loss: 1.7648235653075972\n",
            "batch 1101 loss: 1.5480277363664117\n",
            "batch 1201 loss: 1.7696927469117874\n",
            "batch 1301 loss: 1.6383938799143651\n",
            "batch 1401 loss: 2.0079473512020196\n",
            "batch 1501 loss: 1.4749816910504705\n",
            "batch 1601 loss: 1.4657923872217362\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.646724 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0902779886691132\n",
            "batch 101 loss: 1.6786539626168087\n",
            "batch 201 loss: 1.679597268300131\n",
            "batch 301 loss: 1.621158519097371\n",
            "batch 401 loss: 1.7086098627845059\n",
            "batch 501 loss: 1.8688442926965763\n",
            "batch 601 loss: 1.601480500795751\n",
            "batch 701 loss: 1.7600845397310332\n",
            "batch 801 loss: 1.5366325648897328\n",
            "batch 901 loss: 1.8242810948519037\n",
            "batch 1001 loss: 1.5486898086231669\n",
            "batch 1101 loss: 2.042654646325973\n",
            "batch 1201 loss: 1.7437296300926937\n",
            "batch 1301 loss: 1.68231603788312\n",
            "batch 1401 loss: 1.7489040641859175\n",
            "batch 1501 loss: 1.7111551843216875\n",
            "batch 1601 loss: 1.687629677072109\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 2.267948 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "batch 1 loss: 1.253050495875359\n",
            "batch 101 loss: 1.7951801573333797\n",
            "batch 201 loss: 1.9383490666368743\n",
            "batch 301 loss: 1.6595953182247467\n",
            "batch 401 loss: 1.6355597162904452\n",
            "batch 501 loss: 1.8026053922908614\n",
            "batch 601 loss: 1.9739722179237287\n",
            "batch 701 loss: 1.755485384450294\n",
            "batch 801 loss: 1.6664256667437802\n",
            "batch 901 loss: 1.54470197775634\n",
            "batch 1001 loss: 1.7287446371434454\n",
            "batch 1101 loss: 1.8958941161650182\n",
            "batch 1201 loss: 1.788492140135786\n",
            "batch 1301 loss: 1.649529955277103\n",
            "batch 1401 loss: 1.4529470287238018\n",
            "batch 1501 loss: 1.6396351813385264\n",
            "batch 1601 loss: 1.9228977485934593\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.807768 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "batch 1 loss: 1.019247872913693\n",
            "batch 101 loss: 1.8232270542757851\n",
            "batch 201 loss: 1.6970846172354503\n",
            "batch 301 loss: 1.7964752078265882\n",
            "batch 401 loss: 1.813826120658341\n",
            "batch 501 loss: 1.905362447028747\n",
            "batch 601 loss: 1.614855104126168\n",
            "batch 701 loss: 1.600733122510137\n",
            "batch 801 loss: 1.8117612382100197\n",
            "batch 901 loss: 1.5589236557104595\n",
            "batch 1001 loss: 1.8061079514812446\n",
            "batch 1101 loss: 1.6658717241523846\n",
            "batch 1201 loss: 1.7611447052186122\n",
            "batch 1301 loss: 1.604120199168101\n",
            "batch 1401 loss: 1.638651831151219\n",
            "batch 1501 loss: 1.8522478404510183\n",
            "batch 1601 loss: 2.0219814729822247\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.616837 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9061818520492761\n",
            "batch 101 loss: 1.519108573872363\n",
            "batch 201 loss: 1.7932317393875565\n",
            "batch 301 loss: 1.5930358779833478\n",
            "batch 401 loss: 1.632128573395894\n",
            "batch 501 loss: 1.7721482423588168\n",
            "batch 601 loss: 1.6957930689533531\n",
            "batch 701 loss: 1.7250706088315928\n",
            "batch 801 loss: 1.8905270796670812\n",
            "batch 901 loss: 1.9689155351999217\n",
            "batch 1001 loss: 1.846299622328952\n",
            "batch 1101 loss: 1.8466381497803377\n",
            "batch 1201 loss: 1.909520807414374\n",
            "batch 1301 loss: 1.6285429507959635\n",
            "batch 1401 loss: 1.7725910168581869\n",
            "batch 1501 loss: 1.9243997100768866\n",
            "batch 1601 loss: 1.6560279309633188\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.928908 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0667817188054323\n",
            "batch 101 loss: 1.6835829919180834\n",
            "batch 201 loss: 1.779769985548337\n",
            "batch 301 loss: 1.8426836073381128\n",
            "batch 401 loss: 1.8824832375277765\n",
            "batch 501 loss: 1.7176330702533595\n",
            "batch 601 loss: 1.5021632239152678\n",
            "batch 701 loss: 1.603302872177519\n",
            "batch 801 loss: 1.6715864625305403\n",
            "batch 901 loss: 1.84307487069309\n",
            "batch 1001 loss: 1.6406382750999182\n",
            "batch 1101 loss: 1.6042373104698708\n",
            "batch 1201 loss: 1.8066747212625343\n",
            "batch 1301 loss: 1.8586312400306633\n",
            "batch 1401 loss: 1.6982724116940517\n",
            "batch 1501 loss: 1.5336682391690557\n",
            "batch 1601 loss: 2.024576989239431\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.775218 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1651898434385657\n",
            "batch 101 loss: 1.7136210036313082\n",
            "batch 201 loss: 1.60129688469111\n",
            "batch 301 loss: 1.6908312297496013\n",
            "batch 401 loss: 1.7667966892992262\n",
            "batch 501 loss: 1.828963463931832\n",
            "batch 601 loss: 2.0267032954514277\n",
            "batch 701 loss: 1.6548064062817138\n",
            "batch 801 loss: 1.6472036361391655\n",
            "batch 901 loss: 1.8484091819886816\n",
            "batch 1001 loss: 1.7118440983328036\n",
            "batch 1101 loss: 1.911034016251797\n",
            "batch 1201 loss: 1.6572735390346498\n",
            "batch 1301 loss: 1.7762304727104492\n",
            "batch 1401 loss: 1.7752425860916263\n",
            "batch 1501 loss: 1.6135086174472235\n",
            "batch 1601 loss: 1.7163676107442007\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.646380 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8544010116920435\n",
            "batch 101 loss: 1.5152094015356852\n",
            "batch 201 loss: 1.6257382301660255\n",
            "batch 301 loss: 1.716893812135677\n",
            "batch 401 loss: 1.7630858061357868\n",
            "batch 501 loss: 1.61068341190673\n",
            "batch 601 loss: 1.8213615847364417\n",
            "batch 701 loss: 1.6963554013802786\n",
            "batch 801 loss: 1.8557901615379524\n",
            "batch 901 loss: 1.546673511993797\n",
            "batch 1001 loss: 1.6468695904873312\n",
            "batch 1101 loss: 1.7720500334422105\n",
            "batch 1201 loss: 1.8414632048618296\n",
            "batch 1301 loss: 1.6801101017078508\n",
            "batch 1401 loss: 1.9143210098450436\n",
            "batch 1501 loss: 1.691124000349091\n",
            "batch 1601 loss: 1.674682683009014\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.747674 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2164755659317599\n",
            "batch 101 loss: 1.873946043156029\n",
            "batch 201 loss: 1.467821629405953\n",
            "batch 301 loss: 1.7290195617558857\n",
            "batch 401 loss: 1.5204465386917583\n",
            "batch 501 loss: 1.8932357865115046\n",
            "batch 601 loss: 1.798888265020796\n",
            "batch 701 loss: 2.027041030328255\n",
            "batch 801 loss: 1.7947629073355347\n",
            "batch 901 loss: 1.644925433085882\n",
            "batch 1001 loss: 1.7248476648097857\n",
            "batch 1101 loss: 1.7382352154469118\n",
            "batch 1201 loss: 1.5968838355131447\n",
            "batch 1301 loss: 1.6716423798672622\n",
            "batch 1401 loss: 1.8307078742718226\n",
            "batch 1501 loss: 1.7639868658367777\n",
            "batch 1601 loss: 1.7956786699150689\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.618011 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0010513442207594\n",
            "batch 101 loss: 1.74662922043035\n",
            "batch 201 loss: 1.673020122133603\n",
            "batch 301 loss: 1.7752872890364961\n",
            "batch 401 loss: 1.970993678951636\n",
            "batch 501 loss: 1.7829956923236023\n",
            "batch 601 loss: 1.7799554644714226\n",
            "batch 701 loss: 1.6780208485551702\n",
            "batch 801 loss: 1.8512743572538601\n",
            "batch 901 loss: 1.6674272624534934\n",
            "batch 1001 loss: 1.7358991435554345\n",
            "batch 1101 loss: 1.5045128295046744\n",
            "batch 1201 loss: 1.837821431953476\n",
            "batch 1301 loss: 1.7923641123708376\n",
            "batch 1401 loss: 1.5679569004187397\n",
            "batch 1501 loss: 1.5462514992803336\n",
            "batch 1601 loss: 1.7716576140830875\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.706050 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0193000265117735\n",
            "batch 101 loss: 1.651306869050022\n",
            "batch 201 loss: 1.7001063879625873\n",
            "batch 301 loss: 1.713871213672246\n",
            "batch 401 loss: 1.548203758692398\n",
            "batch 501 loss: 1.7652135093078323\n",
            "batch 601 loss: 1.58358400078665\n",
            "batch 701 loss: 1.5323591692476521\n",
            "batch 801 loss: 1.837853099398426\n",
            "batch 901 loss: 1.9206363434589002\n",
            "batch 1001 loss: 1.662398988588402\n",
            "batch 1101 loss: 1.932517078986857\n",
            "batch 1201 loss: 1.570124595296802\n",
            "batch 1301 loss: 1.8546597188874148\n",
            "batch 1401 loss: 1.970946664160583\n",
            "batch 1501 loss: 1.5880290140703437\n",
            "batch 1601 loss: 1.6354034885975124\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.916405 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "batch 1 loss: 1.055515810057259\n",
            "batch 101 loss: 1.433704337351155\n",
            "batch 201 loss: 1.8055059502050972\n",
            "batch 301 loss: 1.6647898925840128\n",
            "batch 401 loss: 1.9205547184473835\n",
            "batch 501 loss: 1.685171897576656\n",
            "batch 601 loss: 1.8285276055859867\n",
            "batch 701 loss: 1.8919447260699236\n",
            "batch 801 loss: 1.5700562379555776\n",
            "batch 901 loss: 1.8282186309993267\n",
            "batch 1001 loss: 1.8151258006691933\n",
            "batch 1101 loss: 1.6821540053188802\n",
            "batch 1201 loss: 1.7265389595204033\n",
            "batch 1301 loss: 1.8149827498849482\n",
            "batch 1401 loss: 1.8003980339434928\n",
            "batch 1501 loss: 1.9433382075469126\n",
            "batch 1601 loss: 1.7463389010392711\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.622912 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "batch 1 loss: 1.066226420784369\n",
            "batch 101 loss: 1.8873294258955866\n",
            "batch 201 loss: 1.401197353111347\n",
            "batch 301 loss: 1.7831428102161953\n",
            "batch 401 loss: 1.7432335949945263\n",
            "batch 501 loss: 1.7820214482839218\n",
            "batch 601 loss: 1.7962162076612003\n",
            "batch 701 loss: 1.7516181251238914\n",
            "batch 801 loss: 1.5563859082300728\n",
            "batch 901 loss: 1.560334134830482\n",
            "batch 1001 loss: 1.677928194340784\n",
            "batch 1101 loss: 1.9444365069144987\n",
            "batch 1201 loss: 1.6315856130747124\n",
            "batch 1301 loss: 1.740464840441964\n",
            "batch 1401 loss: 1.7797691066085826\n",
            "batch 1501 loss: 1.5413746214527055\n",
            "batch 1601 loss: 1.822829534355551\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.663917 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2021436423712657\n",
            "batch 101 loss: 1.7086490511719603\n",
            "batch 201 loss: 1.7398032062826678\n",
            "batch 301 loss: 1.6331151424202834\n",
            "batch 401 loss: 1.786686084944231\n",
            "batch 501 loss: 1.9823740301181898\n",
            "batch 601 loss: 1.3968208570918068\n",
            "batch 701 loss: 1.7702035442867783\n",
            "batch 801 loss: 2.0011717037010386\n",
            "batch 901 loss: 1.8662198271419037\n",
            "batch 1001 loss: 1.8545066824377683\n",
            "batch 1101 loss: 1.4667489611069322\n",
            "batch 1201 loss: 1.6016942119289888\n",
            "batch 1301 loss: 1.7874665173824178\n",
            "batch 1401 loss: 1.7064550029463135\n",
            "batch 1501 loss: 1.6187045663247408\n",
            "batch 1601 loss: 1.617878184680667\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.711171 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2605228147568415\n",
            "batch 101 loss: 1.9305224739763072\n",
            "batch 201 loss: 1.8022782764063232\n",
            "batch 301 loss: 1.7912559017265448\n",
            "batch 401 loss: 1.596323859768729\n",
            "batch 501 loss: 1.808572833977687\n",
            "batch 601 loss: 1.7678463874477892\n",
            "batch 701 loss: 1.8562201633531368\n",
            "batch 801 loss: 1.5998483757726536\n",
            "batch 901 loss: 1.7044519875207151\n",
            "batch 1001 loss: 1.7835056660136615\n",
            "batch 1101 loss: 1.6151416129918652\n",
            "batch 1201 loss: 1.5365663869469426\n",
            "batch 1301 loss: 1.4826221560651902\n",
            "batch 1401 loss: 1.7823707535365247\n",
            "batch 1501 loss: 1.8331876058085255\n",
            "batch 1601 loss: 1.698667013050963\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.648867 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0711032119113952\n",
            "batch 101 loss: 1.792146110830363\n",
            "batch 201 loss: 1.837905928582768\n",
            "batch 301 loss: 1.5620746919832709\n",
            "batch 401 loss: 1.554379868318174\n",
            "batch 501 loss: 1.8691029719356447\n",
            "batch 601 loss: 1.5601480338121296\n",
            "batch 701 loss: 1.8473461935179876\n",
            "batch 801 loss: 1.7699655264540797\n",
            "batch 901 loss: 1.9804997513796117\n",
            "batch 1001 loss: 1.9015964754661399\n",
            "batch 1101 loss: 1.844427129162441\n",
            "batch 1201 loss: 1.5487691179697867\n",
            "batch 1301 loss: 1.5962519868835807\n",
            "batch 1401 loss: 1.795656500655823\n",
            "batch 1501 loss: 1.7777177725665387\n",
            "batch 1601 loss: 1.7499957164068474\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.792292 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9554771728772903\n",
            "batch 101 loss: 1.4835240818478632\n",
            "batch 201 loss: 1.8234224865469515\n",
            "batch 301 loss: 2.0105516690976217\n",
            "batch 401 loss: 1.900958110822321\n",
            "batch 501 loss: 1.6681923159857979\n",
            "batch 601 loss: 1.6341324267662276\n",
            "batch 701 loss: 1.8818140420739018\n",
            "batch 801 loss: 1.6206169508144377\n",
            "batch 901 loss: 1.708071443689987\n",
            "batch 1001 loss: 1.888846569476882\n",
            "batch 1101 loss: 1.39175662067237\n",
            "batch 1201 loss: 1.88454787556082\n",
            "batch 1301 loss: 1.7196457154979\n",
            "batch 1401 loss: 1.8388939403276163\n",
            "batch 1501 loss: 1.6075950258923695\n",
            "batch 1601 loss: 1.891464562459223\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.622146 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "batch 1 loss: 1.046912841311423\n",
            "batch 101 loss: 1.8356485577591228\n",
            "batch 201 loss: 1.6963337841430075\n",
            "batch 301 loss: 1.7328981790548277\n",
            "batch 401 loss: 1.7197001161961816\n",
            "batch 501 loss: 1.5933569462702144\n",
            "batch 601 loss: 1.6931163265719078\n",
            "batch 701 loss: 1.653754475118185\n",
            "batch 801 loss: 1.7168992994892323\n",
            "batch 901 loss: 1.687612521259871\n",
            "batch 1001 loss: 1.682243229300948\n",
            "batch 1101 loss: 1.5203325156895153\n",
            "batch 1201 loss: 1.9290725136874243\n",
            "batch 1301 loss: 1.7519520932397064\n",
            "batch 1401 loss: 1.4686819911398925\n",
            "batch 1501 loss: 1.7712389119225553\n",
            "batch 1601 loss: 1.9445586024344856\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 2.215713 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9723960684097619\n",
            "batch 101 loss: 1.7442992584589228\n",
            "batch 201 loss: 1.6281775697757621\n",
            "batch 301 loss: 1.8238833611021437\n",
            "batch 401 loss: 1.9322586297767703\n",
            "batch 501 loss: 1.6954705082275905\n",
            "batch 601 loss: 1.717410359782516\n",
            "batch 701 loss: 1.45101882079005\n",
            "batch 801 loss: 1.7895263242628425\n",
            "batch 901 loss: 1.8293459637231717\n",
            "batch 1001 loss: 1.5782615879530066\n",
            "batch 1101 loss: 1.7145995532299276\n",
            "batch 1201 loss: 1.4542696612980217\n",
            "batch 1301 loss: 1.9831335051318455\n",
            "batch 1401 loss: 1.785280406351667\n",
            "batch 1501 loss: 1.6645157454389845\n",
            "batch 1601 loss: 1.8088509992486796\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.636733 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "batch 1 loss: 1.3326155286398715\n",
            "batch 101 loss: 1.9212360705787115\n",
            "batch 201 loss: 1.7973643148969858\n",
            "batch 301 loss: 1.7051053033256904\n",
            "batch 401 loss: 1.7516352073125017\n",
            "batch 501 loss: 1.5337116825453267\n",
            "batch 601 loss: 1.765662322207354\n",
            "batch 701 loss: 1.7368726117053301\n",
            "batch 801 loss: 1.8543042654098827\n",
            "batch 901 loss: 1.714665431688327\n",
            "batch 1001 loss: 1.849150666058995\n",
            "batch 1101 loss: 1.8228694516653194\n",
            "batch 1201 loss: 1.4382277784777215\n",
            "batch 1301 loss: 1.7019556854007534\n",
            "batch 1401 loss: 1.7338019335583612\n",
            "batch 1501 loss: 1.715931553855262\n",
            "batch 1601 loss: 1.7929827747796663\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.871708 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "batch 1 loss: 1.028587070995709\n",
            "batch 101 loss: 1.892957533232402\n",
            "batch 201 loss: 1.7216021848769743\n",
            "batch 301 loss: 1.799355056631244\n",
            "batch 401 loss: 1.6012131806369871\n",
            "batch 501 loss: 2.0723574804038276\n",
            "batch 601 loss: 1.4883073531213813\n",
            "batch 701 loss: 1.9068048422472201\n",
            "batch 801 loss: 1.5932266045245342\n",
            "batch 901 loss: 1.8926757093460764\n",
            "batch 1001 loss: 1.6466807077050907\n",
            "batch 1101 loss: 1.4961407828470692\n",
            "batch 1201 loss: 1.7326587172519066\n",
            "batch 1301 loss: 1.793851917698048\n",
            "batch 1401 loss: 1.7241262942773756\n",
            "batch 1501 loss: 1.5283100647958054\n",
            "batch 1601 loss: 1.7220367124143012\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 2.006779 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0950195096276092\n",
            "batch 101 loss: 1.5849850109920953\n",
            "batch 201 loss: 1.6884819202776362\n",
            "batch 301 loss: 1.6616760645761337\n",
            "batch 401 loss: 1.7922138320702652\n",
            "batch 501 loss: 1.8445562275178964\n",
            "batch 601 loss: 1.926731964426872\n",
            "batch 701 loss: 1.9207340897718677\n",
            "batch 801 loss: 1.692894957277458\n",
            "batch 901 loss: 1.6182569238507676\n",
            "batch 1001 loss: 1.7841732838621829\n",
            "batch 1101 loss: 1.886435476198094\n",
            "batch 1201 loss: 1.6904662203794578\n",
            "batch 1301 loss: 1.7184679481852663\n",
            "batch 1401 loss: 1.711322734404821\n",
            "batch 1501 loss: 1.5438516063465795\n",
            "batch 1601 loss: 1.7239521284172952\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.691036 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9512953697249759\n",
            "batch 101 loss: 1.6250535807770212\n",
            "batch 201 loss: 1.4782181633333675\n",
            "batch 301 loss: 1.8998490700335242\n",
            "batch 401 loss: 1.7174776086909695\n",
            "batch 501 loss: 1.5613918654492591\n",
            "batch 601 loss: 1.8448563096759607\n",
            "batch 701 loss: 1.8140617531471457\n",
            "batch 801 loss: 1.7011896958289434\n",
            "batch 901 loss: 1.6099139268869476\n",
            "batch 1001 loss: 1.5950254060210136\n",
            "batch 1101 loss: 2.0109063920614245\n",
            "batch 1201 loss: 1.693609707689029\n",
            "batch 1301 loss: 1.4493400861299597\n",
            "batch 1401 loss: 1.7740704919741257\n",
            "batch 1501 loss: 1.7506421985789802\n",
            "batch 1601 loss: 1.8624485686188563\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.913647 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1770433279708958\n",
            "batch 101 loss: 1.76645034759189\n",
            "batch 201 loss: 1.6155284822010436\n",
            "batch 301 loss: 1.8486642722622673\n",
            "batch 401 loss: 1.4425224990816787\n",
            "batch 501 loss: 1.8855426479084416\n",
            "batch 601 loss: 1.5515525175532092\n",
            "batch 701 loss: 1.6754147120716516\n",
            "batch 801 loss: 1.9482695601781597\n",
            "batch 901 loss: 1.641893131825782\n",
            "batch 1001 loss: 1.8459764407831245\n",
            "batch 1101 loss: 1.5940123382639286\n",
            "batch 1201 loss: 1.8544883087265043\n",
            "batch 1301 loss: 1.7336852857569465\n",
            "batch 1401 loss: 1.8941183466620715\n",
            "batch 1501 loss: 1.7918938437226826\n",
            "batch 1601 loss: 1.7295372707140633\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.871699 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8861346079222858\n",
            "batch 101 loss: 1.914303016171325\n",
            "batch 201 loss: 1.797370699064486\n",
            "batch 301 loss: 1.6878650645766173\n",
            "batch 401 loss: 1.8007187482144218\n",
            "batch 501 loss: 1.6186533294114633\n",
            "batch 601 loss: 1.6130623325627311\n",
            "batch 701 loss: 1.64904311765451\n",
            "batch 801 loss: 1.7259969056420958\n",
            "batch 901 loss: 1.7567626039488824\n",
            "batch 1001 loss: 1.734465543178012\n",
            "batch 1101 loss: 1.604871176937595\n",
            "batch 1201 loss: 1.583237390320064\n",
            "batch 1301 loss: 1.78763756036642\n",
            "batch 1401 loss: 1.6866608350654133\n",
            "batch 1501 loss: 1.77626290062326\n",
            "batch 1601 loss: 1.912830798617506\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.631237 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9598116700240644\n",
            "batch 101 loss: 1.7704981523018797\n",
            "batch 201 loss: 1.8845943327049464\n",
            "batch 301 loss: 1.4538777152820694\n",
            "batch 401 loss: 1.7517304505361244\n",
            "batch 501 loss: 1.8528712310665287\n",
            "batch 601 loss: 1.5901678156614616\n",
            "batch 701 loss: 1.8465352115250244\n",
            "batch 801 loss: 1.5487522363069002\n",
            "batch 901 loss: 1.7930921015926287\n",
            "batch 1001 loss: 1.539876974914223\n",
            "batch 1101 loss: 1.8714788355016208\n",
            "batch 1201 loss: 1.6943912031524815\n",
            "batch 1301 loss: 1.710013962118137\n",
            "batch 1401 loss: 1.304122949971643\n",
            "batch 1501 loss: 2.2979704053886234\n",
            "batch 1601 loss: 1.976973753975786\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.646678 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0951781453867442\n",
            "batch 101 loss: 1.7457557448425631\n",
            "batch 201 loss: 2.0511462557458433\n",
            "batch 301 loss: 1.7532169979419086\n",
            "batch 401 loss: 1.7997789115109482\n",
            "batch 501 loss: 2.0349170900788156\n",
            "batch 601 loss: 2.0476992598432115\n",
            "batch 701 loss: 1.6927477294460913\n",
            "batch 801 loss: 1.7522806436649534\n",
            "batch 901 loss: 1.6551602704786728\n",
            "batch 1001 loss: 1.8165271969990868\n",
            "batch 1101 loss: 1.6134713229503905\n",
            "batch 1201 loss: 1.5691036818377324\n",
            "batch 1301 loss: 1.5740313022083137\n",
            "batch 1401 loss: 1.3238707580472693\n",
            "batch 1501 loss: 1.9025875058010933\n",
            "batch 1601 loss: 1.7907901317199866\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.620487 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9450529603688324\n",
            "batch 101 loss: 1.5435926516639302\n",
            "batch 201 loss: 1.612902783213358\n",
            "batch 301 loss: 1.7077699794387444\n",
            "batch 401 loss: 2.0487484428519385\n",
            "batch 501 loss: 2.0069515194745327\n",
            "batch 601 loss: 1.5178948608100473\n",
            "batch 701 loss: 1.6199181793286699\n",
            "batch 801 loss: 1.7323342191563278\n",
            "batch 901 loss: 1.5880626940744969\n",
            "batch 1001 loss: 1.4916696905484423\n",
            "batch 1101 loss: 1.795871101218081\n",
            "batch 1201 loss: 1.8322720162133919\n",
            "batch 1301 loss: 1.8249494985543424\n",
            "batch 1401 loss: 1.7371437200484798\n",
            "batch 1501 loss: 1.6554456988159973\n",
            "batch 1601 loss: 1.8739133691295864\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.642611 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0714177985829156\n",
            "batch 101 loss: 1.7950902120023966\n",
            "batch 201 loss: 1.9880432647443376\n",
            "batch 301 loss: 1.4994882060340025\n",
            "batch 401 loss: 1.750784691512963\n",
            "batch 501 loss: 1.7676775805768556\n",
            "batch 601 loss: 1.4866543031347101\n",
            "batch 701 loss: 1.823932018449359\n",
            "batch 801 loss: 1.7251120834471658\n",
            "batch 901 loss: 1.6634342417315702\n",
            "batch 1001 loss: 1.7891008000397415\n",
            "batch 1101 loss: 1.7600953616629704\n",
            "batch 1201 loss: 1.6277527536451817\n",
            "batch 1301 loss: 1.8100451087950205\n",
            "batch 1401 loss: 1.6357841646671296\n",
            "batch 1501 loss: 1.8887092428882897\n",
            "batch 1601 loss: 1.6753810186428018\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.630722 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9066156353606494\n",
            "batch 101 loss: 1.6319370350443205\n",
            "batch 201 loss: 1.4687076413819113\n",
            "batch 301 loss: 1.9605130135640503\n",
            "batch 401 loss: 1.9256629567418713\n",
            "batch 501 loss: 1.6162266693360288\n",
            "batch 601 loss: 1.8742961324332281\n",
            "batch 701 loss: 1.7212025329979224\n",
            "batch 801 loss: 1.7377372879261384\n",
            "batch 901 loss: 1.7160002068959874\n",
            "batch 1001 loss: 1.5715790011844364\n",
            "batch 1101 loss: 1.7403903012024238\n",
            "batch 1201 loss: 1.6739175678818718\n",
            "batch 1301 loss: 1.6254131671585492\n",
            "batch 1401 loss: 1.592859662916453\n",
            "batch 1501 loss: 1.7567132267786656\n",
            "batch 1601 loss: 2.026561470638937\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.624339 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "batch 1 loss: 1.3644946917216294\n",
            "batch 101 loss: 1.8693935644836166\n",
            "batch 201 loss: 1.7843505228875438\n",
            "batch 301 loss: 1.721457413406315\n",
            "batch 401 loss: 1.3993948749268748\n",
            "batch 501 loss: 1.811603588925209\n",
            "batch 601 loss: 1.6801199099091269\n",
            "batch 701 loss: 1.5928119513049024\n",
            "batch 801 loss: 1.5154542232854873\n",
            "batch 901 loss: 1.8029883168013203\n",
            "batch 1001 loss: 1.9515863446018193\n",
            "batch 1101 loss: 1.7854984202935884\n",
            "batch 1201 loss: 1.8916517004308708\n",
            "batch 1301 loss: 1.6945756144967528\n",
            "batch 1401 loss: 1.7135274824954103\n",
            "batch 1501 loss: 1.526313764164588\n",
            "batch 1601 loss: 1.7016539990250021\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.703336 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9143385452894655\n",
            "batch 101 loss: 1.7019151476805563\n",
            "batch 201 loss: 1.8754451022847207\n",
            "batch 301 loss: 1.5518716224480886\n",
            "batch 401 loss: 1.5940638542821397\n",
            "batch 501 loss: 1.6666695001375047\n",
            "batch 601 loss: 1.783945869624149\n",
            "batch 701 loss: 1.6399085973976981\n",
            "batch 801 loss: 1.5949475507748867\n",
            "batch 901 loss: 1.8892081514175516\n",
            "batch 1001 loss: 1.8853816148103215\n",
            "batch 1101 loss: 1.6592987598817126\n",
            "batch 1201 loss: 1.8159596304266825\n",
            "batch 1301 loss: 1.857203147604305\n",
            "batch 1401 loss: 1.7437946564191953\n",
            "batch 1501 loss: 1.4392880476440768\n",
            "batch 1601 loss: 1.6250994789878315\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.642450 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0424257321329788\n",
            "batch 101 loss: 1.5488406134117394\n",
            "batch 201 loss: 1.6028976632468401\n",
            "batch 301 loss: 2.1235848227349923\n",
            "batch 401 loss: 1.805981095241441\n",
            "batch 501 loss: 1.9088802998809842\n",
            "batch 601 loss: 1.5985836078103604\n",
            "batch 701 loss: 1.6057033704809465\n",
            "batch 801 loss: 1.7438082554657013\n",
            "batch 901 loss: 1.8038382775895299\n",
            "batch 1001 loss: 1.696722352937213\n",
            "batch 1101 loss: 1.7249709810841887\n",
            "batch 1201 loss: 1.9191520630399463\n",
            "batch 1301 loss: 1.6070264154855454\n",
            "batch 1401 loss: 1.5157515538838924\n",
            "batch 1501 loss: 1.8338862882787361\n",
            "batch 1601 loss: 1.6862350864509061\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.692493 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0987116163782775\n",
            "batch 101 loss: 1.976496602706611\n",
            "batch 201 loss: 1.6485447525000199\n",
            "batch 301 loss: 1.7599534325202695\n",
            "batch 401 loss: 1.652339172672946\n",
            "batch 501 loss: 1.8366919308691285\n",
            "batch 601 loss: 1.8752160966185785\n",
            "batch 701 loss: 1.7130885376668448\n",
            "batch 801 loss: 1.756158224225801\n",
            "batch 901 loss: 1.625060759677517\n",
            "batch 1001 loss: 1.7149605447264913\n",
            "batch 1101 loss: 1.6895569717516628\n",
            "batch 1201 loss: 1.7599900558582886\n",
            "batch 1301 loss: 1.6006134307058528\n",
            "batch 1401 loss: 1.7950921822358055\n",
            "batch 1501 loss: 1.6143361371330684\n",
            "batch 1601 loss: 1.849360552157741\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.912554 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "batch 1 loss: 1.070808068048209\n",
            "batch 101 loss: 1.742568369700457\n",
            "batch 201 loss: 2.0515831651352348\n",
            "batch 301 loss: 1.7068084693013226\n",
            "batch 401 loss: 1.9615488221895794\n",
            "batch 501 loss: 1.7763337778107962\n",
            "batch 601 loss: 1.9224526632440393\n",
            "batch 701 loss: 1.5731468904897157\n",
            "batch 801 loss: 2.0531688425590984\n",
            "batch 901 loss: 1.7121534597920254\n",
            "batch 1001 loss: 1.6528131046514825\n",
            "batch 1101 loss: 1.373593864866998\n",
            "batch 1201 loss: 1.6578600401082804\n",
            "batch 1301 loss: 1.5629300089282332\n",
            "batch 1401 loss: 1.5394253976434993\n",
            "batch 1501 loss: 1.4645690602340984\n",
            "batch 1601 loss: 1.7442885688715615\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.787688 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9313637103667134\n",
            "batch 101 loss: 1.9945440986193717\n",
            "batch 201 loss: 1.365183052649554\n",
            "batch 301 loss: 1.8003912652237342\n",
            "batch 401 loss: 2.0502195255213884\n",
            "batch 501 loss: 1.692600070194967\n",
            "batch 601 loss: 2.0379132396448405\n",
            "batch 701 loss: 1.580620780249592\n",
            "batch 801 loss: 1.832824249698315\n",
            "batch 901 loss: 1.5900973964719742\n",
            "batch 1001 loss: 1.7654150918626692\n",
            "batch 1101 loss: 1.6338851134664583\n",
            "batch 1201 loss: 1.5434673661598937\n",
            "batch 1301 loss: 1.9020176349382383\n",
            "batch 1401 loss: 1.6505764220176935\n",
            "batch 1501 loss: 1.834640036240744\n",
            "batch 1601 loss: 1.8523231758805923\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.615186 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0786109792860226\n",
            "batch 101 loss: 1.730990426641656\n",
            "batch 201 loss: 1.752866868657584\n",
            "batch 301 loss: 1.855053243585571\n",
            "batch 401 loss: 1.676115685054101\n",
            "batch 501 loss: 1.7443878745718393\n",
            "batch 601 loss: 1.5409943572414704\n",
            "batch 701 loss: 1.5143775212415493\n",
            "batch 801 loss: 1.6493514803209108\n",
            "batch 901 loss: 1.7928186280751834\n",
            "batch 1001 loss: 1.917426005652669\n",
            "batch 1101 loss: 1.8165698233945295\n",
            "batch 1201 loss: 1.7310068096540636\n",
            "batch 1301 loss: 1.7957620000839234\n",
            "batch 1401 loss: 1.6862777552113402\n",
            "batch 1501 loss: 1.867622924776988\n",
            "batch 1601 loss: 1.653197099779063\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.666770 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8920806002034806\n",
            "batch 101 loss: 1.6955853738086444\n",
            "batch 201 loss: 1.8200894619431347\n",
            "batch 301 loss: 1.7960004703986487\n",
            "batch 401 loss: 1.9670230601541698\n",
            "batch 501 loss: 1.7957617069533809\n",
            "batch 601 loss: 1.7619624615141947\n",
            "batch 701 loss: 1.9249250616435893\n",
            "batch 801 loss: 1.660528355852366\n",
            "batch 901 loss: 1.4912025474623078\n",
            "batch 1001 loss: 1.5719363735580214\n",
            "batch 1101 loss: 1.62639240410208\n",
            "batch 1201 loss: 1.574115199559601\n",
            "batch 1301 loss: 1.7442318048793823\n",
            "batch 1401 loss: 1.7833898619417232\n",
            "batch 1501 loss: 1.8264731506025418\n",
            "batch 1601 loss: 1.8396541368705221\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.625835 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0448889858357142\n",
            "batch 101 loss: 1.815959040466696\n",
            "batch 201 loss: 2.029365623169142\n",
            "batch 301 loss: 1.4719456155761146\n",
            "batch 401 loss: 1.8198958569216483\n",
            "batch 501 loss: 1.89210197381326\n",
            "batch 601 loss: 1.5936686970421579\n",
            "batch 701 loss: 1.6014139436641381\n",
            "batch 801 loss: 1.7264561836476786\n",
            "batch 901 loss: 1.8672255804455926\n",
            "batch 1001 loss: 1.7269241276782124\n",
            "batch 1101 loss: 1.6528097470726062\n",
            "batch 1201 loss: 1.8343908075141735\n",
            "batch 1301 loss: 1.5977285703536472\n",
            "batch 1401 loss: 1.479230633172574\n",
            "batch 1501 loss: 1.473049780480942\n",
            "batch 1601 loss: 1.7501110518840142\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.678879 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0322736419521061\n",
            "batch 101 loss: 1.668131482792087\n",
            "batch 201 loss: 1.7865139947473654\n",
            "batch 301 loss: 1.7386833743300985\n",
            "batch 401 loss: 1.6104342324905883\n",
            "batch 501 loss: 1.5881467659039685\n",
            "batch 601 loss: 1.7612583674371125\n",
            "batch 701 loss: 1.6856712376460201\n",
            "batch 801 loss: 1.5976824240339607\n",
            "batch 901 loss: 1.7716572605422698\n",
            "batch 1001 loss: 1.7617951633589108\n",
            "batch 1101 loss: 1.7399089259048923\n",
            "batch 1201 loss: 1.8596406892618689\n",
            "batch 1301 loss: 1.7919147185242037\n",
            "batch 1401 loss: 1.5526204664009855\n",
            "batch 1501 loss: 1.5569463535794785\n",
            "batch 1601 loss: 1.9591798623709473\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.870852 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "batch 1 loss: 1.2729920232249423\n",
            "batch 101 loss: 1.9072208844449778\n",
            "batch 201 loss: 1.3146247735700398\n",
            "batch 301 loss: 1.8510684715062649\n",
            "batch 401 loss: 1.8344121619926954\n",
            "batch 501 loss: 1.5676579181361012\n",
            "batch 601 loss: 1.5346186548518017\n",
            "batch 701 loss: 1.8816442432490295\n",
            "batch 801 loss: 1.7658606415428222\n",
            "batch 901 loss: 1.6285754828973587\n",
            "batch 1001 loss: 1.7515084430818388\n",
            "batch 1101 loss: 1.826455802375567\n",
            "batch 1201 loss: 1.5992307480145973\n",
            "batch 1301 loss: 1.6735664370644372\n",
            "batch 1401 loss: 1.8390248876222177\n",
            "batch 1501 loss: 1.592512424835004\n",
            "batch 1601 loss: 1.6857708321977407\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.614993 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "batch 1 loss: 1.3032763632386923\n",
            "batch 101 loss: 1.6697742313041817\n",
            "batch 201 loss: 1.744955964241526\n",
            "batch 301 loss: 1.557122247323059\n",
            "batch 401 loss: 1.7088644184198347\n",
            "batch 501 loss: 1.9741366537078284\n",
            "batch 601 loss: 1.688497464418324\n",
            "batch 701 loss: 1.9564008591903257\n",
            "batch 801 loss: 2.2859611473488624\n",
            "batch 901 loss: 1.6106383938807993\n",
            "batch 1001 loss: 1.8643048270169675\n",
            "batch 1101 loss: 1.5880879823071883\n",
            "batch 1201 loss: 1.7984367500490044\n",
            "batch 1301 loss: 1.5640274641269933\n",
            "batch 1401 loss: 1.6011108150007203\n",
            "batch 1501 loss: 1.783005913211964\n",
            "batch 1601 loss: 1.7443244953500108\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.780468 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0587443644096493\n",
            "batch 101 loss: 1.907508487822488\n",
            "batch 201 loss: 1.5714928777202295\n",
            "batch 301 loss: 1.5409180542279501\n",
            "batch 401 loss: 1.8677706696589302\n",
            "batch 501 loss: 1.6977293719290447\n",
            "batch 601 loss: 1.964724587353412\n",
            "batch 701 loss: 1.7024257872367161\n",
            "batch 801 loss: 1.8211766668269411\n",
            "batch 901 loss: 1.4409179519566266\n",
            "batch 1001 loss: 1.698697818450164\n",
            "batch 1101 loss: 1.6251176480384821\n",
            "batch 1201 loss: 1.5108963257889263\n",
            "batch 1301 loss: 1.6021630542568164\n",
            "batch 1401 loss: 1.8099544549119309\n",
            "batch 1501 loss: 1.94278369041067\n",
            "batch 1601 loss: 1.9901051802001894\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.667490 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "batch 1 loss: 1.052422256709542\n",
            "batch 101 loss: 1.867879638156155\n",
            "batch 201 loss: 1.8514420269511174\n",
            "batch 301 loss: 1.7861396518867696\n",
            "batch 401 loss: 1.565069881376694\n",
            "batch 501 loss: 1.651314481612353\n",
            "batch 601 loss: 2.0376131451557105\n",
            "batch 701 loss: 1.8659193123475597\n",
            "batch 801 loss: 1.4978044021409733\n",
            "batch 901 loss: 1.8810190744261308\n",
            "batch 1001 loss: 1.5814458971994463\n",
            "batch 1101 loss: 1.6685708098392933\n",
            "batch 1201 loss: 1.560895584896207\n",
            "batch 1301 loss: 1.97776310356483\n",
            "batch 1401 loss: 1.9240373449905745\n",
            "batch 1501 loss: 1.5209991469226951\n",
            "batch 1601 loss: 1.678290365629946\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.629168 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "batch 1 loss: 1.083338444456458\n",
            "batch 101 loss: 1.8406524750898825\n",
            "batch 201 loss: 1.7454309652373194\n",
            "batch 301 loss: 1.5984120687817631\n",
            "batch 401 loss: 1.6218747902712494\n",
            "batch 501 loss: 1.6507206103636418\n",
            "batch 601 loss: 1.7690701425727458\n",
            "batch 701 loss: 1.603248104113154\n",
            "batch 801 loss: 1.8099276777287014\n",
            "batch 901 loss: 1.6643578529974912\n",
            "batch 1001 loss: 1.624103238946991\n",
            "batch 1101 loss: 1.6482116627856158\n",
            "batch 1201 loss: 2.127638123188226\n",
            "batch 1301 loss: 1.582622926309541\n",
            "batch 1401 loss: 1.729234084583586\n",
            "batch 1501 loss: 1.6501726110070012\n",
            "batch 1601 loss: 1.7576730380090886\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.816992 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1232385370554403\n",
            "batch 101 loss: 1.5424808435793966\n",
            "batch 201 loss: 1.4939935259586583\n",
            "batch 301 loss: 1.632540426935011\n",
            "batch 401 loss: 1.790120835628477\n",
            "batch 501 loss: 1.4695976344146766\n",
            "batch 601 loss: 1.8062805655348348\n",
            "batch 701 loss: 1.6061275389096772\n",
            "batch 801 loss: 1.9241011235839687\n",
            "batch 901 loss: 1.7052486610944106\n",
            "batch 1001 loss: 1.8329699846515723\n",
            "batch 1101 loss: 1.7985863973957021\n",
            "batch 1201 loss: 1.673859454880876\n",
            "batch 1301 loss: 2.0718913814844564\n",
            "batch 1401 loss: 1.6901837230889942\n",
            "batch 1501 loss: 1.7412270223116502\n",
            "batch 1601 loss: 1.7971235536877066\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.618848 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "batch 1 loss: 1.101872550045373\n",
            "batch 101 loss: 1.8732235092660994\n",
            "batch 201 loss: 1.562954257861711\n",
            "batch 301 loss: 1.6130739709681075\n",
            "batch 401 loss: 1.706243934539525\n",
            "batch 501 loss: 1.9416536696068942\n",
            "batch 601 loss: 1.877888927119784\n",
            "batch 701 loss: 1.6014656618449317\n",
            "batch 801 loss: 1.8799390385821608\n",
            "batch 901 loss: 1.9287405631240109\n",
            "batch 1001 loss: 1.4620144741531111\n",
            "batch 1101 loss: 1.754162106590302\n",
            "batch 1201 loss: 1.4237748274338902\n",
            "batch 1301 loss: 2.0112957267498133\n",
            "batch 1401 loss: 1.8153854455336114\n",
            "batch 1501 loss: 1.7508446098952481\n",
            "batch 1601 loss: 1.460082734809839\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.906257 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9540299148461782\n",
            "batch 101 loss: 1.8458424658857984\n",
            "batch 201 loss: 1.9473015972551366\n",
            "batch 301 loss: 1.803556732595898\n",
            "batch 401 loss: 1.660908181690611\n",
            "batch 501 loss: 1.6133993226737038\n",
            "batch 601 loss: 1.5874674990633502\n",
            "batch 701 loss: 1.4061463270147214\n",
            "batch 801 loss: 1.9290294975892994\n",
            "batch 901 loss: 1.9671823617309565\n",
            "batch 1001 loss: 1.6052283369335054\n",
            "batch 1101 loss: 1.5816351838037372\n",
            "batch 1201 loss: 1.7481407069630222\n",
            "batch 1301 loss: 1.7576940015581204\n",
            "batch 1401 loss: 1.7773225974457454\n",
            "batch 1501 loss: 1.6581211007533057\n",
            "batch 1601 loss: 1.6988348689148915\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.721866 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0132849993900164\n",
            "batch 101 loss: 2.015082287917321\n",
            "batch 201 loss: 2.086576324819471\n",
            "batch 301 loss: 1.7633642506133969\n",
            "batch 401 loss: 1.787620379556829\n",
            "batch 501 loss: 1.5403172595379875\n",
            "batch 601 loss: 1.6276959580650145\n",
            "batch 701 loss: 1.6277783333789557\n",
            "batch 801 loss: 1.6037010874461703\n",
            "batch 901 loss: 1.536006672673393\n",
            "batch 1001 loss: 1.6046626214662683\n",
            "batch 1101 loss: 1.8952367362807854\n",
            "batch 1201 loss: 1.6144670125719858\n",
            "batch 1301 loss: 1.8206077661422022\n",
            "batch 1401 loss: 1.609828680587234\n",
            "batch 1501 loss: 1.771445988586056\n",
            "batch 1601 loss: 1.7269763348734706\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.639311 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "batch 1 loss: 0.8464591407988337\n",
            "batch 101 loss: 1.6837362785195364\n",
            "batch 201 loss: 1.538317720018258\n",
            "batch 301 loss: 1.8025491242692806\n",
            "batch 401 loss: 1.5955418593588728\n",
            "batch 501 loss: 1.6479056288639549\n",
            "batch 601 loss: 1.7024342958972556\n",
            "batch 701 loss: 1.8130902808048268\n",
            "batch 801 loss: 1.8486557015005383\n",
            "batch 901 loss: 1.976409076853888\n",
            "batch 1001 loss: 1.8634377327003313\n",
            "batch 1101 loss: 1.7340180366579443\n",
            "batch 1201 loss: 1.8639903190359473\n",
            "batch 1301 loss: 1.6189192151417955\n",
            "batch 1401 loss: 1.6854977864472311\n",
            "batch 1501 loss: 1.527232089573713\n",
            "batch 1601 loss: 1.7404836911847814\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.631113 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1718317021161784\n",
            "batch 101 loss: 1.8073533697977837\n",
            "batch 201 loss: 1.6722879847028524\n",
            "batch 301 loss: 1.9444818140217104\n",
            "batch 401 loss: 1.6507621827306866\n",
            "batch 501 loss: 1.7750439501365327\n",
            "batch 601 loss: 1.8512257619109005\n",
            "batch 701 loss: 1.4961120649799704\n",
            "batch 801 loss: 1.710228269855345\n",
            "batch 901 loss: 1.5252082522772252\n",
            "batch 1001 loss: 1.7222794486710336\n",
            "batch 1101 loss: 1.7902424230682663\n",
            "batch 1201 loss: 1.504899171921461\n",
            "batch 1301 loss: 1.8854314441385214\n",
            "batch 1401 loss: 1.817880520101171\n",
            "batch 1501 loss: 1.8887603456143642\n",
            "batch 1601 loss: 1.7036515104351566\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.703200 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "batch 1 loss: 0.946391487660876\n",
            "batch 101 loss: 1.3750110843451693\n",
            "batch 201 loss: 1.5420528614064097\n",
            "batch 301 loss: 1.8392754636308382\n",
            "batch 401 loss: 1.775562408825499\n",
            "batch 501 loss: 1.6886744881266735\n",
            "batch 601 loss: 1.627247828175864\n",
            "batch 701 loss: 1.8204773328076407\n",
            "batch 801 loss: 1.7231985946337227\n",
            "batch 901 loss: 1.7071444140675884\n",
            "batch 1001 loss: 1.8689737471775152\n",
            "batch 1101 loss: 1.9706471609400251\n",
            "batch 1201 loss: 1.8148215122683906\n",
            "batch 1301 loss: 1.514672840587191\n",
            "batch 1401 loss: 2.1797811878620994\n",
            "batch 1501 loss: 1.601216229286656\n",
            "batch 1601 loss: 1.5636181287688669\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.618777 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1688948340278058\n",
            "batch 101 loss: 1.687273716552782\n",
            "batch 201 loss: 1.5695566887064616\n",
            "batch 301 loss: 1.746675942413276\n",
            "batch 401 loss: 1.5281300066013506\n",
            "batch 501 loss: 1.69882869863417\n",
            "batch 601 loss: 1.8893996511097066\n",
            "batch 701 loss: 1.6722795614530332\n",
            "batch 801 loss: 1.415626855262235\n",
            "batch 901 loss: 1.7240463359018394\n",
            "batch 1001 loss: 1.5960065825498895\n",
            "batch 1101 loss: 1.8275005481060453\n",
            "batch 1201 loss: 1.5245978954841848\n",
            "batch 1301 loss: 1.9066219543191254\n",
            "batch 1401 loss: 1.9485785040282644\n",
            "batch 1501 loss: 1.601839503395313\n",
            "batch 1601 loss: 1.7339321158966596\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.614534 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "batch 1 loss: 0.9833231174340472\n",
            "batch 101 loss: 2.013280157405625\n",
            "batch 201 loss: 1.4376977973314933\n",
            "batch 301 loss: 1.8219801255743369\n",
            "batch 401 loss: 1.7750457320816166\n",
            "batch 501 loss: 1.682746839231986\n",
            "batch 601 loss: 1.7415297781396657\n",
            "batch 701 loss: 1.9114503927501938\n",
            "batch 801 loss: 1.735772423716262\n",
            "batch 901 loss: 1.5042926806603736\n",
            "batch 1001 loss: 1.7817831243717228\n",
            "batch 1101 loss: 1.7362136586533596\n",
            "batch 1201 loss: 1.6979825014262315\n",
            "batch 1301 loss: 1.6376623665806846\n",
            "batch 1401 loss: 1.9138112044963054\n",
            "batch 1501 loss: 2.0222558599965144\n",
            "batch 1601 loss: 1.7951266416773433\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.686309 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "batch 1 loss: 0.755450002681464\n",
            "batch 101 loss: 1.8597139684692956\n",
            "batch 201 loss: 1.5409579481417313\n",
            "batch 301 loss: 1.7403850899043027\n",
            "batch 401 loss: 1.673600399173738\n",
            "batch 501 loss: 1.5648255691002122\n",
            "batch 601 loss: 1.6048716830392369\n",
            "batch 701 loss: 1.7743616185235442\n",
            "batch 801 loss: 1.6298982462508138\n",
            "batch 901 loss: 1.872833605923879\n",
            "batch 1001 loss: 1.9334889956235566\n",
            "batch 1101 loss: 1.78911111278343\n",
            "batch 1201 loss: 1.735944010528765\n",
            "batch 1301 loss: 1.6256308492181786\n",
            "batch 1401 loss: 1.6176850727957208\n",
            "batch 1501 loss: 1.4944402587336663\n",
            "batch 1601 loss: 1.9707812696998916\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.617080 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "batch 1 loss: 1.0055498115159571\n",
            "batch 101 loss: 1.3867019146601525\n",
            "batch 201 loss: 1.6704983459044025\n",
            "batch 301 loss: 1.618978580688854\n",
            "batch 401 loss: 1.5800786611294462\n",
            "batch 501 loss: 1.7967889619033668\n",
            "batch 601 loss: 1.706373258883832\n",
            "batch 701 loss: 1.7289441929315217\n",
            "batch 801 loss: 1.7094564712641296\n",
            "batch 901 loss: 1.740156825601589\n",
            "batch 1001 loss: 1.940333861745894\n",
            "batch 1101 loss: 1.790029139893013\n",
            "batch 1201 loss: 1.9130692612892017\n",
            "batch 1301 loss: 1.5634258921956643\n",
            "batch 1401 loss: 1.8755543439649045\n",
            "batch 1501 loss: 1.7694301664366503\n",
            "batch 1601 loss: 1.689674600129947\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.723429 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "batch 1 loss: 1.1189962841388479\n",
            "batch 101 loss: 1.6140079204153153\n",
            "batch 201 loss: 1.9715055419648706\n",
            "batch 301 loss: 1.7054022952547712\n",
            "batch 401 loss: 2.103394864369184\n",
            "batch 501 loss: 1.6602759369357956\n",
            "batch 601 loss: 1.5971272402386238\n",
            "batch 701 loss: 1.7576306620877586\n",
            "batch 801 loss: 1.567839565453469\n",
            "batch 901 loss: 1.6581135512377205\n",
            "batch 1001 loss: 1.790118002308218\n",
            "batch 1101 loss: 1.8679747371852864\n",
            "batch 1201 loss: 1.8833694802620449\n",
            "batch 1301 loss: 1.677573414526414\n",
            "batch 1401 loss: 1.709951183393132\n",
            "batch 1501 loss: 1.5526958485421347\n",
            "batch 1601 loss: 1.6155775536363945\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 1.638985 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "batch 1 loss: 1.099282314698794\n",
            "batch 101 loss: 1.8111246545682662\n",
            "batch 201 loss: 1.7413672405575518\n",
            "batch 301 loss: 1.9195510395996098\n",
            "batch 401 loss: 1.7532292021472768\n",
            "batch 501 loss: 1.5935909251477278\n",
            "batch 601 loss: 1.7802769399969838\n",
            "batch 701 loss: 1.5253213850956309\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.000003)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "model.train()\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        # print('X', X.shape)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        # print(\"Prediction\")\n",
        "        pred = pred.squeeze()\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # losses.append(loss.to('cpu').detach().numpy())\n",
        "        # iterations += 1\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # print(\"Loss gradient\", loss.grad)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            last_loss = running_loss / 100  # loss per batch\n",
        "            print('batch {} loss: {}'.format(batch + 1, last_loss))\n",
        "            # tb_x = epoch * len(dataloader) + batch + 1\n",
        "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    model.eval()\n",
        "    size = len(dataloader_val.dataset)\n",
        "    num_batches = len(dataloader_val)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch, (X, y) in enumerate(dataloader_val):\n",
        "          X = X.to(device)\n",
        "          y = y.to(device)\n",
        "          pred = model(X)\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    \n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Yn7xm6aGUvMP",
        "outputId": "9f0a1429-44ea-4384-b593-185a5575fb30"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'losses' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# run\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(losses)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mIterations\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
          ]
        }
      ],
      "source": [
        "# run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "# plt.savefig(\"training.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHImI0fDU8mK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
