{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVAa1tdOE-5-",
        "outputId": "f9d81291-60c2-43c8-dfae-d65c9e1228d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in ./venv/lib/python3.8/site-packages (0.10.1)\n",
            "Requirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (3.7.3)\n",
            "Requirement already satisfied: spafe in ./venv/lib/python3.8/site-packages (0.3.2)\n",
            "Collecting torch\n",
            "  Using cached torch-2.1.0-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n",
            "Requirement already satisfied: pandas in ./venv/lib/python3.8/site-packages (2.0.3)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in ./venv/lib/python3.8/site-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: pooch>=1.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=1.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: soxr>=0.3.2 in ./venv/lib/python3.8/site-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in ./venv/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in ./venv/lib/python3.8/site-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in ./venv/lib/python3.8/site-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in ./venv/lib/python3.8/site-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: joblib>=0.14 in ./venv/lib/python3.8/site-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.3.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in ./venv/lib/python3.8/site-packages (from librosa) (0.58.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./venv/lib/python3.8/site-packages (from librosa) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (10.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 KB\u001b[0m \u001b[31m447.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m310.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m404.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:25\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m415.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0\n",
            "  Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m484.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m665.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m597.2/731.7 MB\u001b[0m \u001b[31m459.2 kB/s\u001b[0m eta \u001b[36m0:04:54\u001b[0m^C\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m597.2/731.7 MB\u001b[0m \u001b[31m460.0 kB/s\u001b[0m eta \u001b[36m0:04:53\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/home/ashutosh/Desktop/ugmqa_project/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install librosa matplotlib spafe torch pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqdzhPSpFJ5z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHXfPJFBGX63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, num_heads):\n",
        "\n",
        "    super(SelfAttention, self).__init__()\n",
        "\n",
        "    self.embed_size = embed_size\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embed_size // num_heads\n",
        "\n",
        "    assert (self.head_dim * num_heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
        "\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
        "\n",
        "\n",
        "  def forward(self, values, keys, query, mask):\n",
        "    N = query.shape[0]\n",
        "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    # Split embedding into self.num_heads pieces\n",
        "    values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "    print(\"values:\", values.shape)\n",
        "    keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "    print(\"keys:\", keys.shape)\n",
        "    queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "    print(\"queries:\", query.shape)\n",
        "\n",
        "\n",
        "    values = self.values(values)\n",
        "    keys = self.keys(keys)\n",
        "    queries = self.queries(queries)\n",
        "    energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) # MatMul Q and K\n",
        "    # queries shape: (N, query_len, heads, heads_dim)\n",
        "    # keys shape: (N, query_len, heads, heads_dim)\n",
        "    # energy shape: (N, heads, query_len, key_len)\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "    attention = torch.softmax(energy / (self.embed_size ** 0.5), dim = 3)\n",
        "\n",
        "    out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "      N, query_len, self.num_heads * self.head_dim\n",
        "    )\n",
        "\n",
        "    out = self.fc_out(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBb-2No1IHlL"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, dropout, max_len = 5000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
        "    self.position_encoding = torch.zeros(max_len, 1, embed_size).to(device)\n",
        "    self.position_encoding[:, 0, 0::2] = torch.sin(position * div_term).to(device)\n",
        "    self.position_encoding[:, 0, 1::2] = torch.cos(position * div_term).to(device)\n",
        "    self.register_buffer('pe', self.position_encoding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.position_encoding[:x.size(0)]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6FIvPEgIJib"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, value, key, query, mask):\n",
        "    attention = self.attention(value, key, query, mask)\n",
        "\n",
        "    x = self.dropout(self.norm1((attention + query)))\n",
        "\n",
        "    forward = self.feed_forward(x)\n",
        "    out = self.dropout(self.norm2(x + forward))\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVmNcXqTILWw"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    src_vocab_size,\n",
        "    embed_size,\n",
        "    num_layers,\n",
        "    heads,\n",
        "    device,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_length\n",
        "  ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device\n",
        "    self.position_embedding = PositionalEncoding(embed_size, dropout, src_vocab_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "      [\n",
        "        TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    print(x.shape)\n",
        "    N, seq_length = x.shape\n",
        "\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "    out = self.dropout(self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, out, out, mask)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-NbKpF1ITI-"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    src_vocab_size,\n",
        "    src_pad_index,\n",
        "    embed_size = 256,\n",
        "    num_layers = 6,\n",
        "    forward_expansion = 4,\n",
        "    heads = 8,\n",
        "    dropout = 0,\n",
        "    device = \"cuda\",\n",
        "    max_length = 500\n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "      src_vocab_size, embed_size, num_layers, heads,\n",
        "      device, forward_expansion, dropout, max_length)\n",
        "\n",
        "    self.src_pad_index = src_pad_index\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_index).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # (N, 1, 1, src_len)\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self, src):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    encoder_output = self.encoder(src, src_mask)\n",
        "\n",
        "    return encoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-1iH8TeIWVc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "\n",
        "# class AudioFeatureDataset2(Dataset):\n",
        "#     def __init__(self, annotations_file, mode = 'train'):\n",
        "#         self.features = pd.read_csv(annotations_file)\n",
        "#         self.features = self.features.drop(['Unnamed: 0'], axis = 1)\n",
        "#         # splitting the test and validation dataset in (80%,20%)\n",
        "#         self.train_subset, self.valid_subset = random_split(self.features, [0.8, 0.2])\n",
        "#         # defining the x_train and y_train(target == \"class\")\n",
        "#         self.x_train = torch.Tensor(self.train_subset.dataset.drop(['class', str(len(self.train_subset.dataset.columns) - 2)], axis = 1).values)\n",
        "#         self.y_train = torch.Tensor(self.train_subset.dataset['class'].values)\n",
        "#         # defining the x_valid and y_valid(target == \"class\")\n",
        "#         self.x_valid = torch.Tensor(self.valid_subset.dataset.drop(['class', str(len(self.train_subset.dataset.columns) - 2)], axis = 1).values)\n",
        "#         self.y_valid = torch.Tensor(self.valid_subset.dataset['class'].values)\n",
        "#         self.mode = mode\n",
        "#         print(\"x_train: \" , self.x_train.shape, \" x_valid:\", self.x_valid.shape  )\n",
        "\n",
        "#     def __len__(self):\n",
        "#         if self.mode == 'train':\n",
        "#             return self.x_train.shape[0]\n",
        "\n",
        "#         return self.x_valid.shape[0]\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         if self.mode == 'train':\n",
        "#             return self.x_train[idx], self.y_train[idx]\n",
        "\n",
        "#         return self.x_valid[idx], self.y_valid[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EVy_wyEJsQb",
        "outputId": "55a617f9-7705-4383-8bf6-c22b8b83035d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/DL_course/features'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/DL_course/features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "soinXdqjMxj5",
        "outputId": "fe5e896e-934e-4498-9401-51310533a317"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a1b02049d9f2>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioFeatureDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./2075_concatenate_dm_4featu.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a1b02049d9f2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotations_file, mode)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAudioFeatureDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './2075_concatenate_dm_4featu.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class AudioFeatureDataset(Dataset):\n",
        "    def __init__(self, annotations_file, mode='train'):\n",
        "        self.data = pd.read_csv(annotations_file)\n",
        "        self.data = self.data.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "        # Splitting the dataset into train and validation sets\n",
        "        total_samples = len(self.data)\n",
        "        train_size = int(0.8 * total_samples)\n",
        "        valid_size = total_samples - train_size\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.data = self.data.iloc[:train_size]\n",
        "        else:\n",
        "            self.data = self.data.iloc[train_size:]\n",
        "\n",
        "        self.features = torch.Tensor(self.data.drop(['0', 'class'], axis=1).values)\n",
        "        self.labels = torch.Tensor(self.data['class'].values)\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Example usage\n",
        "dataset = AudioFeatureDataset('./2075_concatenate_dm_4featu.csv', mode='train')\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    x, y = batch\n",
        "    # Your training code here\n",
        "x_train = dataset[0][0]\n",
        "x_target = dataset[0][1]\n",
        "\n",
        "print(len(dataset))\n",
        "print(len(dataset[0][0]))\n",
        "\n",
        "dataset_val = AudioFeatureDataset('./2075_concatenate_dm_4featu.csv',mode=\"val\")\n",
        "dataloader_val = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "print(len(dataset_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKBauT7rV9dJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL44nAxMPBBV"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "src_vocab_size = 1\n",
        "src_pad_index = 0\n",
        "embed_size = 296\n",
        "num_heads = 4\n",
        "dropout = 0.1\n",
        "output_size = 1\n",
        "batch_size = 64\n",
        "\n",
        "model = Transformer(\n",
        "  src_vocab_size,\n",
        "  src_pad_index,\n",
        "  embed_size = embed_size,\n",
        "  dropout = dropout,\n",
        "  heads = num_heads,\n",
        "  num_layers = num_layers,\n",
        "  ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys2SVUuyQZou"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.000003)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "model.train()\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    # print(X.shape)\n",
        "    # break\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        loss, current = loss.item(), (batch + 1) * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  size = len(dataloader_val.dataset)\n",
        "  num_batches = len(dataloader_val)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X in dataloader_val:\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "print('Done')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
