{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVAa1tdOE-5-",
        "outputId": "11440f54-2b15-430a-f6b3-93ddc01a1917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/home/ashutosh/Desktop/ugmqa_project/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q librosa matplotlib spafe torch pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pknZQa7wt4L3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QqdzhPSpFJ5z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nHXfPJFBGX63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        assert (self.head_dim * num_heads ==\n",
        "                embed_size), \"Embed size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n",
        "\n",
        "        # Split embedding into self.num_heads pieces\n",
        "        value = value.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "        key = key.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        values = self.values(value)\n",
        "        keys = self.keys(key)\n",
        "        queries = self.queries(query)\n",
        "        energy = torch.einsum(\n",
        "            \"nqhd,nkhd->nhqk\", [queries, keys])  # MatMul Q and K\n",
        "        # queries shape: (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, query_len, heads, heads_dim)\n",
        "        # energy shape: (N, heads, query_len, key_len)\n",
        "        # print(\"Mask\", mask.shape)\n",
        "        # print(\"Energy\", energy.shape)\n",
        "\n",
        "        # energy = torch.zeros((N, self.num_heads, query_len, key_len)).to(device)\n",
        "\n",
        "        # mask = torch.zeros((1, 1, 1, key_len)).to(device)\n",
        "\n",
        "        if mask is not None:\n",
        "            # print(mask)\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "            # print(energy[0][0][0])\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
        "        # print(\"output_out:\", out.shape)\n",
        "        # print('Out shape', out.shape)\n",
        "        out = out.reshape(\n",
        "            N, query_len, self.num_heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uBb-2No1IHlL"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, dropout, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2)\n",
        "                             * (-math.log(10000.0) / embed_size))\n",
        "        self.position_encoding = torch.zeros(max_len, embed_size).to(device)\n",
        "        self.position_encoding[:, 0::2] = torch.sin(\n",
        "            position * div_term).to(device)\n",
        "        self.position_encoding[:, 1::2] = torch.cos(\n",
        "            position * div_term).to(device)\n",
        "        self.register_buffer('pe', self.position_encoding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('pe_x', x.shape)\n",
        "        # print('pe', self.position_encoding.shape)\n",
        "        x = x + self.position_encoding[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m6FIvPEgIJib"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        x = self.dropout(self.norm1((attention + query)))\n",
        "\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(x + forward))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mVmNcXqTILWw"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        # self.position_embedding = PositionalEncoding(embed_size, dropout, src_vocab_size)\n",
        "        self.position_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        # print(N, seq_length)\n",
        "\n",
        "        positions = torch.arange(0, seq_length).expand(\n",
        "            N, seq_length).to(self.device)\n",
        "        # print('positions shape', positions.shape)\n",
        "        out = self.dropout(self.position_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4-NbKpF1ITI-"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        src_pad_index,\n",
        "        embed_size=256,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "        device=\"cuda\",\n",
        "        max_length=100\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size, embed_size, num_layers, heads,\n",
        "            device, forward_expansion, dropout, max_length)\n",
        "\n",
        "        self.src_pad_index = src_pad_index\n",
        "        self.output = nn.Linear(src_vocab_size * embed_size, 1)\n",
        "        self.device = device\n",
        "        self.embed_size = embed_size\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_index).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        out = self.encoder(src, src_mask)\n",
        "\n",
        "        out = out.reshape(-1, self.src_vocab_size * self.embed_size)\n",
        "\n",
        "        return self.output(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "soinXdqjMxj5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "\n",
        "class AudioFeatureDataset(Dataset):\n",
        "    def __init__(self, annotations_file, mode='train'):\n",
        "        self.data = pd.read_csv(annotations_file)\n",
        "        self.data = self.data.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "        # Splitting the dataset into train and validation sets\n",
        "        total_samples = len(self.data)\n",
        "        train_size = int(0.8 * total_samples)\n",
        "        valid_size = total_samples - train_size\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.data = self.data.iloc[:train_size]\n",
        "        else:\n",
        "            self.data = self.data.iloc[train_size:]\n",
        "\n",
        "        self.features = torch.Tensor(self.data.drop(['class'], axis=1).values)\n",
        "        self.labels = torch.Tensor(self.data['class'].values)\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        # print(self.features.shape)\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "dataset = AudioFeatureDataset('./working_dataset.csv', mode='train')\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# for batch in dataloader:\n",
        "#     x, y = batch\n",
        "#     # Your training code here\n",
        "# x_train = dataset[0][0]\n",
        "# x_target = dataset[0][1]\n",
        "\n",
        "# print(len(dataset))\n",
        "# print(len(dataset[0][0]))\n",
        "\n",
        "dataset_val = AudioFeatureDataset('./working_dataset.csv', mode=\"val\")\n",
        "dataloader_val = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "# print(len(dataset_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JKBauT7rV9dJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "eL44nAxMPBBV",
        "outputId": "2c6fa457-7d18-468d-d2ff-f3bb5ae04d8c"
      },
      "outputs": [],
      "source": [
        "num_layers = 1\n",
        "src_vocab_size = 297  # TIME-STEPS\n",
        "src_pad_index = 0\n",
        "embed_size = 296  # D-Model\n",
        "num_heads = 1\n",
        "dropout = 0.1\n",
        "output_size = 1\n",
        "forward_expansion = 4\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size,\n",
        "    src_pad_index,\n",
        "    embed_size=embed_size,\n",
        "    dropout=dropout,\n",
        "    heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    forward_expansion=forward_expansion\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 297])\n",
            "tensor(-0.8205, device='cuda:0', grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "X = torch.Tensor([[0, 124.46949, -21.54154, 0, 16.483715, 36.6015, 0, -25.834284, 0, 10.178377, 0, -7.345142, 10.264649, 16.472473, 7.895236, 5.5313177, 6.2877584, -4.182223, -14.012452, -7.7094264, 0.4547759, -5.8861904, -13.388045, -6.9823565, 1.1916866, -1.7707733, -5.276302, -0.35934502, 1.7799568, -4.779073, -6.8897967, 0.79337484, 4.2686896, -2.5059521, -6.496715, -1.0943556, 3.1531215, 0.25546533, -2.272549, -0.73324627, 0.0020391261, 0.012858815, 0.062261246, 0.030906683, 0.033696823, 0.06754137, 0.054984007, 0.36406443, 0.18963988, 0.10330482, 0.12989527, 0.42003468, 0.48301712, 0.11749379, 0.06369407, 0.12233541, 0.2568086, 0.5382474, 1.106481, 1.4184383, 0.71715385, 0.35170192, 0.09534879, 0.0366585, 0.102917545, 0.087631606, 0.03625296, 0.06454905, 0.095770694, 0.12915519, 0.25286278, 0.2482152, 0.20051777, 0.39578456, 0.28170255, 0.3759659, 0.4401238, 0.90044194, 0.8636782, 1.1747689, 0.7714911, 0.818637, 0.37247756, 0.3380456, 0.7661559, 2.4523528, 2.7824976, 8.302962, 16.734638, 8.80889, 12.927655, 13.964713, 7.212821, 7.622612, 17.17792, 10.23075, 8.3349285, 5.6397433, 3.6112895, 0.6386233, 0.24162434, 0.34277353, 0.36261582, 0.3141538, 0.38225174, 0.47966504, 0.03040914, 0.0012386683, 9.3100425e-05, 7.8610115e-05, 7.8027195e-05, 5.490173e-06, 1.6225702e-05, 5.6767494e-06, 4.4215603e-06, 9.099585e-06, 1.2576394e-06, 2.412565e-06, 7.926269e-07, 2.0946077e-06, 1.3263315e-06, 6.3015705e-07, 4.1206027e-07, 7.42592e-07, 5.666581e-07, 2.967201e-07, 1.7622109e-07, 4.3961313e-07, 1.7293216e-07, 1.4194566e-07, 1.8899503e-07, 1.5307475e-07, 9.427612e-08, 1.06292724e-07, 1.01943236e-07, 6.155833e-08, 7.990511e-08, 6.0735445e-08, 4.232562e-08, 6.248923e-08, 3.7052516e-08, 4.0408686e-08, 3.633701e-08, 2.8342097e-08, 3.3516038e-08, 2.3205962e-08, 2.8330943e-08, 2.0632546e-08, 2.3197451e-08, 1.7732157e-08, 1.965794e-08, 1.6046194e-08, 1.7022142e-08, 1.4855194e-08, 1.4529622e-08, 1.42051055e-08, 1.3379163e-08, 1.3088586e-08, 1.2451093e-08, 1.209253e-08, 1.2065797e-08, 1.1807311e-08, 1.1646068e-08, 1.16453025e-08, 1.1936379e-08, 1.2163221e-08, 1.1026455e-08, 7.149512e-09, 0.36461177, 0.34337756, 0.27882302, 0.48984087, 0.58728004, 0.33111906, 0.35057455, 0.40294, 0.37982342, 0.38546938, 0.35827973, 0.3449619, 4098.515885613107,\n",
        "                 4394.679597751644, 2173.6066930992697, 660.1920797101604, 666.3801569895414, 679.7044596914801, 646.1064276646537, 678.6052827762849, 636.4525179151398, 588.8034579300204, 588.6927126149499, 605.9566149564441, 554.6815422905305, 643.55633138673, 635.2709118421336, 593.1425591116601, 689.9515513706316, 753.8378337006548, 713.6116050230352, 665.575526379199, 639.8702521661144, 566.4381100383742, 602.769675543065, 636.9758223614031, 838.8391505866864, 1478.874805344335, 1459.149044006308, 1387.6915345139455, 1353.654060604915, 1380.012526068448, 1383.4946564371012, 1341.579689123074, 1427.5678622884227, 1397.7808753654772, 1344.037783237732, 1257.285128280657, 1297.332690017431, 1316.4388778935704, 1307.883347941082, 1283.9768778672214, 1273.1843530634096, 1278.0824064357296, 1321.431115766408, 1354.4814820287468, 1333.5878041563171, 1357.2018216795293, 1347.5298328353635, 1317.862385602503, 1316.7824410928667, 1363.7911817467896, 1421.45053482153, 1473.7295655749122, 1513.3380612007554, 1527.07320879706, 1520.3705478366887, 1481.8238543145394, 1500.151441619609, 1508.9669344602714, 1481.524474097877, 1445.1450034130887, 1417.4826755078072, 1453.824676109189, 1476.554974221759, 1457.7835067613787, 1442.5007903703054, 1446.138078746579, 1480.3479277869212, 1474.320222966424, 1423.3878323139418, 1360.8577255891676, 1337.3912372750488, 1400.7434254248976, 1406.7530196210892, 1371.0172331395806, 1360.0256467642882, 1360.1123709007084, 1359.7692618475817, 1349.6076963128105, 1345.1738370383607, 1343.4491732619308, 1328.2974945469014, 1314.617505626696, 1321.0909928662404, 1335.4841595848136, 1343.4321393541222, 1344.775088467436, 1333.9444077975136, 1305.7116252799508, 1309.2780904215947, 1308.5759110007473, 1306.0999714339998, 1355.6284484769956, 1452.5165920413374, 1475.6366023948713, 1447.970350106245, 1406.542142884293, 1377.6039021203871, 1278.4930177581896, 1189.0178172699625, 1205.2225452548764, 1179.4867044106006, 881.8538957920834, 1197.5584167943596, 1324.3785722370592, 1361.4066076471468, 1263.928152316742, 1005.8203163981548, 759.0834395666938, 679.2875862507892, 708.2743155303299, 712.1759249973968, 604.9816438504809, 659.412993337054, 789.7184528406086, 815.1398626985261, 915.8414552062778, 773.3202934324213],])\n",
        "# [-397.86472,124.46949,0,-48.849564,16.483715,36.6015,-6.4991527,-25.834284,-1.2522386,10.178377,-5.218914,-7.345142,10.264649,16.472473,7.895236,5.5313177,6.2877584,-4.182223,-14.012452,-7.7094264,0.4547759,-5.8861904,-13.388045,-6.9823565,1.1916866,-1.7707733,-5.276302,-0.35934502,1.7799568,-4.779073,-6.8897967,0.79337484,4.2686896,-2.5059521,-6.496715,-1.0943556,3.1531215,0.25546533,-2.272549,-0.73324627,0.0020391261,0.012858815,0.062261246,0.030906683,0.033696823,0.06754137,0.054984007,0.36406443,0.18963988,0.10330482,0.12989527,0.42003468,0.48301712,0.11749379,0.06369407,0.12233541,0.2568086,0.5382474,1.106481,1.4184383,0.71715385,0.35170192,0.09534879,0.0366585,0.102917545,0.087631606,0.03625296,0.06454905,0.095770694,0.12915519,0.25286278,0.2482152,0.20051777,0.39578456,0.28170255,0.3759659,0.4401238,0.90044194,0.8636782,1.1747689,0.7714911,0.818637,0.37247756,0.3380456,0.7661559,2.4523528,2.7824976,8.302962,16.734638,8.80889,12.927655,13.964713,7.212821,7.622612,17.17792,10.23075,8.3349285,5.6397433,3.6112895,0.6386233,0.24162434,0.34277353,0.36261582,0.3141538,0.38225174,0.47966504,0.03040914,0.0012386683,9.3100425e-05,7.8610115e-05,7.8027195e-05,5.490173e-06,1.6225702e-05,5.6767494e-06,4.4215603e-06,9.099585e-06,1.2576394e-06,2.412565e-06,7.926269e-07,2.0946077e-06,1.3263315e-06,6.3015705e-07,4.1206027e-07,7.42592e-07,5.666581e-07,2.967201e-07,1.7622109e-07,4.3961313e-07,1.7293216e-07,1.4194566e-07,1.8899503e-07,1.5307475e-07,9.427612e-08,1.06292724e-07,1.01943236e-07,6.155833e-08,7.990511e-08,6.0735445e-08,4.232562e-08,6.248923e-08,3.7052516e-08,4.0408686e-08,3.633701e-08,2.8342097e-08,3.3516038e-08,2.3205962e-08,2.8330943e-08,2.0632546e-08,2.3197451e-08,1.7732157e-08,1.965794e-08,1.6046194e-08,1.7022142e-08,1.4855194e-08,1.4529622e-08,1.42051055e-08,1.3379163e-08,1.3088586e-08,1.2451093e-08,1.209253e-08,1.2065797e-08,1.1807311e-08,1.1646068e-08,1.16453025e-08,1.1936379e-08,1.2163221e-08,1.1026455e-08,7.149512e-09,0.36461177,0.34337756,0.27882302,0.48984087,0.58728004,0.33111906,0.35057455,0.40294,0.37982342,0.38546938,0.35827973,0.3449619,4098.515885613107,4394.679597751644,2173.6066930992697,660.1920797101604,666.3801569895414,679.7044596914801,646.1064276646537,678.6052827762849,636.4525179151398,588.8034579300204,588.6927126149499,605.9566149564441,554.6815422905305,643.55633138673,635.2709118421336,593.1425591116601,689.9515513706316,753.8378337006548,713.6116050230352,665.575526379199,639.8702521661144,566.4381100383742,602.769675543065,636.9758223614031,838.8391505866864,1478.874805344335,1459.149044006308,1387.6915345139455,1353.654060604915,1380.012526068448,1383.4946564371012,1341.579689123074,1427.5678622884227,1397.7808753654772,1344.037783237732,1257.285128280657,1297.332690017431,1316.4388778935704,1307.883347941082,1283.9768778672214,1273.1843530634096,1278.0824064357296,1321.431115766408,1354.4814820287468,1333.5878041563171,1357.2018216795293,1347.5298328353635,1317.862385602503,1316.7824410928667,1363.7911817467896,1421.45053482153,1473.7295655749122,1513.3380612007554,1527.07320879706,1520.3705478366887,1481.8238543145394,1500.151441619609,1508.9669344602714,1481.524474097877,1445.1450034130887,1417.4826755078072,1453.824676109189,1476.554974221759,1457.7835067613787,1442.5007903703054,1446.138078746579,1480.3479277869212,1474.320222966424,1423.3878323139418,1360.8577255891676,1337.3912372750488,1400.7434254248976,1406.7530196210892,1371.0172331395806,1360.0256467642882,1360.1123709007084,1359.7692618475817,1349.6076963128105,1345.1738370383607,1343.4491732619308,1328.2974945469014,1314.617505626696,1321.0909928662404,1335.4841595848136,1343.4321393541222,1344.775088467436,1333.9444077975136,1305.7116252799508,1309.2780904215947,1308.5759110007473,1306.0999714339998,1355.6284484769956,1452.5165920413374,1475.6366023948713,1447.970350106245,1406.542142884293,1377.6039021203871,1278.4930177581896,1189.0178172699625,1205.2225452548764,1179.4867044106006,881.8538957920834,1197.5584167943596,1324.3785722370592,1361.4066076471468,1263.928152316742,1005.8203163981548,759.0834395666938,679.2875862507892,708.2743155303299,712.1759249973968,604.9816438504809,659.412993337054,789.7184528406086,815.1398626985261,915.8414552062778,773.3202934324213],\n",
        "# [-397.86472,0,-21.54154,-48.849564,16.483715,36.6015,-6.4991527,-25.834284,-1.2522386,10.178377,-5.218914,-7.345142,10.264649,16.472473,7.895236,5.5313177,6.2877584,-4.182223,-14.012452,-7.7094264,0.4547759,-5.8861904,-13.388045,-6.9823565,1.1916866,-1.7707733,-5.276302,-0.35934502,1.7799568,-4.779073,-6.8897967,0.79337484,4.2686896,-2.5059521,-6.496715,-1.0943556,3.1531215,0.25546533,-2.272549,-0.73324627,0.0020391261,0.012858815,0.062261246,0.030906683,0.033696823,0.06754137,0.054984007,0.36406443,0.18963988,0.10330482,0.12989527,0.42003468,0.48301712,0.11749379,0.06369407,0.12233541,0.2568086,0.5382474,1.106481,1.4184383,0.71715385,0.35170192,0.09534879,0.0366585,0.102917545,0.087631606,0.03625296,0.06454905,0.095770694,0.12915519,0.25286278,0.2482152,0.20051777,0.39578456,0.28170255,0.3759659,0.4401238,0.90044194,0.8636782,1.1747689,0.7714911,0.818637,0.37247756,0.3380456,0.7661559,2.4523528,2.7824976,8.302962,16.734638,8.80889,12.927655,13.964713,7.212821,7.622612,17.17792,10.23075,8.3349285,5.6397433,3.6112895,0.6386233,0.24162434,0.34277353,0.36261582,0.3141538,0.38225174,0.47966504,0.03040914,0.0012386683,9.3100425e-05,7.8610115e-05,7.8027195e-05,5.490173e-06,1.6225702e-05,5.6767494e-06,4.4215603e-06,9.099585e-06,1.2576394e-06,2.412565e-06,7.926269e-07,2.0946077e-06,1.3263315e-06,6.3015705e-07,4.1206027e-07,7.42592e-07,5.666581e-07,2.967201e-07,1.7622109e-07,4.3961313e-07,1.7293216e-07,1.4194566e-07,1.8899503e-07,1.5307475e-07,9.427612e-08,1.06292724e-07,1.01943236e-07,6.155833e-08,7.990511e-08,6.0735445e-08,4.232562e-08,6.248923e-08,3.7052516e-08,4.0408686e-08,3.633701e-08,2.8342097e-08,3.3516038e-08,2.3205962e-08,2.8330943e-08,2.0632546e-08,2.3197451e-08,1.7732157e-08,1.965794e-08,1.6046194e-08,1.7022142e-08,1.4855194e-08,1.4529622e-08,1.42051055e-08,1.3379163e-08,1.3088586e-08,1.2451093e-08,1.209253e-08,1.2065797e-08,1.1807311e-08,1.1646068e-08,1.16453025e-08,1.1936379e-08,1.2163221e-08,1.1026455e-08,7.149512e-09,0.36461177,0.34337756,0.27882302,0.48984087,0.58728004,0.33111906,0.35057455,0.40294,0.37982342,0.38546938,0.35827973,0.3449619,4098.515885613107,4394.679597751644,2173.6066930992697,660.1920797101604,666.3801569895414,679.7044596914801,646.1064276646537,678.6052827762849,636.4525179151398,588.8034579300204,588.6927126149499,605.9566149564441,554.6815422905305,643.55633138673,635.2709118421336,593.1425591116601,689.9515513706316,753.8378337006548,713.6116050230352,665.575526379199,639.8702521661144,566.4381100383742,602.769675543065,636.9758223614031,838.8391505866864,1478.874805344335,1459.149044006308,1387.6915345139455,1353.654060604915,1380.012526068448,1383.4946564371012,1341.579689123074,1427.5678622884227,1397.7808753654772,1344.037783237732,1257.285128280657,1297.332690017431,1316.4388778935704,1307.883347941082,1283.9768778672214,1273.1843530634096,1278.0824064357296,1321.431115766408,1354.4814820287468,1333.5878041563171,1357.2018216795293,1347.5298328353635,1317.862385602503,1316.7824410928667,1363.7911817467896,1421.45053482153,1473.7295655749122,1513.3380612007554,1527.07320879706,1520.3705478366887,1481.8238543145394,1500.151441619609,1508.9669344602714,1481.524474097877,1445.1450034130887,1417.4826755078072,1453.824676109189,1476.554974221759,1457.7835067613787,1442.5007903703054,1446.138078746579,1480.3479277869212,1474.320222966424,1423.3878323139418,1360.8577255891676,1337.3912372750488,1400.7434254248976,1406.7530196210892,1371.0172331395806,1360.0256467642882,1360.1123709007084,1359.7692618475817,1349.6076963128105,1345.1738370383607,1343.4491732619308,1328.2974945469014,1314.617505626696,1321.0909928662404,1335.4841595848136,1343.4321393541222,1344.775088467436,1333.9444077975136,1305.7116252799508,1309.2780904215947,1308.5759110007473,1306.0999714339998,1355.6284484769956,1452.5165920413374,1475.6366023948713,1447.970350106245,1406.542142884293,1377.6039021203871,1278.4930177581896,1189.0178172699625,1205.2225452548764,1179.4867044106006,881.8538957920834,1197.5584167943596,1324.3785722370592,1361.4066076471468,1263.928152316742,1005.8203163981548,759.0834395666938,679.2875862507892,708.2743155303299,712.1759249973968,604.9816438504809,659.412993337054,789.7184528406086,815.1398626985261,915.8414552062778,773.3202934324213],\n",
        "# [0,124.46949,-21.54154,-48.849564,16.483715,36.6015,-6.4991527,-25.834284,-1.2522386,10.178377,-5.218914,-7.345142,10.264649,16.472473,7.895236,5.5313177,6.2877584,-4.182223,-14.012452,-7.7094264,0.4547759,-5.8861904,-13.388045,-6.9823565,1.1916866,-1.7707733,-5.276302,-0.35934502,1.7799568,-4.779073,-6.8897967,0.79337484,4.2686896,-2.5059521,-6.496715,-1.0943556,3.1531215,0.25546533,-2.272549,-0.73324627,0.0020391261,0.012858815,0.062261246,0.030906683,0.033696823,0.06754137,0.054984007,0.36406443,0.18963988,0.10330482,0.12989527,0.42003468,0.48301712,0.11749379,0.06369407,0.12233541,0.2568086,0.5382474,1.106481,1.4184383,0.71715385,0.35170192,0.09534879,0.0366585,0.102917545,0.087631606,0.03625296,0.06454905,0.095770694,0.12915519,0.25286278,0.2482152,0.20051777,0.39578456,0.28170255,0.3759659,0.4401238,0.90044194,0.8636782,1.1747689,0.7714911,0.818637,0.37247756,0.3380456,0.7661559,2.4523528,2.7824976,8.302962,16.734638,8.80889,12.927655,13.964713,7.212821,7.622612,17.17792,10.23075,8.3349285,5.6397433,3.6112895,0.6386233,0.24162434,0.34277353,0.36261582,0.3141538,0.38225174,0.47966504,0.03040914,0.0012386683,9.3100425e-05,7.8610115e-05,7.8027195e-05,5.490173e-06,1.6225702e-05,5.6767494e-06,4.4215603e-06,9.099585e-06,1.2576394e-06,2.412565e-06,7.926269e-07,2.0946077e-06,1.3263315e-06,6.3015705e-07,4.1206027e-07,7.42592e-07,5.666581e-07,2.967201e-07,1.7622109e-07,4.3961313e-07,1.7293216e-07,1.4194566e-07,1.8899503e-07,1.5307475e-07,9.427612e-08,1.06292724e-07,1.01943236e-07,6.155833e-08,7.990511e-08,6.0735445e-08,4.232562e-08,6.248923e-08,3.7052516e-08,4.0408686e-08,3.633701e-08,2.8342097e-08,3.3516038e-08,2.3205962e-08,2.8330943e-08,2.0632546e-08,2.3197451e-08,1.7732157e-08,1.965794e-08,1.6046194e-08,1.7022142e-08,1.4855194e-08,1.4529622e-08,1.42051055e-08,1.3379163e-08,1.3088586e-08,1.2451093e-08,1.209253e-08,1.2065797e-08,1.1807311e-08,1.1646068e-08,1.16453025e-08,1.1936379e-08,1.2163221e-08,1.1026455e-08,7.149512e-09,0.36461177,0.34337756,0.27882302,0.48984087,0.58728004,0.33111906,0.35057455,0.40294,0.37982342,0.38546938,0.35827973,0.3449619,4098.515885613107,4394.679597751644,2173.6066930992697,660.1920797101604,666.3801569895414,679.7044596914801,646.1064276646537,678.6052827762849,636.4525179151398,588.8034579300204,588.6927126149499,605.9566149564441,554.6815422905305,643.55633138673,635.2709118421336,593.1425591116601,689.9515513706316,753.8378337006548,713.6116050230352,665.575526379199,639.8702521661144,566.4381100383742,602.769675543065,636.9758223614031,838.8391505866864,1478.874805344335,1459.149044006308,1387.6915345139455,1353.654060604915,1380.012526068448,1383.4946564371012,1341.579689123074,1427.5678622884227,1397.7808753654772,1344.037783237732,1257.285128280657,1297.332690017431,1316.4388778935704,1307.883347941082,1283.9768778672214,1273.1843530634096,1278.0824064357296,1321.431115766408,1354.4814820287468,1333.5878041563171,1357.2018216795293,1347.5298328353635,1317.862385602503,1316.7824410928667,1363.7911817467896,1421.45053482153,1473.7295655749122,1513.3380612007554,1527.07320879706,1520.3705478366887,1481.8238543145394,1500.151441619609,1508.9669344602714,1481.524474097877,1445.1450034130887,1417.4826755078072,1453.824676109189,1476.554974221759,1457.7835067613787,1442.5007903703054,1446.138078746579,1480.3479277869212,1474.320222966424,1423.3878323139418,1360.8577255891676,1337.3912372750488,1400.7434254248976,1406.7530196210892,1371.0172331395806,1360.0256467642882,1360.1123709007084,1359.7692618475817,1349.6076963128105,1345.1738370383607,1343.4491732619308,1328.2974945469014,1314.617505626696,1321.0909928662404,1335.4841595848136,1343.4321393541222,1344.775088467436,1333.9444077975136,1305.7116252799508,1309.2780904215947,1308.5759110007473,1306.0999714339998,1355.6284484769956,1452.5165920413374,1475.6366023948713,1447.970350106245,1406.542142884293,1377.6039021203871,1278.4930177581896,1189.0178172699625,1205.2225452548764,1179.4867044106006,881.8538957920834,1197.5584167943596,1324.3785722370592,1361.4066076471468,1263.928152316742,1005.8203163981548,759.0834395666938,679.2875862507892,708.2743155303299,712.1759249973968,604.9816438504809,659.412993337054,789.7184528406086,815.1398626985261,915.8414552062778,773.3202934324213]])\n",
        "\n",
        "print(X.shape)\n",
        "# y = torch.Tensor([3.21]).to(device)\n",
        "\n",
        "pred = model(X).squeeze()\n",
        "print(pred)\n",
        "# loss_fn = nn.MSELoss()\n",
        "# print(loss_fn(pred, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ys2SVUuyQZou"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "batch 1 loss: 0.3137862396240234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ashutosh/Desktop/ugmqa_project/venv/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch 11 loss: 0.8799844142200891\n",
            "batch 21 loss: 0.3316475596372038\n",
            "batch 31 loss: 0.18205110877053812\n",
            "batch 41 loss: 0.15711148275062442\n",
            "batch 51 loss: 0.16122780553996563\n",
            "batch 61 loss: 0.1625233540865662\n",
            "batch 71 loss: 0.15334290117025376\n",
            "batch 81 loss: 0.1982310657016933\n",
            "batch 91 loss: 0.25095088094472884\n",
            "batch 101 loss: 0.11188910387456417\n",
            "batch 111 loss: 0.35187231820076703\n",
            "batch 121 loss: 0.15820662846788763\n",
            "batch 131 loss: 0.23669531404972077\n",
            "batch 141 loss: 0.2049787424504757\n",
            "batch 151 loss: 0.14772035262431019\n",
            "batch 161 loss: 0.11071514077484608\n",
            "batch 171 loss: 0.24033718943595886\n",
            "batch 181 loss: 0.2651617312431335\n",
            "batch 191 loss: 0.17597591083496808\n",
            "batch 201 loss: 0.09521109985653312\n",
            "batch 211 loss: 0.15024343252182007\n",
            "batch 221 loss: 0.19694268659513908\n",
            "batch 231 loss: 0.1372122206632048\n",
            "batch 241 loss: 0.20314003211795353\n",
            "batch 251 loss: 0.20448532313108445\n",
            "batch 261 loss: 0.25657039068639276\n",
            "batch 271 loss: 0.2921939765755087\n",
            "batch 281 loss: 0.14327034532674587\n",
            "batch 291 loss: 0.2635837719135452\n",
            "batch 301 loss: 0.25630649741506206\n",
            "batch 311 loss: 0.11939661441370845\n",
            "batch 321 loss: 0.14082976905629038\n",
            "batch 331 loss: 0.2408275881409645\n",
            "batch 341 loss: 0.16953838009852917\n",
            "batch 351 loss: 0.15830085568130017\n",
            "batch 361 loss: 0.1704409060254693\n",
            "batch 371 loss: 0.0999297676095739\n",
            "batch 381 loss: 0.1617041607559804\n",
            "batch 391 loss: 0.17401415050029756\n",
            "batch 401 loss: 0.06690494203008711\n",
            "batch 411 loss: 0.17364383969455957\n",
            "batch 421 loss: 0.17632860515266657\n",
            "batch 431 loss: 0.188264605703298\n",
            "batch 441 loss: 0.148898336738348\n",
            "batch 451 loss: 0.224057107870467\n",
            "batch 461 loss: 0.1318717165151611\n",
            "batch 471 loss: 0.23932779499886464\n",
            "batch 481 loss: 0.16043965732678772\n",
            "batch 491 loss: 0.15427734270691873\n",
            "batch 501 loss: 0.18921271331608294\n",
            "batch 511 loss: 0.1813458404719131\n",
            "batch 521 loss: 0.18570727795362474\n",
            "batch 531 loss: 0.20830121810548008\n",
            "batch 541 loss: 0.2425629749149084\n",
            "batch 551 loss: 0.11732707567629404\n",
            "batch 561 loss: 0.14418456236409838\n",
            "batch 571 loss: 0.14281892352257272\n",
            "batch 581 loss: 0.22434469735249876\n",
            "batch 591 loss: 0.14060008602798915\n",
            "batch 601 loss: 0.19539238914847373\n",
            "batch 611 loss: 0.16733892669086345\n",
            "batch 621 loss: 0.20500266648828983\n",
            "batch 631 loss: 0.27451700329780576\n",
            "batch 641 loss: 0.17401080321782503\n",
            "batch 651 loss: 0.1867427171394229\n",
            "batch 661 loss: 0.2412618172899238\n",
            "batch 671 loss: 0.09175870600156485\n",
            "batch 681 loss: 0.10591359928250313\n",
            "batch 691 loss: 0.08878500721883029\n",
            "batch 701 loss: 0.19520824790000915\n",
            "batch 711 loss: 0.10466306445305236\n",
            "batch 721 loss: 0.18400063775479794\n",
            "batch 731 loss: 0.2676899788156152\n",
            "batch 741 loss: 0.2072221253812313\n",
            "batch 751 loss: 0.12487787276506424\n",
            "batch 761 loss: 0.15967615269124508\n",
            "batch 771 loss: 0.1898450840264559\n",
            "batch 781 loss: 0.1400856015080899\n",
            "batch 791 loss: 0.20725995607674122\n",
            "batch 801 loss: 0.08420230686664582\n",
            "batch 811 loss: 0.17654597367625682\n",
            "batch 821 loss: 0.16921762559562922\n",
            "batch 831 loss: 0.1643996349722147\n",
            "batch 841 loss: 0.22266158871352673\n",
            "batch 851 loss: 0.08912513762712479\n",
            "batch 861 loss: 0.28289470668882133\n",
            "batch 871 loss: 0.11262974843382835\n",
            "batch 881 loss: 0.19412114724516868\n",
            "batch 891 loss: 0.14059323260560633\n",
            "batch 901 loss: 0.1901768944435753\n",
            "batch 911 loss: 0.12414102010428905\n",
            "batch 921 loss: 0.1405030071362853\n",
            "batch 931 loss: 0.1755282206600532\n",
            "batch 941 loss: 0.11105894776061177\n",
            "batch 951 loss: 0.14302107621449978\n",
            "batch 961 loss: 0.3249036918953061\n",
            "batch 971 loss: 0.16529501527547835\n",
            "batch 981 loss: 0.17969389147838227\n",
            "batch 991 loss: 0.07065250704064965\n",
            "batch 1001 loss: 0.20423564249183981\n",
            "batch 1011 loss: 0.23395749012939632\n",
            "batch 1021 loss: 0.13521130118519067\n",
            "batch 1031 loss: 0.20842313033121174\n",
            "batch 1041 loss: 0.22109910175204278\n",
            "batch 1051 loss: 0.1773498110845685\n",
            "batch 1061 loss: 0.16393800739198924\n",
            "batch 1071 loss: 0.2574063688516617\n",
            "batch 1081 loss: 0.16617594264447688\n",
            "batch 1091 loss: 0.11766138726845383\n",
            "batch 1101 loss: 0.12759570207446813\n",
            "batch 1111 loss: 0.23513126909732818\n",
            "batch 1121 loss: 0.26976062878966334\n",
            "batch 1131 loss: 0.15128074752225074\n",
            "batch 1141 loss: 0.19852347848936916\n",
            "batch 1151 loss: 0.24277920005763917\n",
            "batch 1161 loss: 0.2238038246333599\n",
            "batch 1171 loss: 0.2716988307237625\n",
            "batch 1181 loss: 0.2363216575840488\n",
            "batch 1191 loss: 0.18522681398317217\n",
            "batch 1201 loss: 0.2816934287548065\n",
            "batch 1211 loss: 0.20819337822496892\n",
            "batch 1221 loss: 0.18215633168816567\n",
            "batch 1231 loss: 0.11920377980917692\n",
            "batch 1241 loss: 0.14742878273129464\n",
            "batch 1251 loss: 0.281537603251636\n",
            "batch 1261 loss: 0.13341238549910486\n",
            "batch 1271 loss: 0.20853384472429753\n",
            "batch 1281 loss: 0.22075894160196186\n",
            "batch 1291 loss: 0.210180642225896\n",
            "batch 1301 loss: 0.27851666152477267\n",
            "batch 1311 loss: 0.25017073505558074\n",
            "batch 1321 loss: 0.2074112022027839\n",
            "batch 1331 loss: 0.2184725057182368\n",
            "batch 1341 loss: 0.20746607006161866\n",
            "batch 1351 loss: 0.2690564772114158\n",
            "batch 1361 loss: 0.16075425691902637\n",
            "batch 1371 loss: 0.13726047143340112\n",
            "batch 1381 loss: 0.21035214084782639\n",
            "batch 1391 loss: 0.13344032660126687\n",
            "batch 1401 loss: 0.18807932855561377\n",
            "batch 1411 loss: 0.15515978124458343\n",
            "batch 1421 loss: 0.23287482231855391\n",
            "batch 1431 loss: 0.24807456866838037\n",
            "batch 1441 loss: 0.1524362379516242\n",
            "batch 1451 loss: 0.10079102726333077\n",
            "batch 1461 loss: 0.1701354444772005\n",
            "batch 1471 loss: 0.12121869526803493\n",
            "batch 1481 loss: 0.14916345037519932\n",
            "batch 1491 loss: 0.16720598393818364\n",
            "batch 1501 loss: 0.1720743878762005\n",
            "batch 1511 loss: 0.22426876410841942\n",
            "batch 1521 loss: 0.19002220910042525\n",
            "batch 1531 loss: 0.08446499524292449\n",
            "batch 1541 loss: 0.23758628210052848\n",
            "batch 1551 loss: 0.20591787992023455\n",
            "batch 1561 loss: 0.18006849367404357\n",
            "batch 1571 loss: 0.16938797533512115\n",
            "batch 1581 loss: 0.24328968415036797\n",
            "batch 1591 loss: 0.16198129430413247\n",
            "batch 1601 loss: 0.1507755568959692\n",
            "batch 1611 loss: 0.12974022146314382\n",
            "batch 1621 loss: 0.16695675377035513\n",
            "batch 1631 loss: 0.13284973222762347\n",
            "batch 1641 loss: 0.10637063045287505\n",
            "batch 1651 loss: 0.29673379150102847\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.000003)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "model.train()\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        # print('X', X.shape)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        # print(\"Prediction\")\n",
        "        pred = pred.squeeze()\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # losses.append(loss.to('cpu').detach().numpy())\n",
        "        # iterations += 1\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # print(\"Loss gradient\", loss.grad)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            last_loss = running_loss / 100  # loss per batch\n",
        "            print('batch {} loss: {}'.format(batch + 1, last_loss))\n",
        "            # tb_x = epoch * len(dataloader) + batch + 1\n",
        "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    # model.eval()\n",
        "    # size = len(dataloader_val.dataset)\n",
        "    # num_batches = len(dataloader_val)\n",
        "    # test_loss, correct = 0, 0\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   for batch, (X, y) in enumerate(dataloader_val):\n",
        "    #       X = X.to(device)\n",
        "    #       y = y.to(device)\n",
        "    #       pred = model(X)\n",
        "    #       test_loss += loss_fn(pred, y).item()\n",
        "    #       correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    break\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Yn7xm6aGUvMP",
        "outputId": "9f0a1429-44ea-4384-b593-185a5575fb30"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'losses' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#run\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(losses)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mIterations\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/colab_model.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
          ]
        }
      ],
      "source": [
        "# run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "# plt.savefig(\"training.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHImI0fDU8mK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
