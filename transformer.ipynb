{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./venv/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (3.7.3)\n",
      "Requirement already satisfied: spafe in ./venv/lib/python3.8/site-packages (0.3.2)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./venv/lib/python3.8/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./venv/lib/python3.8/site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.14 in ./venv/lib/python3.8/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./venv/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./venv/lib/python3.8/site-packages (from librosa) (0.58.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./venv/lib/python3.8/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./venv/lib/python3.8/site-packages (from librosa) (0.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./venv/lib/python3.8/site-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./venv/lib/python3.8/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./venv/lib/python3.8/site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./venv/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./venv/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./venv/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./venv/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./venv/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./venv/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./venv/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./venv/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./venv/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (56.0.0)\n",
      "Requirement already satisfied: wheel in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
      "Requirement already satisfied: cmake in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.27.6)\n",
      "Requirement already satisfied: lit in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch) (17.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in ./venv/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (0.41.0)\n",
      "Requirement already satisfied: importlib-metadata in ./venv/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (6.8.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./venv/lib/python3.8/site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.8/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./venv/lib/python3.8/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa matplotlib spafe torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from spafe.utils import vis\n",
    "from spafe.features.lfcc import lfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_size, num_heads):\n",
    "\n",
    "    super(SelfAttention, self).__init__()\n",
    "\n",
    "    self.embed_size = embed_size\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = embed_size // num_heads\n",
    "\n",
    "    assert (self.head_dim * num_heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "    self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "    self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "  def forward(self, values, keys, query, mask):\n",
    "    N = query.shape[0]\n",
    "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "    # Split embedding into self.num_heads pieces\n",
    "    values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "    keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "    queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "    values = self.values(values)\n",
    "    keys = self.keys(keys)\n",
    "    queries = self.queries(queries)\n",
    "\n",
    "    energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) # MatMul Q and K\n",
    "    # queries shape: (N, query_len, heads, heads_dim)\n",
    "    # keys shape: (N, query_len, heads, heads_dim)\n",
    "    # energy shape: (N, heads, query_len, key_len)\n",
    "\n",
    "    if mask is not None:\n",
    "      energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "    attention = torch.softmax(energy / (self.embed_size ** 0.5), dim = 3)\n",
    "\n",
    "    out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "      N, query_len, self.num_heads * self.head_dim\n",
    "    )\n",
    " \n",
    "    out = self.fc_out(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_size, dropout, max_len = 5000):\n",
    "    super().__init__()\n",
    "    print('Embed Size', embed_size)\n",
    "\n",
    "    self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "    position_encoding = torch.zeros(max_len, 1, embed_size)\n",
    "    position_encoding[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "    position_encoding[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', position_encoding)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.position_encoding[:x.size(0)]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "\n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "  def forward(self, value, key, query, mask):\n",
    "    attention = self.attention(value, key, query, mask)\n",
    "\n",
    "    x = self.dropout(self.norm1((attention + query)))\n",
    "\n",
    "    forward = self.feed_forward(x)\n",
    "    out = self.dropout(self.norm2(x + forward))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    device,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_length\n",
    "  ):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.device = device\n",
    "    self.position_embedding = PositionalEncoding(embed_size, dropout, src_vocab_size)\n",
    "\n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    N, seq_length = x.shape\n",
    "\n",
    "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "    out = self.dropout(self.position_embedding(positions))\n",
    "\n",
    "    for layer in self.layers:\n",
    "      out = layer(out, out, out, mask)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    src_pad_index,\n",
    "    embed_size = 256,\n",
    "    num_layers = 6,\n",
    "    forward_expansion = 4,\n",
    "    heads = 8,\n",
    "    dropout = 0,\n",
    "    device = \"cuda\",\n",
    "    max_length = 500\n",
    "  ):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(\n",
    "      src_vocab_size, embed_size, num_layers, heads,\n",
    "      device, forward_expansion, dropout, max_length)\n",
    "\n",
    "    self.src_pad_index = src_pad_index\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def make_src_mask(self, src):\n",
    "    src_mask = (src != self.src_pad_index).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # (N, 1, 1, src_len)\n",
    "    return src_mask.to(self.device)\n",
    "\n",
    "  \n",
    "  def forward(self, src, target):\n",
    "    src_mask = self.make_src_mask(src)\n",
    "    encoder_output = self.encoder(src, src_mask)\n",
    "\n",
    "    print(encoder_output)\n",
    "\n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed Size 512\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_layers = 4\n",
    "src_vocab_size = 256 # X_train.shape[1]\n",
    "src_pad_index = 0\n",
    "embed_size = 512 # X_train.shape[2]\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "output_size = 1\n",
    "batch_size = 64\n",
    "\n",
    "model = Transformer(\n",
    "  src_vocab_size,\n",
    "  src_pad_index,\n",
    "  embed_size = embed_size,\n",
    "  dropout = dropout,\n",
    "  heads = num_heads,\n",
    "  num_layers = num_layers,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   inputs, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_loader' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.000003)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "\n",
    "for i, data in enumerate(training_loader):\n",
    "  inputs, labels = data\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  outputs = model(inputs)\n",
    "\n",
    "  loss = loss_fn(outputs, labels)\n",
    "  loss.backward()\n",
    "\n",
    "  optimizer.step()\n",
    "\n",
    "  running_loss += loss.item()\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    last_loss = running_loss / 1000\n",
    "    print(' batch {} loss: {}'.format(i + 1, last_loss))\n",
    "    running_loss = 0.\n",
    "\n",
    "\n",
    "print('Loss : ', last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# 1. https://www.youtube.com/watch?v=zxQyTK8quyY\n",
    "# 2. https://www.youtube.com/watch?v=bCz4OMemCcA\n",
    "# 3. https://www.youtube.com/watch?v=gJ9kaJsE78k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
