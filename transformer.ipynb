{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: matplotlib in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (3.7.3)\n",
      "Requirement already satisfied: spafe in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (0.3.2)\n",
      "Requirement already satisfied: torch in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: pandas in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (1.24.4)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (0.58.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from librosa) (4.7.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (3.12.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: jinja2 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: networkx in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: sympy in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: setuptools in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (56.0.0)\n",
      "Requirement already satisfied: wheel in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
      "Requirement already satisfied: lit in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.16.2)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (0.41.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (6.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ashutosh/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/home/ashutosh/.pyenv/versions/3.8.13/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa matplotlib spafe torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from spafe.utils import vis\n",
    "from spafe.features.lfcc import lfcc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_size, num_heads):\n",
    "\n",
    "    super(SelfAttention, self).__init__()\n",
    "\n",
    "    self.embed_size = embed_size\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = embed_size // num_heads\n",
    "\n",
    "    assert (self.head_dim * num_heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "    self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "    self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "  def forward(self, values, keys, query, mask):\n",
    "    N = query.shape[0]\n",
    "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "    # Split embedding into self.num_heads pieces\n",
    "    values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "    keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "    queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "    values = self.values(values)\n",
    "    keys = self.keys(keys)\n",
    "    queries = self.queries(queries)\n",
    "\n",
    "    energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) # MatMul Q and K\n",
    "    # queries shape: (N, query_len, heads, heads_dim)\n",
    "    # keys shape: (N, query_len, heads, heads_dim)\n",
    "    # energy shape: (N, heads, query_len, key_len)\n",
    "\n",
    "    if mask is not None:\n",
    "      energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "    attention = torch.softmax(energy / (self.embed_size ** 0.5), dim = 3)\n",
    "\n",
    "    out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "      N, query_len, self.num_heads * self.head_dim\n",
    "    )\n",
    " \n",
    "    out = self.fc_out(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_size, dropout, max_len = 5000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "    self.position_encoding = torch.zeros(max_len, 1, embed_size).to(device)\n",
    "    self.position_encoding[:, 0, 0::2] = torch.sin(position * div_term).to(device)\n",
    "    self.position_encoding[:, 0, 1::2] = torch.cos(position * div_term).to(device)\n",
    "    self.register_buffer('pe', self.position_encoding)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.position_encoding[:x.size(0)]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "\n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "  def forward(self, value, key, query, mask):\n",
    "    attention = self.attention(value, key, query, mask)\n",
    "\n",
    "    x = self.dropout(self.norm1((attention + query)))\n",
    "\n",
    "    forward = self.feed_forward(x)\n",
    "    out = self.dropout(self.norm2(x + forward))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    device,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_length\n",
    "  ):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embed_size = embed_size\n",
    "    self.device = device\n",
    "    self.position_embedding = PositionalEncoding(embed_size, dropout, src_vocab_size)\n",
    "\n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    N, seq_length = x.shape\n",
    "\n",
    "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "    out = self.dropout(self.position_embedding(positions))\n",
    "\n",
    "    for layer in self.layers:\n",
    "      out = layer(out, out, out, mask)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    src_pad_index,\n",
    "    embed_size = 256,\n",
    "    num_layers = 6,\n",
    "    forward_expansion = 4,\n",
    "    heads = 8,\n",
    "    dropout = 0,\n",
    "    device = \"cuda\",\n",
    "    max_length = 500\n",
    "  ):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(\n",
    "      src_vocab_size, embed_size, num_layers, heads,\n",
    "      device, forward_expansion, dropout, max_length)\n",
    "\n",
    "    self.src_pad_index = src_pad_index\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def make_src_mask(self, src):\n",
    "    src_mask = (src != self.src_pad_index).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # (N, 1, 1, src_len)\n",
    "    return src_mask.to(self.device)\n",
    "\n",
    "  \n",
    "  def forward(self, src):\n",
    "    src_mask = self.make_src_mask(src)\n",
    "    encoder_output = self.encoder(src, src_mask)\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, annotations_file, mode = 'train'):\n",
    "        self.features = pd.read_csv(annotations_file)\n",
    "        self.features = self.features.drop(['Unnamed: 0'], axis = 1)\n",
    "        self.train_subset, self.valid_subset = random_split(self.features, [0.8, 0.2])\n",
    "        self.x_train = torch.Tensor(self.train_subset.dataset.drop([\n",
    "            'class', str(len(self.train_subset.dataset.columns) - 2)\n",
    "        ], axis = 1).values)\n",
    "        self.y_train = torch.Tensor(self.train_subset.dataset['class'].values)\n",
    "        self.x_valid = torch.Tensor(self.valid_subset.dataset.drop([\n",
    "            'class', str(len(self.train_subset.dataset.columns) - 2)], axis = 1).values)\n",
    "        self.y_valid = torch.Tensor(self.valid_subset.dataset['class'].values)\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return self.x_train.shape[0]\n",
    "\n",
    "        return self.x_valid.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            return self.x_train[idx], self.y_train[idx]\n",
    "        \n",
    "        return self.x_valid[idx], self.y_valid[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = AudioFeatureDataset('./DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm_4featu.csv')\n",
    "validation_dataset = AudioFeatureDataset(\n",
    "  './DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm_4featu.csv',\n",
    "  mode = 'valid'\n",
    ")\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size = 2, shuffle = True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_layers \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m src_vocab_size \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m src_pad_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m embed_size \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "src_vocab_size = X_train.shape[1]\n",
    "src_pad_index = 0\n",
    "embed_size = X_train.shape[2]\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "output_size = 1\n",
    "batch_size = 64\n",
    "\n",
    "model = Transformer(\n",
    "  src_vocab_size,\n",
    "  src_pad_index,\n",
    "  embed_size = embed_size,\n",
    "  dropout = dropout,\n",
    "  heads = num_heads,\n",
    "  num_layers = num_layers,\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (296) must match the size of tensor b (2) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m   src_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m   encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, src_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m encoder_output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding(positions))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   out \u001b[39m=\u001b[39m layer(out, out, out, mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, value, key, query, mask):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(value, key, query, mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1((attention \u001b[39m+\u001b[39m query)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# queries shape: (N, query_len, heads, heads_dim)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# keys shape: (N, query_len, heads, heads_dim)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# energy shape: (N, heads, query_len, key_len)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m   energy \u001b[39m=\u001b[39m energy\u001b[39m.\u001b[39;49mmasked_fill(mask \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m, \u001b[39mfloat\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m-1e20\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(energy \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_size \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m), dim \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mnhql,nlhd->nqhd\u001b[39m\u001b[39m\"\u001b[39m, [attention, values])\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m   N, query_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashutosh/Desktop/ugmqa_project/transformer.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (296) must match the size of tensor b (2) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.000003)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  for batch, (X, y) in enumerate(training_dataloader):\n",
    "    # Compute prediction and loss\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "  \n",
    "  \n",
    "  model.eval()\n",
    "  size = len(validation_dataloader.dataset)\n",
    "  num_batches = len(validation_dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in validation_dataloader:\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# 1. https://www.youtube.com/watch?v=zxQyTK8quyY\n",
    "# 2. https://www.youtube.com/watch?v=bCz4OMemCcA\n",
    "# 3. https://www.youtube.com/watch?v=gJ9kaJsE78k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
